{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dbcf9b4-483a-40ea-8c03-bed78cda47c3",
   "metadata": {},
   "source": [
    "* https://towardsdatascience.com/transformers-141e32e69591\n",
    "* https://towardsdatascience.com/stock-predictions-with-state-of-the-art-transformer-and-time-embeddings-3a4485237de6\n",
    "* https://towardsdatascience.com/lstm-networks-a-detailed-explanation-8fae6aefc7f9#:~:text=LSTMs%20use%20a%20series%20of,each%20their%20own%20neural%20network.\n",
    "* https://stats.stackexchange.com/questions/365778/what-should-i-do-when-my-neural-network-doesnt-generalize-well\n",
    "* https://www.youtube.com/watch?v=ROLugVqjf00\n",
    "* https://lilianweng.github.io/posts/2019-03-14-overfit/    (very important)\n",
    "* https://atcold.github.io/pytorch-Deep-Learning/en/week11/11-3/\n",
    "* https://towardsdatascience.com/choosing-and-customizing-loss-functions-for-image-processing-a0e4bf665b0a\n",
    "* https://towardsdatascience.com/learning-to-rank-a-complete-guide-to-ranking-using-machine-learning-4c9688d370d4\n",
    "* https://embracingtherandom.com/machine-learning/tensorflow/ranking/deep-learning/learning-to-rank-part-2/#where-do-probabilities-fit-into-listnet\n",
    "* https://www.topbots.com/attention-for-time-series-forecasting-and-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90dc90ea-65c6-4440-8410-11d2f6afb4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NR_OF_PIXELS=4\n",
    "BACKWARD_WINDOW_LENGTH = 3\n",
    "FORWARD_WINDOW_LENGTH = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3092ce-8ffd-42ce-bf10-41cf93cb035e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b11b8bc3-2e60-4f49-9359-a751c5d2e45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import MetaTrader5 as mt5\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import pytz\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import entropy\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_ranking as tfr\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e040057-d8d5-42e5-b0f5-91046b417cf0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e4ff210-342f-4879-82a6-3e6ee032b63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', '{:.5f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e03a0df7-f9f2-46a6-b110-2570b4fe1406",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_o_TIME_ZONE = pytz.timezone(\"Etc/UTC\")\n",
    "gc_dt_FROM = datetime(2022, 8, 1, tzinfo=gc_o_TIME_ZONE)\n",
    "gc_dt_TO = datetime(2022, 11, 1, tzinfo=gc_o_TIME_ZONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11878356-d9cb-4d52-a4d3-93ff8637487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sCategory = 'Stocks\\\\Germany\\\\Banking & Investment Services\\\\Banks\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593979c9-3fa4-4747-a198-d6cb2befa91e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# SOURCE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69889ffe-e4c5-470b-9417-328ef9998d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfFetchSampleDataFromMt(p_sSymbolName):\n",
    "    aOhlSample = mt5.copy_rates_range(\n",
    "        p_sSymbolName,\n",
    "        mt5.TIMEFRAME_M1,\n",
    "        gc_dt_FROM, \n",
    "        gc_dt_TO\n",
    "    )\n",
    "\n",
    "    dfToReturn = pd.DataFrame(aOhlSample)\n",
    "    if 'time' in list(dfToReturn.columns):\n",
    "        return dfToReturn\n",
    "    else:\n",
    "        return dfFetchSampleDataFromMt(p_sSymbolName)\n",
    "    \n",
    "    \n",
    "\n",
    "# establish connection to the MetaTrader 5 terminal\n",
    "if not mt5.initialize():\n",
    "    print(\"initialize() failed, error code =\",mt5.last_error())\n",
    "    quit()\n",
    "    \n",
    "tplSymbols = mt5.symbols_get()\n",
    "dfSymbols = pd.DataFrame(tplSymbols, columns = tplSymbols[0]._asdict().keys())\n",
    "\n",
    "\n",
    "def aGetUniqueCategories(tplSymbols):\n",
    "    aCategories = []\n",
    "    for i in range(0, len(tplSymbols)):\n",
    "        sCategory = '\\\\'.join(tplSymbols[i]._asdict()['path'].split('\\\\')[:-1]) +'\\\\'\n",
    "        if sCategory not in aCategories:\n",
    "            aCategories.append(sCategory)\n",
    "    return aCategories\n",
    "\n",
    "\n",
    "aCategories = aGetUniqueCategories(tplSymbols)\n",
    "\n",
    "\n",
    "dfFilteredSymbols = dfSymbols[dfSymbols['path'].str.contains(sCategory, regex=False) == True]\n",
    "\n",
    "dfOhlc = pd.DataFrame()\n",
    "\n",
    "for iIndex, srsRow in dfFilteredSymbols.iterrows():\n",
    "    sSymbolName = dfFilteredSymbols.loc[iIndex, 'name']\n",
    "    iDigit  = dfFilteredSymbols.loc[iIndex, 'digits']\n",
    "    \n",
    "    dfOhlcSample =  dfFetchSampleDataFromMt(sSymbolName)\n",
    "    \n",
    "    if len(dfOhlcSample) > 500:\n",
    "        dfOhlcSample['EXCHANGE_RATE'] =  sSymbolName\n",
    "        dfOhlcSample['DIGIT_SENSITIVITY'] =  iDigit\n",
    "        dfOhlc = dfOhlc.append(dfOhlcSample)\n",
    "        \n",
    "dfOhlc.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e6ee16d-0e2b-49e7-b75a-dccfa0889fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfOhlc['time'] = pd.to_datetime(dfOhlc['time'], unit = 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a179b0d-e256-49c5-b1f0-a69d0a969dc7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a5a98da-73a0-4a62-9b32-05185da38660",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPrep = dfOhlc.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450de93f-e6f5-4b78-944a-8b7354e0070a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Add Last Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "570d74e6-f7d1-47fa-97a7-2ec05ba498ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To-be-continued"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea44ade-6230-4f9d-9972-f4bccd0d3263",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Add Candlestick Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c64645d-9248-4584-83b6-94e47ecd5629",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPrep['RETURN'] =  (dfPrep['close']-dfPrep['open'])/dfPrep['open']\n",
    "\n",
    "dfPrep[\"UPPER_SHADOW\"] =( dfPrep[\"high\"] - dfPrep[['close', 'open']].max(axis=1))/ dfPrep[['close', 'open']].max(axis=1)\n",
    "dfPrep[\"LOWER_SHADOW\"] = (dfPrep[['close', 'open']].min(axis=1) - dfPrep[\"low\"])/dfPrep[\"low\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6e36ba-2b5a-4d1b-9399-5c97c86cd8e8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Add Seasonal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44f0aa6e-b2ee-43c6-88a7-b753f4f6cb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPrep.loc[:, 'MINUTE'] = pd.to_datetime(dfPrep.loc[:, 'time'],unit='s').dt.minute\n",
    "dfPrep.loc[:, 'HOUR'] = pd.to_datetime(dfPrep.loc[:, 'time'],unit='s').dt.hour\n",
    "dfPrep.loc[:, 'DAY_OF_WEEK'] = pd.to_datetime(dfPrep.loc[:, 'time'],unit='s').dt.day_of_week\n",
    "dfPrep.loc[:, 'DAY_OF_MONTH'] = pd.to_datetime(dfPrep.loc[:, 'time'],unit='s').dt.day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038d7214-b6af-422a-9b8b-ad613093fa2a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Calculate Support and Resistence Levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c259931-59ff-488b-99b6-ad2b40dde81c",
   "metadata": {},
   "source": [
    "K-Means Algorithm used to identify the support and resistence levels.\n",
    "* https://medium.com/@judopro/using-machine-learning-to-programmatically-determine-stock-support-and-resistance-levels-9bb70777cf8e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0af030f-9ac2-484a-84c6-916c97b0a070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bIsSupport(df, l, n1, n2):\n",
    "#     for i in range(l-n1+1, l+1):\n",
    "#         if (df['low'].iloc[i] > df['low'].iloc[i-1]):\n",
    "#             return 0\n",
    "        \n",
    "#     for i in range(l+1, l+n2+1):\n",
    "#         if (df['low'].iloc[i] < df['low'].iloc[i-1] ):\n",
    "#             return 0\n",
    "        \n",
    "#     return 1\n",
    "\n",
    "\n",
    "# def bIsResistence(df, l, n1, n2):\n",
    "#     for i in range(l-n1+1, l+1):\n",
    "#         if (df['high'].iloc[i] > df['high'].iloc[i-1]):\n",
    "#             return 0\n",
    "        \n",
    "#     for i in range(l+1, l+n2+1):\n",
    "#         if (df['high'].iloc[i] < df['high'].iloc[i-1] ):\n",
    "#             return 0\n",
    "        \n",
    "#     return 1\n",
    "\n",
    "\n",
    "\n",
    "# n1 = 4\n",
    "# n2 = 4\n",
    "# for i in range(0, len(dfPrep)):\n",
    "#     dfPrep.iloc[i].loc['IS_SUPPORT'] = bIsSupport(dfPrep, i, n1, n2)\n",
    "#     dfPrep.iloc[i].loc['IS_RESISTENCE'] = bIsResistence(dfPrep, i, n1, n2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bcb69c-c19c-453f-995c-c5a067ec6d51",
   "metadata": {
    "tags": []
   },
   "source": [
    "# FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "255aa2a3-3b0d-480f-859f-726437023f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GET_DATASETS_2(dfPreprocessed, dfTimeStamps):\n",
    "    #Create PySpark SparkSession\n",
    "    oSparkSess = SparkSession.builder \\\n",
    "        .master(\"local[1]\") \\\n",
    "        .appName(\"SparkByExamples.com\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    oSparkSess.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "    def dfGetPixels(df, iFrom, iTo, dfTimeStamps):\n",
    "        \n",
    "        df['from_time'] = df['time'] + pd.DateOffset(hours=iFrom)\n",
    "        df['to_time'] = df['time'] + pd.DateOffset(hours=iTo)\n",
    "\n",
    "        df = df[['to_time', 'from_time', 'time','close', 'high']]\n",
    "        \n",
    "        \n",
    "        \n",
    "        sdf= oSparkSess.createDataFrame(df)\n",
    "        sdf.createOrReplaceTempView(\"sdf\")\n",
    "\n",
    "        dfPriceAnalysis =  oSparkSess.sql(\"\"\"\n",
    "            SELECT \n",
    "                t.from_time, \n",
    "                t.to_time,\n",
    "                t.time, \n",
    "                t.close, \n",
    "                t.HISTORICAL_HIGH, \n",
    "                count(*) AS NR_OF_MINS\n",
    "            FROM\n",
    "            (\n",
    "                SELECT df1.*, df2.high as HISTORICAL_HIGH FROM sdf df1\n",
    "                INNER JOIN sdf df2\n",
    "                ON df2.time >= df1.from_time and df2.time < df1.to_time\n",
    "            ) t\n",
    "            GROUP BY t.from_time, t.to_time, t.time, t.close, t.HISTORICAL_HIGH\n",
    "            ORDER BY t.from_time, t.to_time, t.time, t.close, t.HISTORICAL_HIGH\n",
    "        \"\"\").toPandas()\n",
    "        \n",
    "                   \n",
    "        \n",
    "        dfToReturn = pd.DataFrame(\n",
    "            data =  np.zeros((len(dfTimeStamps) , NR_OF_PIXELS)).astype(int), \n",
    "            columns = list(range(0,NR_OF_PIXELS)),\n",
    "            index = dfTimeStamps['TIME_STAMP']\n",
    "        )\n",
    "        \n",
    "        \n",
    "        if len(dfPriceAnalysis)==0:\n",
    "            return dfToReturn\n",
    "        else:\n",
    "            dfPriceAnalysis.sort_values(['time', 'HISTORICAL_HIGH'], inplace = True)\n",
    "            dfPriceAnalysis.loc[:, 'DIFF'] =  (dfPriceAnalysis.loc[: , 'HISTORICAL_HIGH']-dfPriceAnalysis.loc[:, 'close'])/dfPriceAnalysis.loc[:, 'close']\n",
    "            \n",
    "            oDiscretizer = KBinsDiscretizer(n_bins=NR_OF_PIXELS, encode='ordinal', strategy='quantile')\n",
    "\n",
    "            dfSample = pd.DataFrame(\n",
    "                data = np.arange(-0.005, 0.005 , (0.010/NR_OF_PIXELS)),\n",
    "                columns = ['DIFF']\n",
    "            )\n",
    "            oDiscretizer.fit(dfSample)\n",
    "\n",
    "            dfPriceAnalysis.loc[:, 'DIFF_BIN'] =  oDiscretizer.transform(dfPriceAnalysis.loc[:, ['DIFF']]).astype(int)\n",
    "            \n",
    "            dfPixel = dfPriceAnalysis.pivot_table(index = 'time', columns = 'DIFF_BIN', values = 'NR_OF_MINS', aggfunc = 'sum', fill_value = 0)\n",
    "            \n",
    "            dfToReturn.loc[dfPixel.index, dfPixel.columns] = dfPixel.values\n",
    "\n",
    "            return dfToReturn\n",
    "\n",
    "    \n",
    "    df = dfPreprocessed.copy()\n",
    "   \n",
    "    aExchangeRates = list(df['EXCHANGE_RATE'].unique())\n",
    "    # build input dataset\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    for sExchangeRate in aExchangeRates:\n",
    "        X_single_exc = []\n",
    "        \n",
    "        df_single_exc = df.query('EXCHANGE_RATE == @sExchangeRate')\n",
    "        df_single_exc = df_single_exc[['time','close', 'high']].fillna(0)\n",
    "        \n",
    "        \n",
    "        for i in range(-BACKWARD_WINDOW_LENGTH, 0):\n",
    "            iFrom = i\n",
    "            iTo = i +1\n",
    "\n",
    "            print(iFrom)\n",
    "            dfPixel = dfGetPixels(df_single_exc, iFrom, iTo, dfTimeStamps)\n",
    "            dfPixel= dfPixel[BACKWARD_WINDOW_LENGTH * 60:]\n",
    "            dfPixel= dfPixel[:-FORWARD_WINDOW_LENGTH * 60]\n",
    "            \n",
    "\n",
    "            if len(X_single_exc) == 0:\n",
    "                X_single_exc = dfPixel.values\n",
    "            else:\n",
    "                X_single_exc = np.append(X_single_exc, dfPixel.values, axis = 1)\n",
    "        \n",
    "        # batch_size, time_steps, feature_size, channel_size\n",
    "        X_single_exc = X_single_exc.reshape((-1, BACKWARD_WINDOW_LENGTH, dfPixel.shape[1], 1))\n",
    "        \n",
    "        if len(X) == 0:\n",
    "            X = X_single_exc\n",
    "        else:\n",
    "            X = np.append(X, X_single_exc, axis = 3)\n",
    "        \n",
    "\n",
    "\n",
    "        # build output dataset\n",
    "        Y_single_exc = []\n",
    "        for i in range(0, FORWARD_WINDOW_LENGTH):\n",
    "            iFrom = i\n",
    "            iTo = i +1\n",
    "\n",
    "            print(iFrom)\n",
    "            dfPixel = dfGetPixels(df_single_exc, iFrom, iTo, dfTimeStamps)\n",
    "\n",
    "            dfPixel= dfPixel[BACKWARD_WINDOW_LENGTH * 60:]\n",
    "            dfPixel= dfPixel[:-FORWARD_WINDOW_LENGTH * 60]\n",
    "\n",
    "            if len(Y_single_exc) == 0:\n",
    "                Y_single_exc = dfPixel.values\n",
    "            else:\n",
    "                Y_single_exc = np.append(Y_single_exc, dfPixel.values, axis = 1)\n",
    "        \n",
    "        # batch_size, time_steps, feature_size, channel_size\n",
    "        Y_single_exc = Y_single_exc.reshape((-1, FORWARD_WINDOW_LENGTH, dfPixel.shape[1], 1))\n",
    "        \n",
    "        if len(Y) == 0:\n",
    "            Y = Y_single_exc\n",
    "        else:\n",
    "            Y = np.append(Y, Y_single_exc, axis = 3)\n",
    "\n",
    "    \n",
    "    \n",
    "    def dfGetTimeFeatures(iFrom, p_dfTimeStamps):\n",
    "            df = p_dfTimeStamps.copy()\n",
    "            df['FROM_TIME_STAMP'] = df['TIME_STAMP'] + pd.DateOffset(hours=iFrom)\n",
    "            df.set_index('TIME_STAMP', inplace = True)\n",
    "            \n",
    "            df.loc[:, 'MINUTE'] = pd.to_datetime(df.loc[:, 'FROM_TIME_STAMP'],unit='s').dt.minute\n",
    "            df.loc[:, 'HOUR'] = pd.to_datetime(df.loc[:, 'FROM_TIME_STAMP'],unit='s').dt.hour\n",
    "            df.loc[:, 'DAY_OF_WEEK'] = pd.to_datetime(df.loc[:, 'FROM_TIME_STAMP'],unit='s').dt.day_of_week\n",
    "            df.loc[:, 'DAY_OF_MONTH'] = pd.to_datetime(df.loc[:, 'FROM_TIME_STAMP'],unit='s').dt.day\n",
    "\n",
    "            df.drop('FROM_TIME_STAMP', axis = 1, inplace = True)\n",
    "            \n",
    "            return df\n",
    "    \n",
    "    \n",
    "    X_TIME = []\n",
    "    for i in range(-BACKWARD_WINDOW_LENGTH, 0):\n",
    "        iFrom  = i\n",
    "        dfTimes = GetTimeFeatures(iFrom, dfTimeStamps)\n",
    "        dfTimes= dfTimes[BACKWARD_WINDOW_LENGTH * 60:]\n",
    "        dfTimes= dfTimes[:-FORWARD_WINDOW_LENGTH * 60]\n",
    "        \n",
    "        if len(X_TIME) == 0:\n",
    "            X_TIME = dfTimes.values\n",
    "        else:\n",
    "            X_TIME = np.append(X_TIME, dfTimes.values, axis = 1)\n",
    "        \n",
    "    \n",
    "    # # divide all the minutes to 60 to normalize between 0-1 (1hr contains maximum 60 1m candlesticks)\n",
    "    X = X/60\n",
    "    Y = Y/60\n",
    "    \n",
    "    # identify samples where there is no available input and output and drop such samples from both X and Y datasets.\n",
    "    ixToDrop = np.nonzero(np.sum(X, axis = (1,2,3))==0)[0]\n",
    "    ixToDrop = np.union1d(ixToDrop, np.nonzero(np.sum(Y, axis = (1,2,3))==0)[0])\n",
    "    \n",
    "    \n",
    "    X = np.delete(X, ixToDrop, axis = 0)\n",
    "    Y = np.delete(Y, ixToDrop, axis = 0)\n",
    "    X_TIME = np.delete(X_TIME, ixToDrop, axis = 0)\n",
    "\n",
    "    return X, Y, X_TIME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770b2e84-0eab-406c-a060-a66b2209a267",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TIME2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b60538e-4a8c-4592-b9fe-7567501ce5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to-be-developed... This is not a model... This is a layer...\n",
    "# We will use weather to include this layer within our model or not... (maybe an involvement coefficient between 0 and 1)\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "\n",
    "class Time2Vec(Layer):\n",
    "    def __init__(self, kernel_size=1):\n",
    "        super(Time2Vec, self).__init__(trainable=True, name='Time2VecLayer')\n",
    "        self.k = kernel_size\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # trend\n",
    "        self.wb = self.add_weight(name='wb',shape=(input_shape[1],),initializer='uniform',trainable=True)\n",
    "        self.bb = self.add_weight(name='bb',shape=(input_shape[1],),initializer='uniform',trainable=True)\n",
    "        # periodic\n",
    "        self.wa = self.add_weight(name='wa',shape=(1, input_shape[1], self.k),initializer='uniform',trainable=True)\n",
    "        self.ba = self.add_weight(name='ba',shape=(1, input_shape[1], self.k),initializer='uniform',trainable=True)\n",
    "        super(Time2Vec, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs, **kwargs):\n",
    "        bias = self.wb * inputs + self.bb\n",
    "        dp = K.dot(inputs, self.wa) + self.ba\n",
    "        wgts = K.sin(dp) # or K.cos(.)\n",
    "\n",
    "        ret = K.concatenate([K.expand_dims(bias, -1), wgts], -1)\n",
    "        ret = K.reshape(ret, (-1, inputs.shape[1]*(self.k+1)))\n",
    "        return ret\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1]*(self.k + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8504b026-173d-426d-8285-f90b89eab751",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3aafa4f-310f-46c2-bcf1-30a2eb4cd207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfX, dfY = GET_DATASETS_1(dfPrep)\n",
    "\n",
    "# # # independent component analysis\n",
    "# # df = dfX.copy()\n",
    "# # oIca = FastICA(n_components=7, random_state=0, whiten='unit-variance')\n",
    "\n",
    "# # df = pd.DataFrame(\n",
    "# #     data = oIca.fit_transform(df),\n",
    "# #     index = df.index\n",
    "# # )\n",
    "# # df = df.add_prefix('ICA_')\n",
    "# # dfX = df\n",
    "\n",
    "# # # remove outliers from X\n",
    "# # ixOutliers = dfX[(((dfX-dfX.mean())/dfX.std()).abs() > 3 ).any(axis = 1) == True].index\n",
    "# # dfX.drop(ixOutliers, inplace = True)\n",
    "\n",
    "# # # remove outliers from Y\n",
    "# # ixOutliers = dfY[(((dfY-dfY.mean())/dfY.std()).abs() > 3 ).any(axis = 1) == True].index\n",
    "# # dfY.drop(ixOutliers, inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "# # standarize input\n",
    "# oScalerInput = StandardScaler()\n",
    "# df =  dfX.copy()\n",
    "\n",
    "# df = pd.DataFrame(\n",
    "#     data = oScalerInput.fit_transform(df),\n",
    "#     columns =  df.columns,\n",
    "#     index = df.index\n",
    "# )\n",
    "\n",
    "# dfX = df.copy()\n",
    "\n",
    "\n",
    "# # standardize output\n",
    "# oScalerOutput = StandardScaler()\n",
    "# df =  dfY.copy()\n",
    "\n",
    "# df = pd.DataFrame(\n",
    "#     data = oScalerOutput.fit_transform(df),\n",
    "#     columns =  df.columns,\n",
    "#     index = df.index\n",
    "# )\n",
    "\n",
    "# dfY = df.copy()\n",
    "\n",
    "\n",
    "# # # PCA input\n",
    "# # oPca = PCA(0.90)\n",
    "# # df = dfX.copy()\n",
    "\n",
    "# # df = pd.DataFrame(\n",
    "# #     data = oPca.fit_transform(df),\n",
    "# #     index = df.index\n",
    "# # )\n",
    "# # df = df.add_prefix('PCA_')\n",
    "# # dfX = df.copy()\n",
    "\n",
    "\n",
    "# # split data to train-validation-test\n",
    "\n",
    "# c_fTrainingRatio = 0.70\n",
    "# c_fValidationRatio = 0.28\n",
    "# c_fTestRatio = 0.02\n",
    "\n",
    "# ixTrain,ixTest = train_test_split(\n",
    "#     dfX.index,\n",
    "#     test_size=1-c_fTrainingRatio,\n",
    "#     shuffle=False,\n",
    "#     random_state = 1\n",
    "# )\n",
    "\n",
    "# ixValidation,ixTest= train_test_split(\n",
    "#     ixTest,\n",
    "#     test_size=c_fTestRatio/(c_fTestRatio + c_fValidationRatio),\n",
    "#     shuffle=False,\n",
    "#     random_state = 1\n",
    "# )\n",
    "\n",
    "\n",
    "# dfX_train = dfX.loc[ixTrain]\n",
    "# dfX_validation = dfX.loc[ixValidation]\n",
    "# dfX_test = dfX.loc[ixTest]\n",
    "\n",
    "# dfY_train = dfY.loc[ixTrain]\n",
    "# dfY_validation = dfY.loc[ixValidation]\n",
    "# dfY_test = dfY.loc[ixTest]\n",
    "\n",
    "\n",
    "# # check identicality of distributions\n",
    "# df = dfX.copy()\n",
    "# dfDistAnalysis = pd.DataFrame(columns = ['FEATURE', 'TRAIN_VAL', 'TRAIN_TEST', 'VAL_TEST'])\n",
    "# for sCol in df.columns:\n",
    "#     # The null hypothesis is that the two distributions are identical\n",
    "#     # If the KS statistic is small or the p-value is high, then we cannot reject the null hypothesis in favor of the alternative.\n",
    "#     fStatsTrainVal, fPValueTrainVal = ks_2samp(\n",
    "#         df.loc[ixTrain, sCol],\n",
    "#         df.loc[ixValidation, sCol]\n",
    "#     )\n",
    "\n",
    "#     fStatsTrainTest, fPValueTrainTest = ks_2samp(\n",
    "#         df.loc[ixTrain, sCol],\n",
    "#         df.loc[ixTest, sCol]\n",
    "#     )\n",
    "\n",
    "#     fStatsValTest, fPValueValTest = ks_2samp(\n",
    "#         df.loc[ixValidation, sCol],\n",
    "#         df.loc[ixTest, sCol]\n",
    "#     )\n",
    "    \n",
    "#     dfDistAnalysis = pd.concat(\n",
    "#          [dfDistAnalysis,\n",
    "#          pd.DataFrame(data = [[sCol, fPValueTrainVal,  fPValueTrainTest, fPValueValTest]], columns = dfDistAnalysis.columns)\n",
    "#          ],\n",
    "#         ignore_index=False\n",
    "#     )\n",
    "\n",
    "# print('identicality of distributions: \\n\\n {}'.format(dfDistAnalysis))\n",
    "\n",
    "\n",
    "\n",
    "# # model development\n",
    "# X_train = dfX_train.values\n",
    "# X_validation = dfX_validation.values\n",
    "# X_test = dfX_test.values\n",
    "\n",
    "\n",
    "# Y_train = dfY_train.values\n",
    "# Y_validation = dfY_validation.values\n",
    "# Y_test = dfY_test.values\n",
    "\n",
    "\n",
    "# # compile model\n",
    "# aInput = tf.keras.Input(\n",
    "#     shape =  X_train.shape[1]\n",
    "# )\n",
    "\n",
    "# aHidden1 = tf.keras.layers.Dense(\n",
    "#     units = 300, \n",
    "#     kernel_initializer='normal',\n",
    "#     # activity_regularizer = tf.keras.regularizers.L2(0.1),\n",
    "#     use_bias = False\n",
    "# )(aInput)\n",
    "\n",
    "# aHidden1 = tf.keras.layers.BatchNormalization()(aHidden1)\n",
    "# aHidden1 = tf.keras.layers.ReLU()(aHidden1)\n",
    "# # aHidden1 = tf.keras.layers.Dropout(0.5)(aHidden1)\n",
    "\n",
    "\n",
    "# aHidden2 = aHidden1\n",
    "# # aHidden2 = tf.keras.layers.Dense(\n",
    "# #     units = 300, \n",
    "# #      kernel_initializer='normal',\n",
    "# #     activity_regularizer = tf.keras.regularizers.L2(0.1),  \n",
    "# #     use_bias = False\n",
    "# # )(aHidden1)\n",
    "\n",
    "# # aHidden2 = tf.keras.layers.BatchNormalization()(aHidden2)\n",
    "# # aHidden2 = tf.keras.layers.ReLU()(aHidden2)\n",
    "# # aHidden2 = tf.keras.layers.Dropout(0.5)(aHidden2)\n",
    "\n",
    "# aOutput = tf.keras.layers.Dense(\n",
    "#     units = Y_train.shape[1],\n",
    "#     kernel_initializer='normal',\n",
    "#     # activation = 'ReLU'\n",
    "# )(aHidden2)\n",
    "\n",
    "# oModel = tf.keras.Model(inputs=aInput, outputs=aOutput)\n",
    "\n",
    "# oLearningRateSchedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#     initial_learning_rate = 1e-03 * 2,\n",
    "#     decay_steps=100000,\n",
    "#     decay_rate=1e-02,\n",
    "#     staircase=True)\n",
    "\n",
    "\n",
    "# oOptimizer = tf.keras.optimizers.Adam(learning_rate=oLearningRateSchedule)\n",
    "\n",
    "# oModel.compile(optimizer=oOptimizer,loss= tf.keras.losses.MeanSquaredError())\n",
    "\n",
    "# tf.keras.utils.plot_model(oModel, show_shapes=True)\n",
    "\n",
    "# print(oModel.summary())\n",
    "\n",
    "\n",
    "# # fit model\n",
    "# oEarlyStop = tf.keras.callbacks.EarlyStopping(\n",
    "#     monitor = 'val_loss', \n",
    "#     mode = 'min', \n",
    "#     verbose = 0 , \n",
    "#     patience = 50, \n",
    "#     restore_best_weights = True)\n",
    "\n",
    "# oModel.fit(\n",
    "#     X_train, \n",
    "#     Y_train, \n",
    "#     epochs= 1000, \n",
    "#     batch_size=2**5, \n",
    "#     verbose=0, \n",
    "#     validation_data= (X_validation, Y_validation),\n",
    "#     # callbacks=[oEarlyStop]\n",
    "# )\n",
    "\n",
    "\n",
    "# # show epoch history\n",
    "# dfHistory = pd.DataFrame(oModel.history.history)\n",
    "\n",
    "# plt.figure(figsize = (20, 8))\n",
    "# sns.lineplot(data = dfHistory['loss'].iloc[1:], legend = True, label = 'Train')\n",
    "# sns.lineplot(data = dfHistory['val_loss'].iloc[1:], legend = True, label = 'Validation')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# # test model\n",
    "# a_Y_datasets = [Y_train, Y_validation]\n",
    "# a_X_datasets = [X_train, X_validation]\n",
    "# a_labels = ['train', 'validation'] \n",
    "\n",
    "# plt.figure(figsize = (20,8 ))\n",
    "# for X,Y, sLabel in list(zip(a_X_datasets, a_Y_datasets, a_labels)):\n",
    "#     aActual = oScalerOutput.inverse_transform(Y)\n",
    "#     aPred = oScalerOutput.inverse_transform(oModel.predict(X))\n",
    "\n",
    "#     df = pd.DataFrame(data = np.column_stack((aActual,aPred)),\n",
    "#                  columns = ['ACTUAL', 'PREDICTION']\n",
    "#                 )\n",
    "    \n",
    "#     fR2Score = round(r2_score(aActual, aPred), 1)\n",
    "#     sLabel = '{}   r2: {}'.format(sLabel,fR2Score)\n",
    "#     sns.scatterplot(data = df, x = 'ACTUAL', y = 'PREDICTION', label = sLabel)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d93e08-dd63-4a02-9461-bfc0cc522838",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# GENERATIVE ADVERSARIAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4ee8bdb-7b92-4f0e-9862-d69e2f6e3972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define the discriminator model\n",
    "# def define_discriminator(image_shape = Y[0].shape):\n",
    "#     # weight initialization\n",
    "#     init = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
    "#     # source image input\n",
    "#     in_src_image = tf.keras.layers.Input(shape=image_shape)\n",
    "#     # target image input\n",
    "#     in_target_image = tf.keras.layers.Input(shape=image_shape)\n",
    "    \n",
    "    \n",
    "#     # concatenate images channel-wise\n",
    "#     merged = tf.keras.layers.Concatenate()([in_src_image, in_target_image])\n",
    "#     # C64\n",
    "#     d = tf.keras.layers.Conv1D(64, 1, strides=1, padding='same', kernel_initializer=init)(merged)\n",
    "#     d = tf.keras.layers.LeakyReLU(alpha=0.2)(d)\n",
    "#     # C128\n",
    "#     d = tf.keras.layers.Conv1D(128, 1, strides=1, padding='same', kernel_initializer=init)(d)\n",
    "#     d = tf.keras.layers.BatchNormalization()(d)\n",
    "#     d = tf.keras.layers.LeakyReLU(alpha=0.2)(d)\n",
    "#     # C256\n",
    "#     d = tf.keras.layers.Conv1D(256, 1, strides=1, padding='same', kernel_initializer=init)(d)\n",
    "#     d = tf.keras.layers.BatchNormalization()(d)\n",
    "#     d = tf.keras.layers.LeakyReLU(alpha=0.2)(d)\n",
    "#     # C512\n",
    "#     d = tf.keras.layers.Conv1D(512, 1, strides=1, padding='same', kernel_initializer=init)(d)\n",
    "#     d = tf.keras.layers.BatchNormalization()(d)\n",
    "#     d = tf.keras.layers.LeakyReLU(alpha=0.2)(d)\n",
    "#     # second last output layer\n",
    "#     d = tf.keras.layers.Conv1D(512, 1, padding='same', kernel_initializer=init)(d)\n",
    "#     d = tf.keras.layers.BatchNormalization()(d)\n",
    "#     d = tf.keras.layers.LeakyReLU(alpha=0.2)(d)\n",
    "#     # patch output\n",
    "#     d = tf.keras.layers.Conv1D(1, 1, padding='same', kernel_initializer=init)(d)\n",
    "#     patch_out = tf.keras.layers.Activation('sigmoid')(d)\n",
    "#     # define model\n",
    "#     model = tf.keras.Model([in_src_image, in_target_image], patch_out, name = 'DISCRIMINATOR_MODEL')\n",
    "\n",
    "#     return model\n",
    "\n",
    "\n",
    "# def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "#     loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    \n",
    "#     real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
    "\n",
    "#     generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "#     total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "#     return total_disc_loss\n",
    "\n",
    "\n",
    "# # define an encoder block\n",
    "# def define_encoder_block(layer_in, n_filters, batchnorm=True):\n",
    "#     # weight initialization\n",
    "#     init = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
    "#     # add downsampling layer\n",
    "#     g = tf.keras.layers.Conv1D(n_filters, 1, strides=1, padding='same', kernel_initializer=init)(layer_in)\n",
    "#     # conditionally add batch normalization\n",
    "#     if batchnorm:\n",
    "#         g = tf.keras.layers.BatchNormalization()(g, training=True)\n",
    "#     # leaky relu activation\n",
    "#     g = tf.keras.layers.LeakyReLU(alpha=0.2)(g)\n",
    "#     return g\n",
    "\n",
    "# # define a decoder block\n",
    "# def decoder_block(layer_in, skip_in, n_filters, dropout=True):\n",
    "#     # weight initialization\n",
    "#     init = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
    "#     # add upsampling layer\n",
    "#     g = tf.keras.layers.Convolution1DTranspose(n_filters, 1, strides=1, padding='same', kernel_initializer=init)(layer_in)\n",
    "#     # add batch normalization\n",
    "#     g = tf.keras.layers.BatchNormalization()(g, training=True)\n",
    "#     # conditionally add dropout\n",
    "#     if dropout:\n",
    "#         g = tf.keras.layers.Dropout(0.5)(g, training=True)\n",
    "#     # merge with skip connection\n",
    "#     g = tf.keras.layers.Concatenate()([g, skip_in])\n",
    "#     # relu activation\n",
    "#     g = tf.keras.layers.Activation('relu')(g)\n",
    "#     return g\n",
    "\n",
    "\n",
    "# # define the standalone generator model\n",
    "# def define_generator(image_shape=X[0].shape):\n",
    "#     # weight initialization\n",
    "#     init = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
    "#     # image input\n",
    "#     in_image = tf.keras.layers.Input(shape=image_shape)\n",
    "#     # encoder model\n",
    "    \n",
    "#     e1 = define_encoder_block(in_image, 64, batchnorm=False)\n",
    "#     e2 = define_encoder_block(e1, 128)\n",
    "#     e3 = define_encoder_block(e2, 256)\n",
    "#     e4 = define_encoder_block(e3, 512)\n",
    "#     e5 = define_encoder_block(e4, 512)\n",
    "#     e6 = define_encoder_block(e5, 512)\n",
    "#     e7 = define_encoder_block(e6, 512)\n",
    "\n",
    "    \n",
    "#     # bottleneck, no batch norm and relu\n",
    "#     b = tf.keras.layers.Conv1D(512, 1, strides=1, padding='same', kernel_initializer=init)(e7)\n",
    "#     b = tf.keras.layers.Activation('relu')(b)\n",
    "    \n",
    "#     # decoder model\n",
    "#     d1 = decoder_block(b, e7, 512)\n",
    "#     d2 = decoder_block(d1, e6, 512)\n",
    "#     d3 = decoder_block(d2, e5, 512)\n",
    "#     d4 = decoder_block(d3, e4, 512, dropout=False)\n",
    "#     d5 = decoder_block(d4, e3, 256, dropout=False)\n",
    "#     d6 = decoder_block(d5, e2, 128, dropout=False)\n",
    "#     d7 = decoder_block(d6, e1, 64, dropout=False)\n",
    "    \n",
    "    \n",
    "#     # output\n",
    "#     g = tf.keras.layers.Conv1DTranspose(FORWARD_WINDOW_LENGTH, 1, strides=1, padding='same', kernel_initializer=init)(d7)\n",
    "#     out_image = tf.keras.layers.Activation('tanh')(g)\n",
    "#     # define model\n",
    "#     model = tf.keras.Model(in_image, out_image, name = 'GENERATOR_MODEL')\n",
    "#     return model\n",
    "\n",
    "\n",
    "# def generator_loss(disc_generated_output, gen_output, target):\n",
    "#     loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    \n",
    "#     gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "#     # Mean absolute error\n",
    "#     l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "\n",
    "#     total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
    "\n",
    "#     return total_gen_loss, gan_loss, l1_loss\n",
    "\n",
    "\n",
    "# oDiscriminator = define_discriminator()\n",
    "\n",
    "# oGenerator = define_generator()\n",
    "\n",
    "# generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "# discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "    \n",
    "# @tf.function\n",
    "# def train_step(input_image, target, step):\n",
    "#     with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "#         gen_output = oGenerator(input_image, training=True)\n",
    "        \n",
    "#         # disc_real_output is not so much applicable for exchange rate prediction.\n",
    "#         # because, in image recognition, templates of both input and target should match.\n",
    "#         # in time series, template of backward and forward should be different.\n",
    "#         # due to this reason, we should either generate different type of loss function.\n",
    "#         # or we should use GAN for another purpose than pix2pix.\n",
    "#         disc_real_output = oDiscriminator([input_image, target], training=True)\n",
    "#         disc_generated_output = oDiscriminator([input_image, gen_output], training=True)\n",
    "\n",
    "#         gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n",
    "#         disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
    "\n",
    "#         generator_gradients = gen_tape.gradient(gen_total_loss, generator.trainable_variables)\n",
    "#         discriminator_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "#         generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
    "#         discriminator_optimizer.apply_gradients(zip(discriminator_gradients,discriminator.trainable_variables))\n",
    "\n",
    "#         with summary_writer.as_default():\n",
    "#             tf.summary.scalar('gen_total_loss', gen_total_loss, step=step//1000)\n",
    "#             tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=step//1000)\n",
    "#             tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=step//1000)\n",
    "#             tf.summary.scalar('disc_loss', disc_loss, step=step//1000)\n",
    "\n",
    "            \n",
    "# def generate_images(model, test_input, tar):\n",
    "#     prediction = model(test_input, training=True)\n",
    "    \n",
    "\n",
    "#     display_list = [test_input[0], tar[0], prediction[0]]\n",
    "#     title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
    "\n",
    "#     for i in range(3):\n",
    "#         plt.figure(figsize=(15, 15))\n",
    "#         plt.subplot(1, 3, i+1)\n",
    "#         plt.title(title[i])\n",
    "#         # Getting the pixel values in the [0, 1] range to plot.\n",
    "#         plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "#         plt.axis('off')\n",
    "#         plt.show()\n",
    "\n",
    "            \n",
    "            \n",
    "# X, Y = GET_DATASETS_2(dfPrep)\n",
    "# # split data to train-validation-test\n",
    "# c_fTrainingRatio = 0.70\n",
    "# c_fValidationRatio = 0.28\n",
    "# c_fTestRatio = 0.02\n",
    "\n",
    "# X_train,X_test, Y_train, Y_test = train_test_split(\n",
    "#     X,Y,\n",
    "#     test_size=1-c_fTrainingRatio,\n",
    "#     shuffle=False,\n",
    "#     random_state = 1\n",
    "# )\n",
    "\n",
    "# X_validation, X_test,Y_validation, Y_test= train_test_split(\n",
    "#     X_test,Y_test,\n",
    "#     test_size=c_fTestRatio/(c_fTestRatio + c_fValidationRatio),\n",
    "#     shuffle=False,\n",
    "#     random_state = 1\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# BATCH_SIZE = 50\n",
    "\n",
    "# train_ds = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "# validation_ds = tf.data.Dataset.from_tensor_slices((X_validation, Y_validation))\n",
    "\n",
    "# example_input, example_target = next(iter(validation_ds.take(1)))\n",
    "# example_input = tf.expand_dims(example_input, axis = 0)            \n",
    "# example_target = tf.expand_dims(example_target, axis = 0)   \n",
    "\n",
    "\n",
    "# start = time.time()\n",
    "\n",
    "# i = 0\n",
    "# for (input_image, target) in train_ds.batch(BATCH_SIZE):\n",
    "\n",
    "#     if (i) % 1000 == 0:\n",
    "#         display.clear_output(wait=True)\n",
    "\n",
    "#         if i != 0:\n",
    "#             print(f'Time taken for 1000 steps: {time.time()-start:.2f} sec\\n')\n",
    "\n",
    "#         start = time.time()\n",
    "        \n",
    "#         generate_images(oGenerator, example_input, example_target)\n",
    "#         print(f\"Step: {i//1000}k\")\n",
    "\n",
    "#     # input_image = tf.expand_dims(input_image, axis = 0)            \n",
    "#     # target = tf.expand_dims(target, axis = 0)           \n",
    "    \n",
    "#     train_step(input_image, target, i)\n",
    "\n",
    "#     # Training step\n",
    "#     if (i+1) % 10 == 0:\n",
    "#         print('.', end='', flush=True)\n",
    "    \n",
    "#     i = i +1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d45e7ab-d360-4866-8918-200424b15908",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# LUONG'S ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f2cfd4-becb-4c2d-8152-0cfd9744a4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to-be-developed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52d4391-2357-455d-8cb2-d7a38d56338d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# BAHDENAU'S ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92fc9c1f-e7ca-46cc-9b12-f2600ddaa58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to-be-developed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5be55d0-9c57-45b4-816e-3e6b3afdd292",
   "metadata": {
    "tags": []
   },
   "source": [
    "# VASWANI'S ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60be1d81-fa3b-4748-bc52-d3f9801f9e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3\n",
      "-2\n",
      "-1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "sFromDate = '2022-10-15'\n",
    "dfTimeStamps = pd.DataFrame(\n",
    "    data = pd.date_range(\n",
    "        start='{} 00:00'.format(sFromDate), \n",
    "        end='2022-11-01 00:00', freq = 'min'\n",
    "    ),\n",
    "    columns  = ['TIME_STAMP']\n",
    ")\n",
    "\n",
    "dfTimeStamps.query('TIME_STAMP.dt.day_of_week not in (6,7)', inplace = True)\n",
    "dfTimeStamps.query(' 10 <= TIME_STAMP.dt.hour <= 18', inplace = True)\n",
    "dfTimeStamps.drop(dfTimeStamps.query(' TIME_STAMP.dt.hour == 18 and TIME_STAMP.dt.minute >= 30 ', inplace = False).index, inplace = True)\n",
    "dfTimeStamps.reset_index(drop = True, inplace = True)\n",
    "\n",
    "df = dfPrep.copy()\n",
    "df.query('time >= @sFromDate and EXCHANGE_RATE in (\"CBKG.DE\")', inplace = True)\n",
    "\n",
    "iNrOfExchangeRates = len(df['EXCHANGE_RATE'].unique())\n",
    "X, Y, X_TIME = GET_DATASETS_2(df, dfTimeStamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac3b301-6b63-4554-a293-bbcb302f9d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data to train-validation-test\n",
    "c_fTrainingRatio = 0.70\n",
    "c_fValidationRatio = 0.28\n",
    "c_fTestRatio = 0.02\n",
    "\n",
    "X_train,X_test, Y_train, Y_test, X_TIME_train, X_TIME_test = train_test_split(\n",
    "    X,Y,X_TIME\n",
    "    test_size=1-c_fTrainingRatio,\n",
    "    shuffle=False,\n",
    "    random_state = 1\n",
    ")\n",
    "\n",
    "X_validation, X_test,Y_validation, Y_test, X_TIME_validation, X_TIME_test = train_test_split(\n",
    "    X_test,Y_test,X_TIME_test\n",
    "    test_size=c_fTestRatio/(c_fTestRatio + c_fValidationRatio),\n",
    "    shuffle=False,\n",
    "    random_state = 1\n",
    ")\n",
    "\n",
    "X_train = X_train[:,:,:,0]\n",
    "X_validation = X_validation[:,:,:,0]\n",
    "X_test = X_test[:,:,:,0]\n",
    "\n",
    "Y_train = Y_train[:,:,:,0]\n",
    "Y_validation = Y_validation[:,:,:,0]\n",
    "Y_test = Y_test[:,:,:,0]\n",
    "\n",
    "\n",
    "# compile model\n",
    "def transformer_encoder(inputs):\n",
    "    # Normalization and Attention\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = tf.keras.layers.MultiHeadAttention(key_dim=128, num_heads=4, dropout=0.1)(x, x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = tf.keras.layers.Conv1D(filters=1, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return res\n",
    "\n",
    "\n",
    "TermInput = tf.keras.Input(\n",
    "    shape=(BACKWARD_WINDOW_LENGTH, NR_OF_PIXELS))\n",
    "\n",
    "TimeInput = tf.keras.Input(\n",
    "    shape=(BACKWARD_WINDOW_LENGTH, X_TIME_train.shape[1]))\n",
    "\n",
    "\n",
    "W = Time2Vec(1)(TimeInput)\n",
    "W = tf.keras.layers..concatenate([TermInput, W], -1)\n",
    " \n",
    "for _ in range(1):\n",
    "    W = transformer_encoder(W)\n",
    "    \n",
    "W = tf.keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(W)\n",
    "for _ in range(2):\n",
    "    W = tf.keras.layers.Dense(256)(W)\n",
    "    W = tf.keras.layers.ReLU()(W)\n",
    "    W = tf.keras.layers.Dropout(0.5)(W)\n",
    "\n",
    "W = tf.keras.layers.Flatten()(W)\n",
    "W = tf.keras.layers.Dense(FORWARD_WINDOW_LENGTH* NR_OF_PIXELS)(W)\n",
    "W = tf.keras.layers.Reshape((FORWARD_WINDOW_LENGTH, NR_OF_PIXELS))(W)\n",
    "W = tf.keras.layers.Softmax(axis = 2)(W)\n",
    "\n",
    "\n",
    "\n",
    "ModelOutput = W\n",
    "oModel = tf.keras.Model([TermInput,TimeInput], ModelOutput, name = 'TRANSFORMER_MODEL')\n",
    "\n",
    "oOptimizer = tf.keras.optimizers.Adam(learning_rate=1e-03)\n",
    "oModel.compile(\n",
    "    loss = tf.keras.losses.KLDivergence(), \n",
    "    optimizer=oOptimizer\n",
    ")\n",
    "\n",
    "# # print(oModel.summary())\n",
    "\n",
    "tf.keras.utils.plot_model(oModel, show_shapes=True)\n",
    "\n",
    "\n",
    "# fit model\n",
    "oEarlyStop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss', \n",
    "    mode = 'min', \n",
    "    verbose = 0 , \n",
    "    patience = 50, \n",
    "    restore_best_weights = True,\n",
    "    start_from_epoch=10**5\n",
    ")\n",
    "\n",
    "oModel.fit(\n",
    "    X_train, \n",
    "    Y_train, \n",
    "    epochs= 10**6, \n",
    "    batch_size=2**8, \n",
    "    verbose=1, \n",
    "    validation_data= (X_validation, Y_validation),\n",
    "    callbacks=[oEarlyStop]\n",
    ")\n",
    "\n",
    "# show epoch history\n",
    "dfHistory = pd.DataFrame(oModel.history.history)\n",
    "\n",
    "plt.figure(figsize = (20, 8))\n",
    "sns.lineplot(data = dfHistory['loss'].iloc[1:], legend = True, label = 'Train')\n",
    "sns.lineplot(data = dfHistory['val_loss'].iloc[1:], legend = True, label = 'Validation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a72d84-3a17-43ec-8f55-39d1dc8b8e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "a_Y_datasets = [Y_train, Y_validation]\n",
    "a_X_datasets = [X_train, X_validation]\n",
    "a_labels = ['train', 'validation'] \n",
    "for X_to_visualzie,Y_to_visualize, sLabel in list(zip(a_X_datasets, a_Y_datasets, a_labels)):\n",
    "    pred = oModel.predict(X_to_visualzie)\n",
    "    ground_true = Y_to_visualize\n",
    "    for i in np.random.randint(low = 0,high = len(X_to_visualzie), size = 10):      \n",
    "        \n",
    "        fig, axs = plt.subplots(2,2, figsize=(20, 4))\n",
    "        fig.suptitle('{}--{}'.format(sLabel, i))\n",
    "        \n",
    "        axs[0, 0].imshow(X_to_visualzie[i], cmap='gray')\n",
    "        axs[0, 0].set_title( 'X')\n",
    "\n",
    "        axs[0, 1].imshow(X_to_visualzie[i], cmap='gray')\n",
    "        axs[0, 1].set_title( 'X')\n",
    "\n",
    "    \n",
    "        axs[1, 0].imshow(ground_true[i], cmap='gray')\n",
    "        axs[1, 0].set_title( 'GROUND TRUE')\n",
    "\n",
    "        axs[1, 1].imshow(pred[i], cmap='gray')\n",
    "        axs[1, 1].set_title( 'PREDICTION')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "# # this model should be enough capable to identify the times when the market is closed.\n",
    "# # maybe we can add seasonal features in additon.\n",
    "# # also, it doesn't predict well.\n",
    "# # maybe a learn-to-rank model could be helpful.\n",
    "# # also implement the R2s for each time_step-pixel_bin pairs.\n",
    "\n",
    "# #     aActual = oScalerOutput.inverse_transform(Y)\n",
    "# #     aPred = oScalerOutput.inverse_transform(oModel.predict(X))\n",
    "\n",
    "# #     df = pd.DataFrame(data = np.column_stack((aActual,aPred)),\n",
    "# #                  columns = ['ACTUAL', 'PREDICTION']\n",
    "# #                 )\n",
    "    \n",
    "# #     fR2Score = round(r2_score(aActual, aPred), 1)\n",
    "# #     sLabel = '{}   r2: {}'.format(sLabel,fR2Score)\n",
    "# #     sns.scatterplot(data = df, x = 'ACTUAL', y = 'PREDICTION', label = sLabel)\n",
    "# # plt.show()\n",
    "\n",
    "\n",
    "# # # tf.keras.utils.plot_model(oModel,  show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5420d6-ce06-4394-aa5d-0216f0e57dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
