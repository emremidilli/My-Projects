{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b75722d8-39d9-461c-a8d6-d108c05944e8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6002fc64-bbf8-4326-beac-a6e229713ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################## LIBRARIES ########################################################\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import CONSTANTS as c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaeb3a4-209f-4585-96d1-ac4153ce00d9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35aef691-0e80-46de-8b08-8c4c50231593",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################## CONSTANTS ########################################################\n",
    "PREPROCESSED_DATA_PATH = c.PREPROCESSED_DATA_PATH\n",
    "FEATURE_ENGINEERED_DATA_PATH = c.FEATURE_ENGINEERED_DATA_PATH\n",
    "\n",
    "NR_OF_BOUNDARIES = c.NR_OF_BOUNDARIES\n",
    "BIN_BOUNDARIES = c.BIN_BOUNDARIES\n",
    "SIZE_OF_CHUNK = c.SIZE_OF_CHUNK\n",
    "\n",
    "DISERT_DATA_PATH = c.DISERT_DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c2f7c7-0c0b-4ca2-b3c0-e3037f94b541",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# DATA SOURCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0099ba02-5839-4a65-bb13-55a0e1a7a70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################## DATA SOURCE ########################################################\n",
    "i = 0\n",
    "for oFile in os.walk(PREPROCESSED_DATA_PATH):\n",
    "    sFolderPath = oFile[0]\n",
    "    \n",
    "    if sFolderPath != PREPROCESSED_DATA_PATH:\n",
    "        sSubFolder = sFolderPath.split('\\\\')[-1]\n",
    "        \n",
    "        sFolderToWrite = r'{}\\{}'.format(FEATURE_ENGINEERED_DATA_PATH, sSubFolder)\n",
    "        \n",
    "        if os.path.exists(sFolderToWrite) == False:\n",
    "            os.makedirs(sFolderToWrite)\n",
    "        \n",
    "        if all(elem in os.listdir(sFolderToWrite)  for elem in  ['X_ORIGINAL.npy', 'X_ORIGINAL.npy', 'X_ORIGINAL.npy']) == True:\n",
    "            continue\n",
    "\n",
    "            \n",
    "        X_ORIGINAL = np.load(r'{}\\X_ORIGINAL.npy'.format(sFolderPath))\n",
    "        X_TIME_ORIGINAL = np.load(r'{}\\X_TIME_ORIGINAL.npy'.format(sFolderPath))\n",
    "        Y_ORIGINAL = np.load(r'{}\\Y_ORIGINAL.npy'.format(sFolderPath))\n",
    "        \n",
    "        \n",
    "        if i ==0:\n",
    "            X = X_ORIGINAL\n",
    "            Y = Y_ORIGINAL\n",
    "            X_TIME = X_TIME_ORIGINAL\n",
    "        else:\n",
    "            X = np.concatenate([X, X_ORIGINAL])\n",
    "            X_TIME = np.concatenate([X_TIME, X_TIME_ORIGINAL])\n",
    "            Y = np.concatenate([Y, Y_ORIGINAL])            \n",
    "        \n",
    "        i = i +1\n",
    "        \n",
    "        if i == 3:\n",
    "            break\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "X = X[:,:,:,0]\n",
    "Y = Y[:,:,:,0]\n",
    "\n",
    "\n",
    "# revert back to prices from change_rates\n",
    "sExchangeRate = 'EURUSD'\n",
    "df = pd.read_csv(r'Data\\{}_M1_202010010001_202210312359.csv'.format(sExchangeRate), delimiter = '\\t')\n",
    "df.loc[:, 'PRICE_TIME_STAMP'] = pd.to_datetime(df['<DATE>'] + df['<TIME>'], format='%Y.%m.%d%H:%M:%S')\n",
    "df.drop(['<DATE>', '<TIME>'], axis = 1, inplace = True)\n",
    "df.loc[:, 'EXCHANGE_RATE'] = sExchangeRate\n",
    "\n",
    "\n",
    "dfTime = pd.DataFrame(\n",
    "    data = X_TIME,\n",
    "    columns = ['MINUTE', 'HOUR', 'DAY_OF_WEEK', 'DAY_OF_MONTH', 'MONTH', 'YEAR'])\n",
    "\n",
    "dfTime['TIME_STAMP'] = pd.to_datetime(\n",
    "    dfTime['YEAR'].astype(str) \\\n",
    "    + '-' + dfTime['MONTH'].astype(str) \\\n",
    "    + '-' + dfTime['DAY_OF_MONTH'].astype(str) \\\n",
    "    + ' ' + dfTime['HOUR'].astype(str) \\\n",
    "    + ':' + dfTime['MINUTE'].astype(str) , \n",
    "    format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "dfTime = dfTime[['TIME_STAMP']]\n",
    "\n",
    "\n",
    "aClosePrices = dfTime.merge(\n",
    "    right = df[['PRICE_TIME_STAMP','<CLOSE>']], \n",
    "    left_on = 'TIME_STAMP',\n",
    "    right_on = 'PRICE_TIME_STAMP',\n",
    "    how = 'inner'\n",
    ")[['<CLOSE>']].values\n",
    "aClosePrices = np.expand_dims(aClosePrices, 2)\n",
    "\n",
    "\n",
    "X = (X * aClosePrices) + aClosePrices\n",
    "Y = (Y * aClosePrices) + aClosePrices\n",
    "\n",
    "X = X.reshape(X.shape[0] , -1)\n",
    "Y = Y.reshape(Y.shape[0] , -1)\n",
    "\n",
    "\n",
    "# XTIME consists of minute, hour, day of week, day of month, month, year\n",
    "# exclude month and year.\n",
    "X_TIME = X_TIME[:, :4]\n",
    "\n",
    "X_SOURCE = X.copy()\n",
    "Y_SOURCE = Y.copy()\n",
    "X_TIME_SOURCE = X_TIME.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28b1dff-2750-49e3-9c19-e511a23f99bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6230aa4f-4826-4460-8f73-79ef21a3c840",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################## ANALYSIS ########################################################\n",
    "import tensorflow as tf\n",
    "\n",
    "class DisERT(tf.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        p_bin_boundaries,\n",
    "        p_size_of_chunk\n",
    "    ):\n",
    "        super(DisERT, self).__init__()\n",
    "        \n",
    "        self.size_of_chunk = p_size_of_chunk\n",
    "        \n",
    "        aLayers = [\n",
    "            tf.keras.layers.Reshape((-1, p_size_of_chunk)),\n",
    "            tf.keras.layers.TimeDistributed(\n",
    "                tf.keras.layers.Discretization(\n",
    "                    bin_boundaries = p_bin_boundaries,\n",
    "                    output_mode = 'count'\n",
    "                )\n",
    "            ),\n",
    "            tf.keras.layers.Rescaling(scale = 1/p_size_of_chunk) # could be applied softmax as well.\n",
    "        ]\n",
    "        \n",
    "        self.InputBinDistributionByChunks = tf.keras.Sequential(aLayers)       \n",
    "\n",
    "\n",
    "    # x should be in format (nr_of_samples, time_step, nr_of_features)\n",
    "    def mask(self, x, p_masking_rate = 0.20, p_mask_value = 0.555):\n",
    "        # mask only time steps...\n",
    "        mask = np.random.rand(x.shape[0],x.shape[1]) < p_masking_rate\n",
    "\n",
    "        # Set targets to -1 by default, it means ignore\n",
    "        y_mlm =  -1 * np.ones(x.shape)\n",
    "\n",
    "        # Set labels for masked tokens\n",
    "        y_mlm[mask] = x[mask]\n",
    "        \n",
    "        # set masked mlm_input\n",
    "        x_mlm = np.copy(x)\n",
    "        x_mlm[mask] =  0.555 * np.ones(x.shape[2])\n",
    "        \n",
    "        return x_mlm, y_mlm\n",
    "    \n",
    "    \n",
    "    # x: represents the chunked values with the shape (nr_of_samples, time_steps (chunks), feature_size )\n",
    "    # y: represents the class for next sentence prediction with the shape (nr_of_sampples, 1)\n",
    "    def inject_noise(self, x, y):\n",
    "        # we will roll the bins\n",
    "        x_backward = x[:, :self.backward_nr_of_chunks,:]\n",
    "        x_forward = x[:, self.backward_nr_of_chunks:,:]\n",
    "        \n",
    "        x_forward_noise = tf.random.shuffle(x_forward)\n",
    "        \n",
    "        # if still same forecast chunks comes after shuffling (even thogh small percentage), we are going to set nsp class to 1.\n",
    "        y_forward_noise = tf.cast(tf.math.reduce_all(x_forward_noise == x_forward, axis = (1, 2)), dtype = tf.dtypes.float64)\n",
    "        \n",
    "        y_noise = tf.expand_dims(y_forward_noise,  axis = 1)\n",
    "        \n",
    "        x_noise = tf.keras.layers.concatenate([x_backward,x_forward_noise ], axis = 1)\n",
    "        \n",
    "        y = tf.keras.layers.concatenate([y, y_noise], axis = 0)\n",
    "        x = tf.keras.layers.concatenate([x, x_noise], axis = 0)\n",
    "                \n",
    "        return x, y\n",
    "    \n",
    "    \n",
    "    def shuffle(self, x_mlm, y_mlm, x_nsp, y_nsp):\n",
    "        indices = tf.range(start=0, limit=tf.shape(x_mlm)[0], dtype=tf.int32)\n",
    "        \n",
    "        idx = tf.random.shuffle(indices)\n",
    "        \n",
    "        x_mlm = tf.gather(x_mlm, idx)\n",
    "        y_mlm = tf.gather(y_mlm, idx)\n",
    "        x_nsp = tf.gather(x_nsp, idx)\n",
    "        y_nsp = tf.gather(y_nsp, idx)\n",
    "        \n",
    "        \n",
    "        return x_mlm, y_mlm, x_nsp, y_nsp\n",
    "        \n",
    "    # x: represents the lagged backcast values with the shape (nr_of_samples, time_steps) \n",
    "    # y: represents the lagged forecast values with the shape (nr_of_samples, time_steps) \n",
    "    def preprocess(self, x, y):\n",
    "        \n",
    "        self.backward_time_steps = x.shape[1]\n",
    "        self.forward_time_steps = y.shape[1]\n",
    "        \n",
    "        self.backward_nr_of_chunks = int(self.backward_time_steps/self.size_of_chunk)\n",
    "        self.forward_nr_of_chunks = int(self.forward_time_steps/self.size_of_chunk)\n",
    "        \n",
    "        # for masked language model, both x and y are concatted.\n",
    "        x_mlm = tf.keras.layers.concatenate(\n",
    "            [x, y], axis = 1\n",
    "        )\n",
    "        \n",
    "        x_mlm = self.InputBinDistributionByChunks(x_mlm)\n",
    "        \n",
    "        y_nsp = tf.ones([x_mlm.shape[0], 1])\n",
    "        \n",
    "        x_mlm, y_nsp = self.inject_noise(x_mlm, y_nsp)\n",
    "        \n",
    "        x_mlm, y_mlm = self.mask(x_mlm)\n",
    "        \n",
    "        x_mlm = tf.convert_to_tensor(x_mlm)\n",
    "        y_mlm = tf.convert_to_tensor(y_mlm)\n",
    "\n",
    "        x_nsp = tf.ones([x_mlm.shape[0], 1, x_mlm.shape[2]]) * 0.5 # 0.5 acts as CLS\n",
    "        \n",
    "        x_mlm, y_mlm, x_nsp, y_nsp = self.shuffle(x_mlm, y_mlm, x_nsp, y_nsp)\n",
    "        \n",
    "            \n",
    "        return x_mlm, y_mlm, x_nsp, y_nsp\n",
    "\n",
    "\n",
    "X = X_SOURCE.copy()\n",
    "Y =  Y_SOURCE.copy()\n",
    "\n",
    "# Limits of BIN_BOUNDARIES should be calculated dynamically. Otherwise, it creates too sparse input dataset which may not be good for learning.\n",
    "# Idea #1: to use daily minimum and maximum...\n",
    "# Idea #2: Keep it sparse however, apply FFT to get differences...\n",
    "# Idea #3: We can have several more BIN_BOUNDARIES variables...\n",
    "# I think, Idea #1 is more secure... However, we need to build higher scale logic...\n",
    "# Higher scale can be part of any X_TIME feature.\n",
    "# However, we must keep, NR_OF_BOUNDARIES constant\n",
    "oDisERT = DisERT(\n",
    "        p_bin_boundaries = BIN_BOUNDARIES,\n",
    "        p_size_of_chunk = SIZE_OF_CHUNK\n",
    ")\n",
    "X_MLM, Y_MLM, X_NSP, Y_NSP = oDisERT.preprocess(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcc9aa9-1ef3-47ed-9a06-16d305b133ab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88e9302a-8248-495b-b1cb-8fee3515c41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(DISERT_DATA_PATH) == False:\n",
    "    os.makedirs(DISERT_DATA_PATH)\n",
    "    \n",
    "np.save(f'{DISERT_DATA_PATH}\\X_MLM.npy', X_MLM)\n",
    "np.save(f'{DISERT_DATA_PATH}\\Y_MLM.npy', Y_MLM)\n",
    "np.save(f'{DISERT_DATA_PATH}\\X_NSP.npy', X_NSP)\n",
    "np.save(f'{DISERT_DATA_PATH}\\Y_NSP.npy', Y_NSP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
