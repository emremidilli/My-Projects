{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0611acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import MetaTrader5 as mt5\n",
    "import pytz\n",
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, max_error, median_absolute_error, r2_score, explained_variance_score\n",
    "from Optimize_Portfolio import PortfolioManagement\n",
    "from Long_Short_Term_Memory import Long_Short_Term_Memory\n",
    "from Neural_Attention_Mechanism import Neural_Attention_Mechanism\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import ta\n",
    "from scipy.stats import zscore\n",
    "import numpy as np\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c6fc32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_o_TIME_ZONE = pytz.timezone(\"Etc/UTC\")\n",
    "gc_dt_FROM = datetime(2021, 1, 1, tzinfo=gc_o_TIME_ZONE)\n",
    "gc_dt_TO = datetime(2021, 9, 22, tzinfo=gc_o_TIME_ZONE)\n",
    "gc_dt_SIMULATION_MODEL_FROM = \"2021-09-01 00:00:00\"\n",
    "\n",
    "\n",
    "gc_a_SYMBOLS = []\n",
    "\n",
    "\n",
    "gc_i_BACKWARD_TIME_WINDOW = -1\n",
    "gc_i_FORWARD_TIME_WINDOW = 5\n",
    "\n",
    "\n",
    "gc_dec_TRAINING_RATIO = 0.6\n",
    "gc_dec_VALIDATION_RATIO = 0.2\n",
    "gc_dec_TEST_RATIO = 0.2\n",
    "\n",
    "\n",
    "gc_dec_MAX_RISK_RMSE = 0.10\n",
    "gc_dec_INITIAL_BALANCE = 1000\n",
    "\n",
    "\n",
    "g_aBackwardTimeSteps = range(gc_i_BACKWARD_TIME_WINDOW, 0)\n",
    "g_aForwardTimeSteps = range(0, gc_i_FORWARD_TIME_WINDOW)\n",
    "\n",
    "gc_i_PERIODS_OF_CLASSES = 5\n",
    "\n",
    "g_aInputFeatures = set(['open', 'high', 'low', 'close', 'spread' ,'tick_volume'])\n",
    "g_aOutputFeatures = pd.IntervalIndex.from_breaks(st.norm.ppf(np.linspace(0, 1, gc_i_PERIODS_OF_CLASSES+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4727b224",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def ConvertSpreadValues(dfRates, aSymbolInfo):\n",
    "    iDigits = aSymbolInfo.digits\n",
    "    dfRates['spread'] = dfRates['spread'] * pow(10, -iDigits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "041e482c",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def dfShiftTimeSteps(dfRates, aTimeSteps):\n",
    "    \n",
    "    lstColumnNames = list([])\n",
    "    for iTimeStep in aTimeSteps:\n",
    "        for tplCol in dfRates.columns:\n",
    "            lstColumnNames.append((iTimeStep, ) + tplCol)\n",
    "    \n",
    "    \n",
    "    lstIndexNames = (\"Time Step\",) +  tuple(dfRates.columns.names)\n",
    "    \n",
    "    dicColumnIndices = pd.MultiIndex.from_tuples(\n",
    "        lstColumnNames,\n",
    "        names = lstIndexNames\n",
    "        )\n",
    "\n",
    "\n",
    "    dfShiftedRates = pd.DataFrame(\n",
    "        columns=dicColumnIndices, \n",
    "        index=dfRates.index)\n",
    "    \n",
    "    \n",
    "\n",
    "    for i in aTimeSteps:\n",
    "        dfShiftedRates[i] = dfRates.shift(-i)\n",
    "\n",
    "    dfShiftedRates.dropna(inplace=True)\n",
    "\n",
    "    return dfShiftedRates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bad524f3",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def dfGetMarketData(sSymbol):\n",
    "\n",
    "    if not mt5.initialize():\n",
    "        print(\"initialize() failed, error code =\", mt5.last_error())\n",
    "        sys.exit()\n",
    "\n",
    "    aSymbolInfo = mt5.symbol_info(sSymbol)\n",
    "    if not aSymbolInfo:\n",
    "        print(\"symbol_info() failed, error code =\", mt5.last_error())\n",
    "        sys.exit()\n",
    "\n",
    "    aRates = mt5.copy_rates_range(\n",
    "        sSymbol, mt5.TIMEFRAME_M30, gc_dt_FROM, gc_dt_TO)\n",
    "    if len(aRates) == 0:\n",
    "        print(\"copy_rates_range() failed, error code =\", mt5.last_error())\n",
    "        sys.exit()\n",
    "\n",
    "    mt5.shutdown()\n",
    "\n",
    "    dfRates = pd.DataFrame(aRates)\n",
    "\n",
    "    dfRates['time'] = pd.to_datetime(dfRates['time'], unit='s')\n",
    "    dfRates.set_index('time', inplace=True)\n",
    "    dfRates.drop('real_volume', axis=1, inplace=True)\n",
    "\n",
    "    ConvertSpreadValues(dfRates, aSymbolInfo)\n",
    "    AddSeasonalFeatures(dfRates)\n",
    "    AddReturns(dfRates)\n",
    "    dfRates = dfAddTechnicalIndicators(dfRates)\n",
    "\n",
    "    dfRates.columns  = pd.MultiIndex.from_product(\n",
    "        [[sSymbol], dfRates.columns], \n",
    "        names=[\"Time Series\", \"Feature\"])\n",
    "    \n",
    "    \n",
    "    return dfRates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d9f70b2",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def dfAddTechnicalIndicators(dfRates):\n",
    "    global g_aInputFeatures \n",
    "    \n",
    "    \n",
    "    iTimeWindow = 24\n",
    "    \n",
    "    dfHigh = dfRates[\"high\"]\n",
    "    dfLow = dfRates[\"low\"]\n",
    "    dfClose = dfRates[\"close\"]\n",
    "    \n",
    "    # Average Dricetional Movement Index\n",
    "    oAdx = ta.trend.ADXIndicator(dfHigh, dfLow, dfClose, iTimeWindow, False)\n",
    "    \n",
    "    dfAdx = oAdx.adx()\n",
    "    dfAdx.drop(dfAdx[dfAdx == 0].index, inplace = True)\n",
    "    g_aInputFeatures.add(dfAdx.name)\n",
    "    \n",
    "    dfAdxNeg = oAdx.adx_neg()\n",
    "    dfAdxNeg.drop(dfAdxNeg[dfAdxNeg == 0].index, inplace = True)\n",
    "    g_aInputFeatures.add(dfAdxNeg.name)\n",
    "    \n",
    "    \n",
    "    dfAdxPos = oAdx.adx_pos()\n",
    "    dfAdxPos.drop(dfAdxPos[dfAdxPos == 0].index, inplace = True)\n",
    "    g_aInputFeatures.add(dfAdxPos.name)\n",
    "    \n",
    "    \n",
    "    dfRates = dfRates.join(dfAdx, how = \"inner\")\n",
    "    dfRates = dfRates.join(dfAdxNeg, how = \"inner\")\n",
    "    dfRates = dfRates.join(dfAdxPos, how = \"inner\")\n",
    "\n",
    "    \n",
    "    # Aroon Indicator\n",
    "    oAroon = ta.trend.AroonIndicator(dfClose, iTimeWindow, False)\n",
    "    dfAroonDown = oAroon.aroon_down()\n",
    "    dfAroonDown.dropna(inplace = True)\n",
    "    g_aInputFeatures.add(dfAroonDown.name)\n",
    "    \n",
    "    dfAroonIndicator = oAroon.aroon_indicator() \n",
    "    dfAroonIndicator.dropna(inplace = True)\n",
    "    g_aInputFeatures.add(dfAroonIndicator.name)\n",
    "\n",
    "    \n",
    "    dfAroonUp = oAroon.aroon_up()\n",
    "    dfAroonUp.dropna(inplace = True)\n",
    "    g_aInputFeatures.add(dfAroonUp.name)\n",
    "    \n",
    "    dfRates = dfRates.join(dfAroonDown, how = \"inner\")\n",
    "    dfRates = dfRates.join(dfAroonIndicator, how = \"inner\")\n",
    "    dfRates = dfRates.join(dfAroonUp, how = \"inner\")\n",
    "    \n",
    "    \n",
    "    # Commodity Channel Index\n",
    "    oCci = ta.trend.CCIIndicator(dfHigh, dfLow,dfClose, iTimeWindow)\n",
    "    dfCci = oCci.cci()\n",
    "    dfCci.dropna(inplace = True)\n",
    "    g_aInputFeatures.add(dfCci.name)\n",
    "    \n",
    "    dfRates = dfRates.join(dfCci, how = \"inner\")\n",
    "    \n",
    "    \n",
    "    # Detrended Price Oscillator (DPO)\n",
    "    oDpo = ta.trend.DPOIndicator(dfClose, iTimeWindow)\n",
    "    dfDpo = oDpo.dpo()\n",
    "    dfDpo.dropna(inplace = True)\n",
    "    g_aInputFeatures.add(dfDpo.name)    \n",
    "    \n",
    "    dfRates = dfRates.join(dfDpo, how = \"inner\")\n",
    "    \n",
    "    \n",
    "    # EMA - Exponential Moving Average\n",
    "    oEma = ta.trend.EMAIndicator(dfClose, iTimeWindow)\n",
    "    dfEma = oEma.ema_indicator()\n",
    "    dfEma.dropna(inplace = True)\n",
    "    g_aInputFeatures.add(dfEma.name)\n",
    "    \n",
    "    dfRates = dfRates.join(dfEma, how = \"inner\")\n",
    "    \n",
    "    \n",
    "    return dfRates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f89a65d2",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def AddSeasonalFeatures(dfRates):\n",
    "    global g_aInputFeatures \n",
    "    \n",
    "    c_a_SEASONAL_FEATURES = [\"year\", \"month\", \"day\", \"dayofweek\", \"hour\"]\n",
    "    for sSeasonalFeature in c_a_SEASONAL_FEATURES:\n",
    "        exec(\"dfRates[sSeasonalFeature] = dfRates.index.\" + sSeasonalFeature)\n",
    "        g_aInputFeatures.add(sSeasonalFeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17475e81",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def AddReturns(dfRates):\n",
    "    dfRates[\"return\"] = (dfRates[\"open\"] - dfRates[\"close\"])/dfRates[\"open\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e680ce9d",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def dfOversampleImbalancedData(dfX, dfY):\n",
    "    \n",
    "#     oOversample = SMOTE()\n",
    "#     aX, aY = oOversample.fit_resample(dfX.values, dfY.values)\n",
    "    \n",
    "#     dfX = pd.DataFrame(data = aX, columns = dfX.columns)\n",
    "#     dfY = pd.DataFrame(data = aY, columns = dfY.columns)\n",
    "    \n",
    "    dfXCopy = dfX.copy()\n",
    "    dfYCopy = dfY.copy()\n",
    "        \n",
    "    dfCombinations = dfYCopy.astype(str).agg('-'.join, axis=1)\n",
    "    dfCombinationsStats = dfCombinations.value_counts()\n",
    "    dfCombinationsStats = pd.DataFrame(dfCombinationsStats).reset_index()\n",
    "    \n",
    "    \n",
    "    iMaxAmount = dfCombinationsStats.iloc[0,1]\n",
    "    for i in range(1, len(dfCombinationsStats) ):\n",
    "        \n",
    "        sCombination = dfCombinationsStats.iloc[i, 0]\n",
    "        iSamplesNeeded = iMaxAmount - dfCombinationsStats.iloc[i, 1]\n",
    "        \n",
    "        dfSampledIndex =  dfCombinations[dfCombinations == sCombination].sample(iSamplesNeeded, replace = True).index\n",
    "        \n",
    "        dfSampledX = dfXCopy.loc[dfSampledIndex]\n",
    "        dfSampledY = dfYCopy.loc[dfSampledIndex]\n",
    "        \n",
    "    \n",
    "        dfX = dfX.append(dfSampledX , ignore_index= True)\n",
    "        dfY = dfY.append(dfSampledY , ignore_index= True)\n",
    "        \n",
    "    \n",
    "    dfX,dfY = shuffle(dfX,dfY )\n",
    "    \n",
    "\n",
    "    return dfX, dfY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "682253c6",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def dfSplitData(dfInput, dfOutput):\n",
    "    dfInputTrainValidation, dfInputTest, dfOutputTrainValidation, dfOutputTest = train_test_split(\n",
    "        dfInput,\n",
    "        dfOutput,\n",
    "        test_size=gc_dec_TEST_RATIO,\n",
    "        shuffle=False)\n",
    "\n",
    "    dfInputTrain, dfInputValidation, dfOutputTrain, dfOutputValidation = train_test_split(\n",
    "        dfInputTrainValidation,\n",
    "        dfOutputTrainValidation,\n",
    "        test_size=(1/(1 -gc_dec_TEST_RATIO))-1,\n",
    "        shuffle=False)\n",
    "    \n",
    "    \n",
    "    dfInputTrain = dfInputTrain.astype(float)\n",
    "    dfInputValidation = dfInputValidation.astype(float)\n",
    "    dfInputTest = dfInputTest.astype(float)\n",
    "    dfOutputTrain = dfOutputTrain.astype(float)\n",
    "    dfOutputValidation = dfOutputValidation.astype(float)\n",
    "    dfOutputTest = dfOutputTest.astype(float)\n",
    "    \n",
    "    return dfInputTrain, dfInputValidation, dfInputTest, dfOutputTrain, dfOutputValidation, dfOutputTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca3eba12",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def dfScaleData(sScalerName,dfTrain,dfValidation, dfTest):\n",
    "    sScalersDirectory = os.path.join(sSubModelName , \"__scalers__\")\n",
    "\n",
    "    oScaler = StandardScaler()\n",
    "\n",
    "    oScaler.fit(dfTrain)\n",
    "\n",
    "    aScaledTrain = oScaler.transform(dfTrain)\n",
    "    aScaledValidation = oScaler.transform(dfValidation)\n",
    "    aScaledTest = oScaler.transform(dfTest)\n",
    "\n",
    "    dfScaledTrain = pd.DataFrame(aScaledTrain, columns = dfTrain.columns, index = dfTrain.index)\n",
    "    dfScaledValidation = pd.DataFrame(aScaledValidation, columns = dfValidation.columns, index = dfValidation.index)\n",
    "    dfScaledTest = pd.DataFrame(aScaledTest, columns = dfTest.columns, index = dfTest.index)\n",
    "\n",
    "    sScalerFilePath =os.path.join(sScalersDirectory, sScalerName + \".sav\")\n",
    "    os.makedirs(os.path.dirname(sScalerFilePath), exist_ok=True)\n",
    "    pickle.dump(oScaler, open(sScalerFilePath, 'wb'))\n",
    "    \n",
    "    \n",
    "    return dfScaledTrain, dfScaledValidation, dfScaledTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a44508f4",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "sSymbol = \"NAT.GAS\"\n",
    "aRelevantSymbols = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad7ce049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yunus\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ta\\trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "C:\\Users\\yunus\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ta\\trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n"
     ]
    }
   ],
   "source": [
    "dfRates = dfGetMarketData(sSymbol)\n",
    "\n",
    "for sRelevantSymbol in aRelevantSymbols:\n",
    "    dfRelevantRates = dfGetMarketData(sRelevantSymbol)\n",
    "    dfRates = dfRates.join(dfRelevantRates, how = \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fa7f436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Time Series</th>\n",
       "      <th colspan=\"21\" halign=\"left\">NAT.GAS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>tick_volume</th>\n",
       "      <th>spread</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>...</th>\n",
       "      <th>return</th>\n",
       "      <th>adx</th>\n",
       "      <th>adx_neg</th>\n",
       "      <th>adx_pos</th>\n",
       "      <th>aroon_down_24</th>\n",
       "      <th>aroon_ind_24</th>\n",
       "      <th>aroon_up_24</th>\n",
       "      <th>cci</th>\n",
       "      <th>dpo_24</th>\n",
       "      <th>ema_24</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-01-05 01:30:00</th>\n",
       "      <td>2.582</td>\n",
       "      <td>2.589</td>\n",
       "      <td>2.582</td>\n",
       "      <td>2.588</td>\n",
       "      <td>148</td>\n",
       "      <td>0.010</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002324</td>\n",
       "      <td>18.732942</td>\n",
       "      <td>22.124691</td>\n",
       "      <td>16.667519</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>-41.666667</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>-27.331487</td>\n",
       "      <td>-0.006958</td>\n",
       "      <td>2.590461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-05 02:00:00</th>\n",
       "      <td>2.588</td>\n",
       "      <td>2.589</td>\n",
       "      <td>2.583</td>\n",
       "      <td>2.587</td>\n",
       "      <td>153</td>\n",
       "      <td>0.010</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>18.538557</td>\n",
       "      <td>21.748555</td>\n",
       "      <td>16.384159</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>-41.666667</td>\n",
       "      <td>20.833333</td>\n",
       "      <td>-24.385511</td>\n",
       "      <td>-0.013083</td>\n",
       "      <td>2.590184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-05 02:30:00</th>\n",
       "      <td>2.586</td>\n",
       "      <td>2.592</td>\n",
       "      <td>2.585</td>\n",
       "      <td>2.588</td>\n",
       "      <td>129</td>\n",
       "      <td>0.011</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000773</td>\n",
       "      <td>18.244230</td>\n",
       "      <td>21.307561</td>\n",
       "      <td>16.920949</td>\n",
       "      <td>58.333333</td>\n",
       "      <td>-41.666667</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>-12.569691</td>\n",
       "      <td>-0.011458</td>\n",
       "      <td>2.590010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-05 03:00:00</th>\n",
       "      <td>2.589</td>\n",
       "      <td>2.602</td>\n",
       "      <td>2.588</td>\n",
       "      <td>2.602</td>\n",
       "      <td>270</td>\n",
       "      <td>0.011</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005021</td>\n",
       "      <td>17.621825</td>\n",
       "      <td>20.442495</td>\n",
       "      <td>19.133904</td>\n",
       "      <td>54.166667</td>\n",
       "      <td>-41.666667</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>29.306931</td>\n",
       "      <td>-0.013125</td>\n",
       "      <td>2.590969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-05 03:30:00</th>\n",
       "      <td>2.601</td>\n",
       "      <td>2.601</td>\n",
       "      <td>2.595</td>\n",
       "      <td>2.599</td>\n",
       "      <td>136</td>\n",
       "      <td>0.011</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000769</td>\n",
       "      <td>17.025352</td>\n",
       "      <td>20.018462</td>\n",
       "      <td>18.737015</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>-41.666667</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>31.773597</td>\n",
       "      <td>-0.006375</td>\n",
       "      <td>2.591611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-21 21:30:00</th>\n",
       "      <td>4.843</td>\n",
       "      <td>4.864</td>\n",
       "      <td>4.840</td>\n",
       "      <td>4.849</td>\n",
       "      <td>799</td>\n",
       "      <td>0.005</td>\n",
       "      <td>2021</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001239</td>\n",
       "      <td>21.460824</td>\n",
       "      <td>27.739579</td>\n",
       "      <td>16.741784</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>-45.833333</td>\n",
       "      <td>29.166667</td>\n",
       "      <td>-50.402653</td>\n",
       "      <td>-0.000917</td>\n",
       "      <td>4.889529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-21 22:00:00</th>\n",
       "      <td>4.849</td>\n",
       "      <td>4.885</td>\n",
       "      <td>4.848</td>\n",
       "      <td>4.879</td>\n",
       "      <td>671</td>\n",
       "      <td>0.005</td>\n",
       "      <td>2021</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006187</td>\n",
       "      <td>21.341199</td>\n",
       "      <td>26.658781</td>\n",
       "      <td>18.300860</td>\n",
       "      <td>70.833333</td>\n",
       "      <td>-45.833333</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>-31.879393</td>\n",
       "      <td>0.043042</td>\n",
       "      <td>4.888687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-21 22:30:00</th>\n",
       "      <td>4.879</td>\n",
       "      <td>4.888</td>\n",
       "      <td>4.862</td>\n",
       "      <td>4.872</td>\n",
       "      <td>679</td>\n",
       "      <td>0.005</td>\n",
       "      <td>2021</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001435</td>\n",
       "      <td>21.190593</td>\n",
       "      <td>25.918313</td>\n",
       "      <td>18.113029</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>-45.833333</td>\n",
       "      <td>20.833333</td>\n",
       "      <td>-26.149548</td>\n",
       "      <td>-0.006917</td>\n",
       "      <td>4.887352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-21 23:00:00</th>\n",
       "      <td>4.872</td>\n",
       "      <td>4.873</td>\n",
       "      <td>4.849</td>\n",
       "      <td>4.855</td>\n",
       "      <td>409</td>\n",
       "      <td>0.010</td>\n",
       "      <td>2021</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003489</td>\n",
       "      <td>21.155493</td>\n",
       "      <td>26.654375</td>\n",
       "      <td>17.641062</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>-45.833333</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>-34.438907</td>\n",
       "      <td>-0.007458</td>\n",
       "      <td>4.884764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-21 23:30:00</th>\n",
       "      <td>4.855</td>\n",
       "      <td>4.857</td>\n",
       "      <td>4.828</td>\n",
       "      <td>4.832</td>\n",
       "      <td>230</td>\n",
       "      <td>0.010</td>\n",
       "      <td>2021</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004737</td>\n",
       "      <td>21.291022</td>\n",
       "      <td>28.109943</td>\n",
       "      <td>17.079914</td>\n",
       "      <td>58.333333</td>\n",
       "      <td>-45.833333</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>-47.183227</td>\n",
       "      <td>-0.086042</td>\n",
       "      <td>4.880543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8464 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Time Series         NAT.GAS                                                \\\n",
       "Feature                open   high    low  close tick_volume spread  year   \n",
       "time                                                                        \n",
       "2021-01-05 01:30:00   2.582  2.589  2.582  2.588         148  0.010  2021   \n",
       "2021-01-05 02:00:00   2.588  2.589  2.583  2.587         153  0.010  2021   \n",
       "2021-01-05 02:30:00   2.586  2.592  2.585  2.588         129  0.011  2021   \n",
       "2021-01-05 03:00:00   2.589  2.602  2.588  2.602         270  0.011  2021   \n",
       "2021-01-05 03:30:00   2.601  2.601  2.595  2.599         136  0.011  2021   \n",
       "...                     ...    ...    ...    ...         ...    ...   ...   \n",
       "2021-09-21 21:30:00   4.843  4.864  4.840  4.849         799  0.005  2021   \n",
       "2021-09-21 22:00:00   4.849  4.885  4.848  4.879         671  0.005  2021   \n",
       "2021-09-21 22:30:00   4.879  4.888  4.862  4.872         679  0.005  2021   \n",
       "2021-09-21 23:00:00   4.872  4.873  4.849  4.855         409  0.010  2021   \n",
       "2021-09-21 23:30:00   4.855  4.857  4.828  4.832         230  0.010  2021   \n",
       "\n",
       "Time Series                              ...                                  \\\n",
       "Feature             month day dayofweek  ...    return        adx    adx_neg   \n",
       "time                                     ...                                   \n",
       "2021-01-05 01:30:00     1   5         1  ... -0.002324  18.732942  22.124691   \n",
       "2021-01-05 02:00:00     1   5         1  ...  0.000386  18.538557  21.748555   \n",
       "2021-01-05 02:30:00     1   5         1  ... -0.000773  18.244230  21.307561   \n",
       "2021-01-05 03:00:00     1   5         1  ... -0.005021  17.621825  20.442495   \n",
       "2021-01-05 03:30:00     1   5         1  ...  0.000769  17.025352  20.018462   \n",
       "...                   ...  ..       ...  ...       ...        ...        ...   \n",
       "2021-09-21 21:30:00     9  21         1  ... -0.001239  21.460824  27.739579   \n",
       "2021-09-21 22:00:00     9  21         1  ... -0.006187  21.341199  26.658781   \n",
       "2021-09-21 22:30:00     9  21         1  ...  0.001435  21.190593  25.918313   \n",
       "2021-09-21 23:00:00     9  21         1  ...  0.003489  21.155493  26.654375   \n",
       "2021-09-21 23:30:00     9  21         1  ...  0.004737  21.291022  28.109943   \n",
       "\n",
       "Time Series                                                            \\\n",
       "Feature                adx_pos aroon_down_24 aroon_ind_24 aroon_up_24   \n",
       "time                                                                    \n",
       "2021-01-05 01:30:00  16.667519     66.666667   -41.666667   25.000000   \n",
       "2021-01-05 02:00:00  16.384159     62.500000   -41.666667   20.833333   \n",
       "2021-01-05 02:30:00  16.920949     58.333333   -41.666667   16.666667   \n",
       "2021-01-05 03:00:00  19.133904     54.166667   -41.666667   12.500000   \n",
       "2021-01-05 03:30:00  18.737015     50.000000   -41.666667    8.333333   \n",
       "...                        ...           ...          ...         ...   \n",
       "2021-09-21 21:30:00  16.741784     75.000000   -45.833333   29.166667   \n",
       "2021-09-21 22:00:00  18.300860     70.833333   -45.833333   25.000000   \n",
       "2021-09-21 22:30:00  18.113029     66.666667   -45.833333   20.833333   \n",
       "2021-09-21 23:00:00  17.641062     62.500000   -45.833333   16.666667   \n",
       "2021-09-21 23:30:00  17.079914     58.333333   -45.833333   12.500000   \n",
       "\n",
       "Time Series                                         \n",
       "Feature                    cci    dpo_24    ema_24  \n",
       "time                                                \n",
       "2021-01-05 01:30:00 -27.331487 -0.006958  2.590461  \n",
       "2021-01-05 02:00:00 -24.385511 -0.013083  2.590184  \n",
       "2021-01-05 02:30:00 -12.569691 -0.011458  2.590010  \n",
       "2021-01-05 03:00:00  29.306931 -0.013125  2.590969  \n",
       "2021-01-05 03:30:00  31.773597 -0.006375  2.591611  \n",
       "...                        ...       ...       ...  \n",
       "2021-09-21 21:30:00 -50.402653 -0.000917  4.889529  \n",
       "2021-09-21 22:00:00 -31.879393  0.043042  4.888687  \n",
       "2021-09-21 22:30:00 -26.149548 -0.006917  4.887352  \n",
       "2021-09-21 23:00:00 -34.438907 -0.007458  4.884764  \n",
       "2021-09-21 23:30:00 -47.183227 -0.086042  4.880543  \n",
       "\n",
       "[8464 rows x 21 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfRates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d458f828",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInput  = dfRates.loc[:, dfRates.columns.get_level_values(1).isin(g_aInputFeatures)]\n",
    "dfInput = dfShiftTimeSteps(dfInput, g_aBackwardTimeSteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07c7b3c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Time Step</th>\n",
       "      <th colspan=\"20\" halign=\"left\">-1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time Series</th>\n",
       "      <th colspan=\"20\" halign=\"left\">NAT.GAS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>tick_volume</th>\n",
       "      <th>spread</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>hour</th>\n",
       "      <th>adx</th>\n",
       "      <th>adx_neg</th>\n",
       "      <th>adx_pos</th>\n",
       "      <th>aroon_down_24</th>\n",
       "      <th>aroon_ind_24</th>\n",
       "      <th>aroon_up_24</th>\n",
       "      <th>cci</th>\n",
       "      <th>dpo_24</th>\n",
       "      <th>ema_24</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-01-05 02:00:00</th>\n",
       "      <td>2.582</td>\n",
       "      <td>2.589</td>\n",
       "      <td>2.582</td>\n",
       "      <td>2.588</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.732942</td>\n",
       "      <td>22.124691</td>\n",
       "      <td>16.667519</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>-41.666667</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>-27.331487</td>\n",
       "      <td>-0.006958</td>\n",
       "      <td>2.590461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-05 02:30:00</th>\n",
       "      <td>2.588</td>\n",
       "      <td>2.589</td>\n",
       "      <td>2.583</td>\n",
       "      <td>2.587</td>\n",
       "      <td>153.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.538557</td>\n",
       "      <td>21.748555</td>\n",
       "      <td>16.384159</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>-41.666667</td>\n",
       "      <td>20.833333</td>\n",
       "      <td>-24.385511</td>\n",
       "      <td>-0.013083</td>\n",
       "      <td>2.590184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-05 03:00:00</th>\n",
       "      <td>2.586</td>\n",
       "      <td>2.592</td>\n",
       "      <td>2.585</td>\n",
       "      <td>2.588</td>\n",
       "      <td>129.0</td>\n",
       "      <td>0.011</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.244230</td>\n",
       "      <td>21.307561</td>\n",
       "      <td>16.920949</td>\n",
       "      <td>58.333333</td>\n",
       "      <td>-41.666667</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>-12.569691</td>\n",
       "      <td>-0.011458</td>\n",
       "      <td>2.590010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-05 03:30:00</th>\n",
       "      <td>2.589</td>\n",
       "      <td>2.602</td>\n",
       "      <td>2.588</td>\n",
       "      <td>2.602</td>\n",
       "      <td>270.0</td>\n",
       "      <td>0.011</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.621825</td>\n",
       "      <td>20.442495</td>\n",
       "      <td>19.133904</td>\n",
       "      <td>54.166667</td>\n",
       "      <td>-41.666667</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>29.306931</td>\n",
       "      <td>-0.013125</td>\n",
       "      <td>2.590969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-05 04:00:00</th>\n",
       "      <td>2.601</td>\n",
       "      <td>2.601</td>\n",
       "      <td>2.595</td>\n",
       "      <td>2.599</td>\n",
       "      <td>136.0</td>\n",
       "      <td>0.011</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.025352</td>\n",
       "      <td>20.018462</td>\n",
       "      <td>18.737015</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>-41.666667</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>31.773597</td>\n",
       "      <td>-0.006375</td>\n",
       "      <td>2.591611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-21 21:30:00</th>\n",
       "      <td>4.824</td>\n",
       "      <td>4.844</td>\n",
       "      <td>4.810</td>\n",
       "      <td>4.843</td>\n",
       "      <td>1570.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.318925</td>\n",
       "      <td>28.456738</td>\n",
       "      <td>15.020175</td>\n",
       "      <td>79.166667</td>\n",
       "      <td>-45.833333</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>-69.497198</td>\n",
       "      <td>0.053083</td>\n",
       "      <td>4.893054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-21 22:00:00</th>\n",
       "      <td>4.843</td>\n",
       "      <td>4.864</td>\n",
       "      <td>4.840</td>\n",
       "      <td>4.849</td>\n",
       "      <td>799.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.460824</td>\n",
       "      <td>27.739579</td>\n",
       "      <td>16.741784</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>-45.833333</td>\n",
       "      <td>29.166667</td>\n",
       "      <td>-50.402653</td>\n",
       "      <td>-0.000917</td>\n",
       "      <td>4.889529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-21 22:30:00</th>\n",
       "      <td>4.849</td>\n",
       "      <td>4.885</td>\n",
       "      <td>4.848</td>\n",
       "      <td>4.879</td>\n",
       "      <td>671.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>21.341199</td>\n",
       "      <td>26.658781</td>\n",
       "      <td>18.300860</td>\n",
       "      <td>70.833333</td>\n",
       "      <td>-45.833333</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>-31.879393</td>\n",
       "      <td>0.043042</td>\n",
       "      <td>4.888687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-21 23:00:00</th>\n",
       "      <td>4.879</td>\n",
       "      <td>4.888</td>\n",
       "      <td>4.862</td>\n",
       "      <td>4.872</td>\n",
       "      <td>679.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>21.190593</td>\n",
       "      <td>25.918313</td>\n",
       "      <td>18.113029</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>-45.833333</td>\n",
       "      <td>20.833333</td>\n",
       "      <td>-26.149548</td>\n",
       "      <td>-0.006917</td>\n",
       "      <td>4.887352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-21 23:30:00</th>\n",
       "      <td>4.872</td>\n",
       "      <td>4.873</td>\n",
       "      <td>4.849</td>\n",
       "      <td>4.855</td>\n",
       "      <td>409.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>21.155493</td>\n",
       "      <td>26.654375</td>\n",
       "      <td>17.641062</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>-45.833333</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>-34.438907</td>\n",
       "      <td>-0.007458</td>\n",
       "      <td>4.884764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8463 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Time Step                -1                                                  \\\n",
       "Time Series         NAT.GAS                                                   \n",
       "Feature                open   high    low  close tick_volume spread    year   \n",
       "time                                                                          \n",
       "2021-01-05 02:00:00   2.582  2.589  2.582  2.588       148.0  0.010  2021.0   \n",
       "2021-01-05 02:30:00   2.588  2.589  2.583  2.587       153.0  0.010  2021.0   \n",
       "2021-01-05 03:00:00   2.586  2.592  2.585  2.588       129.0  0.011  2021.0   \n",
       "2021-01-05 03:30:00   2.589  2.602  2.588  2.602       270.0  0.011  2021.0   \n",
       "2021-01-05 04:00:00   2.601  2.601  2.595  2.599       136.0  0.011  2021.0   \n",
       "...                     ...    ...    ...    ...         ...    ...     ...   \n",
       "2021-09-21 21:30:00   4.824  4.844  4.810  4.843      1570.0  0.008  2021.0   \n",
       "2021-09-21 22:00:00   4.843  4.864  4.840  4.849       799.0  0.005  2021.0   \n",
       "2021-09-21 22:30:00   4.849  4.885  4.848  4.879       671.0  0.005  2021.0   \n",
       "2021-09-21 23:00:00   4.879  4.888  4.862  4.872       679.0  0.005  2021.0   \n",
       "2021-09-21 23:30:00   4.872  4.873  4.849  4.855       409.0  0.010  2021.0   \n",
       "\n",
       "Time Step                                                              \\\n",
       "Time Series                                                             \n",
       "Feature             month   day dayofweek  hour        adx    adx_neg   \n",
       "time                                                                    \n",
       "2021-01-05 02:00:00   1.0   5.0       1.0   1.0  18.732942  22.124691   \n",
       "2021-01-05 02:30:00   1.0   5.0       1.0   2.0  18.538557  21.748555   \n",
       "2021-01-05 03:00:00   1.0   5.0       1.0   2.0  18.244230  21.307561   \n",
       "2021-01-05 03:30:00   1.0   5.0       1.0   3.0  17.621825  20.442495   \n",
       "2021-01-05 04:00:00   1.0   5.0       1.0   3.0  17.025352  20.018462   \n",
       "...                   ...   ...       ...   ...        ...        ...   \n",
       "2021-09-21 21:30:00   9.0  21.0       1.0  21.0  21.318925  28.456738   \n",
       "2021-09-21 22:00:00   9.0  21.0       1.0  21.0  21.460824  27.739579   \n",
       "2021-09-21 22:30:00   9.0  21.0       1.0  22.0  21.341199  26.658781   \n",
       "2021-09-21 23:00:00   9.0  21.0       1.0  22.0  21.190593  25.918313   \n",
       "2021-09-21 23:30:00   9.0  21.0       1.0  23.0  21.155493  26.654375   \n",
       "\n",
       "Time Step                                                              \\\n",
       "Time Series                                                             \n",
       "Feature                adx_pos aroon_down_24 aroon_ind_24 aroon_up_24   \n",
       "time                                                                    \n",
       "2021-01-05 02:00:00  16.667519     66.666667   -41.666667   25.000000   \n",
       "2021-01-05 02:30:00  16.384159     62.500000   -41.666667   20.833333   \n",
       "2021-01-05 03:00:00  16.920949     58.333333   -41.666667   16.666667   \n",
       "2021-01-05 03:30:00  19.133904     54.166667   -41.666667   12.500000   \n",
       "2021-01-05 04:00:00  18.737015     50.000000   -41.666667    8.333333   \n",
       "...                        ...           ...          ...         ...   \n",
       "2021-09-21 21:30:00  15.020175     79.166667   -45.833333   33.333333   \n",
       "2021-09-21 22:00:00  16.741784     75.000000   -45.833333   29.166667   \n",
       "2021-09-21 22:30:00  18.300860     70.833333   -45.833333   25.000000   \n",
       "2021-09-21 23:00:00  18.113029     66.666667   -45.833333   20.833333   \n",
       "2021-09-21 23:30:00  17.641062     62.500000   -45.833333   16.666667   \n",
       "\n",
       "Time Step                                           \n",
       "Time Series                                         \n",
       "Feature                    cci    dpo_24    ema_24  \n",
       "time                                                \n",
       "2021-01-05 02:00:00 -27.331487 -0.006958  2.590461  \n",
       "2021-01-05 02:30:00 -24.385511 -0.013083  2.590184  \n",
       "2021-01-05 03:00:00 -12.569691 -0.011458  2.590010  \n",
       "2021-01-05 03:30:00  29.306931 -0.013125  2.590969  \n",
       "2021-01-05 04:00:00  31.773597 -0.006375  2.591611  \n",
       "...                        ...       ...       ...  \n",
       "2021-09-21 21:30:00 -69.497198  0.053083  4.893054  \n",
       "2021-09-21 22:00:00 -50.402653 -0.000917  4.889529  \n",
       "2021-09-21 22:30:00 -31.879393  0.043042  4.888687  \n",
       "2021-09-21 23:00:00 -26.149548 -0.006917  4.887352  \n",
       "2021-09-21 23:30:00 -34.438907 -0.007458  4.884764  \n",
       "\n",
       "[8463 rows x 20 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7dabba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "j = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c4e0cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "aColumns = pd.MultiIndex.from_tuples(\n",
    "    [(i,j)],\n",
    "    names = [\"From\",\"To\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bdfa0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfOutput = pd.DataFrame(index = dfInput.index, \n",
    "                        columns = aColumns)\n",
    "\n",
    "    \n",
    "dfSpread = dfRates[sSymbol][\"spread\"]\n",
    "dfOpen = dfRates[sSymbol][\"open\"].shift(-i)\n",
    "dfClose = dfRates[sSymbol][\"close\"].shift(-j)\n",
    "\n",
    "dfNetReturn = (abs(dfClose - dfOpen) - dfSpread)\n",
    "dfReturn = (dfClose - dfOpen)/dfOpen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b58f09",
   "metadata": {},
   "source": [
    "# INVESTABILITY MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3db7ddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sModelName = os.path.join(sSymbol , \"__investability model__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "449e35ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sSubModelName = os.path.join(sModelName , \"__\"+ str(i) +\"  \" + str(j) + \"__\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575fa9d7",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c9f820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInputInvestability = dfInput.copy()\n",
    "dfOutputInvestability = dfOutput.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87e7d7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMaskNonInvestable = dfNetReturn<=0\n",
    "dfMaskInvestable = dfNetReturn>0\n",
    "\n",
    "dfOutputInvestability.loc[dfMaskNonInvestable] = 0\n",
    "dfOutputInvestability.loc[dfMaskInvestable] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "646509c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>From</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>To</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-01-05 02:00:00</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-05 02:30:00</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-05 03:00:00</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-05 03:30:00</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-05 04:00:00</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-21 21:30:00</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-21 22:00:00</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-21 22:30:00</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-21 23:00:00</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-21 23:30:00</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8463 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "From                   1\n",
       "To                     3\n",
       "time                    \n",
       "2021-01-05 02:00:00    1\n",
       "2021-01-05 02:30:00    1\n",
       "2021-01-05 03:00:00    0\n",
       "2021-01-05 03:30:00    0\n",
       "2021-01-05 04:00:00    0\n",
       "...                  ...\n",
       "2021-09-21 21:30:00    1\n",
       "2021-09-21 22:00:00    1\n",
       "2021-09-21 22:30:00  NaN\n",
       "2021-09-21 23:00:00  NaN\n",
       "2021-09-21 23:30:00  NaN\n",
       "\n",
       "[8463 rows x 1 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfOutputInvestability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16787112",
   "metadata": {},
   "source": [
    "### Remove Empty Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e03e15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aMissingOutputs = dfOutputInvestability[dfOutputInvestability.isna().any(axis=1)].index\n",
    "dfInputInvestability.drop(aMissingOutputs, inplace = True)\n",
    "dfOutputInvestability.drop(aMissingOutputs, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c2102f",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6154ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInputTrain, dfInputValidation, dfInputTest, dfOutputTrain, dfOutputValidation, dfOutputTest = dfSplitData(dfInputInvestability, \n",
    "                                                                                                            dfOutputInvestability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0d669c",
   "metadata": {},
   "source": [
    "### Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d506f074",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfScaledInputTrain, dfScaledInputValidation, dfScaledInputTest = dfScaleData(\"input\", dfInputTrain,dfInputValidation, dfInputTest )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6b9931",
   "metadata": {},
   "source": [
    "### Balance Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89f6c7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)\n",
       "0.0       3171\n",
       "1.0       1905\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfOutputTrain.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53a4994f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfBalancedInputTrain , dfBalancedOutputTrain = dfOversampleImbalancedData(\n",
    "    dfScaledInputTrain,\n",
    "    dfOutputTrain)\n",
    "dfBalancedOutputTrain.columns = aColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9bfa3eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)\n",
       "0.0       3171\n",
       "1.0       3171\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfBalancedOutputTrain.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71839c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)\n",
       "0.0       1008\n",
       "1.0        684\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfOutputValidation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "88179483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)\n",
       "1.0       871\n",
       "0.0       821\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfOutputTest.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d1128f",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ff3f1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "100/100 [==============================] - 4s 12ms/step - loss: 0.7857 - val_loss: 0.7964\n",
      "Epoch 2/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.7807 - val_loss: 0.7888\n",
      "Epoch 3/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.7761 - val_loss: 0.7816\n",
      "Epoch 4/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.7715 - val_loss: 0.7746\n",
      "Epoch 5/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.7668 - val_loss: 0.7683\n",
      "Epoch 6/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.7622 - val_loss: 0.7625\n",
      "Epoch 7/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.7575 - val_loss: 0.7572\n",
      "Epoch 8/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.7528 - val_loss: 0.7517\n",
      "Epoch 9/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.7480 - val_loss: 0.7466\n",
      "Epoch 10/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.7430 - val_loss: 0.7419\n",
      "Epoch 11/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.7379 - val_loss: 0.7371\n",
      "Epoch 12/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.7329 - val_loss: 0.7327\n",
      "Epoch 13/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.7277 - val_loss: 0.7285\n",
      "Epoch 14/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.7225 - val_loss: 0.7247\n",
      "Epoch 15/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.7173 - val_loss: 0.7212\n",
      "Epoch 16/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.7121 - val_loss: 0.7181\n",
      "Epoch 17/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.7071 - val_loss: 0.7154\n",
      "Epoch 18/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.7021 - val_loss: 0.7129\n",
      "Epoch 19/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6971 - val_loss: 0.7108\n",
      "Epoch 20/10000\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.6922 - val_loss: 0.7085\n",
      "Epoch 21/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6877 - val_loss: 0.7067\n",
      "Epoch 22/10000\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.6832 - val_loss: 0.7050\n",
      "Epoch 23/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6789 - val_loss: 0.7036\n",
      "Epoch 24/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6747 - val_loss: 0.7024\n",
      "Epoch 25/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6708 - val_loss: 0.7018\n",
      "Epoch 26/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6672 - val_loss: 0.7009\n",
      "Epoch 27/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6638 - val_loss: 0.7004\n",
      "Epoch 28/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6607 - val_loss: 0.6998\n",
      "Epoch 29/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6578 - val_loss: 0.6989\n",
      "Epoch 30/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6551 - val_loss: 0.6980\n",
      "Epoch 31/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6527 - val_loss: 0.6965\n",
      "Epoch 32/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6504 - val_loss: 0.6955\n",
      "Epoch 33/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6484 - val_loss: 0.6952\n",
      "Epoch 34/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6467 - val_loss: 0.6944\n",
      "Epoch 35/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6451 - val_loss: 0.6927\n",
      "Epoch 36/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6435 - val_loss: 0.6931\n",
      "Epoch 37/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6421 - val_loss: 0.6926\n",
      "Epoch 38/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6407 - val_loss: 0.6921\n",
      "Epoch 39/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6395 - val_loss: 0.6910\n",
      "Epoch 40/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6384 - val_loss: 0.6902\n",
      "Epoch 41/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6373 - val_loss: 0.6900\n",
      "Epoch 42/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6363 - val_loss: 0.6888\n",
      "Epoch 43/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6354 - val_loss: 0.6877\n",
      "Epoch 44/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6345 - val_loss: 0.6874\n",
      "Epoch 45/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6337 - val_loss: 0.6868\n",
      "Epoch 46/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6329 - val_loss: 0.6855\n",
      "Epoch 47/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6322 - val_loss: 0.6846\n",
      "Epoch 48/10000\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.6315 - val_loss: 0.6831\n",
      "Epoch 49/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6309 - val_loss: 0.6817\n",
      "Epoch 50/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6302 - val_loss: 0.6806\n",
      "Epoch 51/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6296 - val_loss: 0.6791\n",
      "Epoch 52/10000\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.6290 - val_loss: 0.6783\n",
      "Epoch 53/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6284 - val_loss: 0.6773\n",
      "Epoch 54/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6278 - val_loss: 0.6767\n",
      "Epoch 55/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6273 - val_loss: 0.6738\n",
      "Epoch 56/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6267 - val_loss: 0.6727\n",
      "Epoch 57/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6261 - val_loss: 0.6724\n",
      "Epoch 58/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6256 - val_loss: 0.6718\n",
      "Epoch 59/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6251 - val_loss: 0.6707\n",
      "Epoch 60/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6246 - val_loss: 0.6700\n",
      "Epoch 61/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6241 - val_loss: 0.6686\n",
      "Epoch 62/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6236 - val_loss: 0.6670\n",
      "Epoch 63/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6231 - val_loss: 0.6666\n",
      "Epoch 64/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6226 - val_loss: 0.6655\n",
      "Epoch 65/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6222 - val_loss: 0.6642\n",
      "Epoch 66/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6217 - val_loss: 0.6634\n",
      "Epoch 67/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6212 - val_loss: 0.6621\n",
      "Epoch 68/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6208 - val_loss: 0.6613\n",
      "Epoch 69/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6203 - val_loss: 0.6604\n",
      "Epoch 70/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6199 - val_loss: 0.6592\n",
      "Epoch 71/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6194 - val_loss: 0.6586\n",
      "Epoch 72/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6189 - val_loss: 0.6583\n",
      "Epoch 73/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6185 - val_loss: 0.6578\n",
      "Epoch 74/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6181 - val_loss: 0.6570\n",
      "Epoch 75/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6176 - val_loss: 0.6561\n",
      "Epoch 76/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6172 - val_loss: 0.6554\n",
      "Epoch 77/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6167 - val_loss: 0.6544\n",
      "Epoch 78/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6163 - val_loss: 0.6537\n",
      "Epoch 79/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6159 - val_loss: 0.6529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6155 - val_loss: 0.6523\n",
      "Epoch 81/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6150 - val_loss: 0.6514\n",
      "Epoch 82/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6146 - val_loss: 0.6509\n",
      "Epoch 83/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6142 - val_loss: 0.6505\n",
      "Epoch 84/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6138 - val_loss: 0.6500\n",
      "Epoch 85/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6134 - val_loss: 0.6498\n",
      "Epoch 86/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6130 - val_loss: 0.6493\n",
      "Epoch 87/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6126 - val_loss: 0.6485\n",
      "Epoch 88/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6122 - val_loss: 0.6479\n",
      "Epoch 89/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6118 - val_loss: 0.6474\n",
      "Epoch 90/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6114 - val_loss: 0.6470\n",
      "Epoch 91/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6110 - val_loss: 0.6467\n",
      "Epoch 92/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6106 - val_loss: 0.6463\n",
      "Epoch 93/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6102 - val_loss: 0.6460\n",
      "Epoch 94/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6098 - val_loss: 0.6457\n",
      "Epoch 95/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6094 - val_loss: 0.6455\n",
      "Epoch 96/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6090 - val_loss: 0.6452\n",
      "Epoch 97/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6086 - val_loss: 0.6450\n",
      "Epoch 98/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6083 - val_loss: 0.6447\n",
      "Epoch 99/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6079 - val_loss: 0.6444\n",
      "Epoch 100/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6075 - val_loss: 0.6442\n",
      "Epoch 101/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6071 - val_loss: 0.6441\n",
      "Epoch 102/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6067 - val_loss: 0.6438\n",
      "Epoch 103/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6063 - val_loss: 0.6436\n",
      "Epoch 104/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6060 - val_loss: 0.6436\n",
      "Epoch 105/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6056 - val_loss: 0.6433\n",
      "Epoch 106/10000\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.6052 - val_loss: 0.6432\n",
      "Epoch 107/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6048 - val_loss: 0.6431\n",
      "Epoch 108/10000\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.6045 - val_loss: 0.6431\n",
      "Epoch 109/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6041 - val_loss: 0.6430\n",
      "Epoch 110/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6038 - val_loss: 0.6428\n",
      "Epoch 111/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6034 - val_loss: 0.6430\n",
      "Epoch 112/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6030 - val_loss: 0.6431\n",
      "Epoch 113/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6027 - val_loss: 0.6432\n",
      "Epoch 114/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6023 - val_loss: 0.6435\n",
      "Epoch 115/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6020 - val_loss: 0.6440\n",
      "Epoch 116/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6016 - val_loss: 0.6436\n",
      "Epoch 117/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6012 - val_loss: 0.6436\n",
      "Epoch 118/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6009 - val_loss: 0.6438\n",
      "Epoch 119/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6005 - val_loss: 0.6439\n",
      "Epoch 120/10000\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.6002 - val_loss: 0.6441\n",
      "Epoch 121/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.5998 - val_loss: 0.6445\n",
      "Epoch 122/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.5995 - val_loss: 0.6443\n",
      "Epoch 123/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.5991 - val_loss: 0.6444\n",
      "Epoch 124/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.5988 - val_loss: 0.6444\n",
      "Epoch 125/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.5984 - val_loss: 0.6451\n",
      "Epoch 126/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.5981 - val_loss: 0.6451\n",
      "Epoch 127/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.5978 - val_loss: 0.6450\n",
      "Epoch 128/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.5974 - val_loss: 0.6455\n",
      "Epoch 129/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.5971 - val_loss: 0.6455\n",
      "Epoch 130/10000\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.5967 - val_loss: 0.6452\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "oPredictiveModel = Long_Short_Term_Memory(\n",
    "    sSubModelName, \n",
    "    dfScaledInputTrain.shape[1], \n",
    "    1,\n",
    "    len(g_aBackwardTimeSteps), \n",
    "    1\n",
    "    )\n",
    "\n",
    "oPredictiveModel.Train(dfBalancedInputTrain, \n",
    "                       dfBalancedOutputTrain,\n",
    "                       dfScaledInputValidation,\n",
    "                       dfOutputValidation\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3389a00a",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1105a7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "aPredictionProb, _ = oPredictiveModel.aPredict(dfScaledInputTest)\n",
    "\n",
    "aPrediction = aPredictionProb.round()\n",
    "\n",
    "dfPrediction = pd.DataFrame(aPrediction, index = dfInputTest.index)\n",
    "dfPrediction.columns = aColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d895dc3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)\n",
       "1.0       1240\n",
       "0.0        452\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfPrediction.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09d189d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.42      0.54       821\n",
      "         1.0       0.61      0.87      0.72       871\n",
      "\n",
      "    accuracy                           0.65      1692\n",
      "   macro avg       0.68      0.64      0.63      1692\n",
      "weighted avg       0.68      0.65      0.63      1692\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(dfOutputTest, dfPrediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1edcb31",
   "metadata": {},
   "source": [
    "# DIRECTIONAL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6fabc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sModelName = os.path.join(sSymbol , \"__directional model__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "294a054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sSubModelName = os.path.join(sModelName , \"__\"+ str(i) +\"  \" + str(j) + \"__\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2850b94",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "38436612",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInputDirectional = dfInput.copy()\n",
    "dfOutputDirectional = dfOutput.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f4d1205",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMaskUpward = dfClose>dfOpen\n",
    "dfMaskDownward = dfClose<dfOpen \n",
    "\n",
    "dfOutputDirectional.loc[(dfMaskInvestable) & (dfMaskDownward)] = 0\n",
    "dfOutputDirectional.loc[(dfMaskInvestable) & (dfMaskUpward)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7446b2b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>From</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>To</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-01-05 02:00:00</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-05 02:30:00</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-05 03:00:00</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-05 03:30:00</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-05 04:00:00</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-21 21:30:00</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-21 22:00:00</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-21 22:30:00</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-21 23:00:00</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-21 23:30:00</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8463 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "From                   1\n",
       "To                     3\n",
       "time                    \n",
       "2021-01-05 02:00:00    1\n",
       "2021-01-05 02:30:00    1\n",
       "2021-01-05 03:00:00  NaN\n",
       "2021-01-05 03:30:00  NaN\n",
       "2021-01-05 04:00:00  NaN\n",
       "...                  ...\n",
       "2021-09-21 21:30:00    1\n",
       "2021-09-21 22:00:00    0\n",
       "2021-09-21 22:30:00  NaN\n",
       "2021-09-21 23:00:00  NaN\n",
       "2021-09-21 23:30:00  NaN\n",
       "\n",
       "[8463 rows x 1 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfOutputDirectional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e02677",
   "metadata": {},
   "source": [
    "### Remove Empty Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "016840c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "aMissingOutputs = dfOutputDirectional[dfOutputDirectional.isna().any(axis=1)].index\n",
    "dfInputDirectional.drop(aMissingOutputs, inplace = True)\n",
    "dfOutputDirectional.drop(aMissingOutputs, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9cd8c1",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e7857648",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInputTrain, dfInputValidation, dfInputTest, dfOutputTrain, dfOutputValidation, dfOutputTest = dfSplitData(dfInputDirectional, \n",
    "                                                                                                            dfOutputDirectional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d867c265",
   "metadata": {},
   "source": [
    "### Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "38a247ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfScaledInputTrain, dfScaledInputValidation, dfScaledInputTest = dfScaleData(\"input\", dfInputTrain,dfInputValidation, dfInputTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d9f864",
   "metadata": {},
   "source": [
    "### Balance Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4f0a0894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)\n",
       "1.0       1073\n",
       "0.0       1003\n",
       "dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfOutputTrain.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "54d54711",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfBalancedInputTrain , dfBalancedOutputTrain = dfOversampleImbalancedData(\n",
    "    dfScaledInputTrain,\n",
    "    dfOutputTrain)\n",
    "dfBalancedOutputTrain.columns = aColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1d7d92a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)\n",
       "0.0       1073\n",
       "1.0       1073\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfBalancedOutputTrain.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7cb2e5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)\n",
       "1.0       371\n",
       "0.0       321\n",
       "dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfOutputValidation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7f6e7f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)\n",
       "1.0       382\n",
       "0.0       310\n",
       "dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfOutputTest.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd272c43",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aadc49b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "34/34 [==============================] - 3s 21ms/step - loss: 0.7844 - val_loss: 0.8100\n",
      "Epoch 2/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7841 - val_loss: 0.8101\n",
      "Epoch 3/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7838 - val_loss: 0.8100\n",
      "Epoch 4/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7835 - val_loss: 0.8098\n",
      "Epoch 5/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7832 - val_loss: 0.8098\n",
      "Epoch 6/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7829 - val_loss: 0.8096\n",
      "Epoch 7/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7826 - val_loss: 0.8096\n",
      "Epoch 8/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7823 - val_loss: 0.8095\n",
      "Epoch 9/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7820 - val_loss: 0.8095\n",
      "Epoch 10/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7817 - val_loss: 0.8094\n",
      "Epoch 11/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7815 - val_loss: 0.8092\n",
      "Epoch 12/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7812 - val_loss: 0.8092\n",
      "Epoch 13/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7809 - val_loss: 0.8091\n",
      "Epoch 14/10000\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.7807 - val_loss: 0.8091\n",
      "Epoch 15/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7804 - val_loss: 0.8089\n",
      "Epoch 16/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7801 - val_loss: 0.8089\n",
      "Epoch 17/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7799 - val_loss: 0.8089\n",
      "Epoch 18/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7796 - val_loss: 0.8089\n",
      "Epoch 19/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7794 - val_loss: 0.8088\n",
      "Epoch 20/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7791 - val_loss: 0.8085\n",
      "Epoch 21/10000\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.7789 - val_loss: 0.8085\n",
      "Epoch 22/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7786 - val_loss: 0.8085\n",
      "Epoch 23/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7784 - val_loss: 0.8083\n",
      "Epoch 24/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7781 - val_loss: 0.8081\n",
      "Epoch 25/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7779 - val_loss: 0.8079\n",
      "Epoch 26/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7777 - val_loss: 0.8076\n",
      "Epoch 27/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7774 - val_loss: 0.8076\n",
      "Epoch 28/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7772 - val_loss: 0.8075\n",
      "Epoch 29/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7770 - val_loss: 0.8074\n",
      "Epoch 30/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7767 - val_loss: 0.8072\n",
      "Epoch 31/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7765 - val_loss: 0.8071\n",
      "Epoch 32/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7763 - val_loss: 0.8070\n",
      "Epoch 33/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7760 - val_loss: 0.8070\n",
      "Epoch 34/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7758 - val_loss: 0.8068\n",
      "Epoch 35/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7756 - val_loss: 0.8067\n",
      "Epoch 36/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7754 - val_loss: 0.8066\n",
      "Epoch 37/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7751 - val_loss: 0.8065\n",
      "Epoch 38/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7749 - val_loss: 0.8064\n",
      "Epoch 39/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7747 - val_loss: 0.8063\n",
      "Epoch 40/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7745 - val_loss: 0.8060\n",
      "Epoch 41/10000\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.7743 - val_loss: 0.8059\n",
      "Epoch 42/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7741 - val_loss: 0.8057\n",
      "Epoch 43/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7738 - val_loss: 0.8056\n",
      "Epoch 44/10000\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.7736 - val_loss: 0.8056\n",
      "Epoch 45/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7734 - val_loss: 0.8054\n",
      "Epoch 46/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7732 - val_loss: 0.8053\n",
      "Epoch 47/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7730 - val_loss: 0.8052\n",
      "Epoch 48/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7728 - val_loss: 0.8051\n",
      "Epoch 49/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7726 - val_loss: 0.8051\n",
      "Epoch 50/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7724 - val_loss: 0.8051\n",
      "Epoch 51/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7722 - val_loss: 0.8050\n",
      "Epoch 52/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7720 - val_loss: 0.8048\n",
      "Epoch 53/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7718 - val_loss: 0.8048\n",
      "Epoch 54/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7715 - val_loss: 0.8048\n",
      "Epoch 55/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7713 - val_loss: 0.8047\n",
      "Epoch 56/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7711 - val_loss: 0.8047\n",
      "Epoch 57/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7709 - val_loss: 0.8046\n",
      "Epoch 58/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7707 - val_loss: 0.8045\n",
      "Epoch 59/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7705 - val_loss: 0.8045\n",
      "Epoch 60/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7703 - val_loss: 0.8044\n",
      "Epoch 61/10000\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.7702 - val_loss: 0.8042\n",
      "Epoch 62/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7700 - val_loss: 0.8041\n",
      "Epoch 63/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7698 - val_loss: 0.8040\n",
      "Epoch 64/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7696 - val_loss: 0.8039\n",
      "Epoch 65/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7694 - val_loss: 0.8038\n",
      "Epoch 66/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7692 - val_loss: 0.8037\n",
      "Epoch 67/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7690 - val_loss: 0.8036\n",
      "Epoch 68/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7688 - val_loss: 0.8033\n",
      "Epoch 69/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7686 - val_loss: 0.8033\n",
      "Epoch 70/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7684 - val_loss: 0.8031\n",
      "Epoch 71/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7682 - val_loss: 0.8029\n",
      "Epoch 72/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7680 - val_loss: 0.8028\n",
      "Epoch 73/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7678 - val_loss: 0.8028\n",
      "Epoch 74/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7676 - val_loss: 0.8027\n",
      "Epoch 75/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7674 - val_loss: 0.8026\n",
      "Epoch 76/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7672 - val_loss: 0.8025\n",
      "Epoch 77/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7670 - val_loss: 0.8024\n",
      "Epoch 78/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7669 - val_loss: 0.8024\n",
      "Epoch 79/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7667 - val_loss: 0.8024\n",
      "Epoch 80/10000\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.7665 - val_loss: 0.8024\n",
      "Epoch 81/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 0s 10ms/step - loss: 0.7663 - val_loss: 0.8024\n",
      "Epoch 82/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7661 - val_loss: 0.8022\n",
      "Epoch 83/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7659 - val_loss: 0.8022\n",
      "Epoch 84/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7658 - val_loss: 0.8021\n",
      "Epoch 85/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7656 - val_loss: 0.8020\n",
      "Epoch 86/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7654 - val_loss: 0.8018\n",
      "Epoch 87/10000\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.7652 - val_loss: 0.8017\n",
      "Epoch 88/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7650 - val_loss: 0.8017\n",
      "Epoch 89/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7648 - val_loss: 0.8016\n",
      "Epoch 90/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7646 - val_loss: 0.8015\n",
      "Epoch 91/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7645 - val_loss: 0.8013\n",
      "Epoch 92/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7643 - val_loss: 0.8013\n",
      "Epoch 93/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7641 - val_loss: 0.8012\n",
      "Epoch 94/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7639 - val_loss: 0.8011\n",
      "Epoch 95/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7638 - val_loss: 0.8010\n",
      "Epoch 96/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7636 - val_loss: 0.8009\n",
      "Epoch 97/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7634 - val_loss: 0.8008\n",
      "Epoch 98/10000\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.7632 - val_loss: 0.8007\n",
      "Epoch 99/10000\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.7630 - val_loss: 0.8005\n",
      "Epoch 100/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7629 - val_loss: 0.8004\n",
      "Epoch 101/10000\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.7627 - val_loss: 0.8003\n",
      "Epoch 102/10000\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.7625 - val_loss: 0.8003\n",
      "Epoch 103/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7623 - val_loss: 0.8001\n",
      "Epoch 104/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7622 - val_loss: 0.8000\n",
      "Epoch 105/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7620 - val_loss: 0.8000\n",
      "Epoch 106/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7618 - val_loss: 0.7999\n",
      "Epoch 107/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7616 - val_loss: 0.7998\n",
      "Epoch 108/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7615 - val_loss: 0.7997\n",
      "Epoch 109/10000\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.7613 - val_loss: 0.7996\n",
      "Epoch 110/10000\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.7611 - val_loss: 0.7995\n",
      "Epoch 111/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7609 - val_loss: 0.7995\n",
      "Epoch 112/10000\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.7608 - val_loss: 0.7993\n",
      "Epoch 113/10000\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.7606 - val_loss: 0.7993\n",
      "Epoch 114/10000\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.7604 - val_loss: 0.7992\n",
      "Epoch 115/10000\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.7602 - val_loss: 0.7991\n",
      "Epoch 116/10000\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.7601 - val_loss: 0.7991\n",
      "Epoch 117/10000\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.7599 - val_loss: 0.7992\n",
      "Epoch 118/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7597 - val_loss: 0.7991\n",
      "Epoch 119/10000\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.7596 - val_loss: 0.7989\n",
      "Epoch 120/10000\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.7594 - val_loss: 0.7990\n",
      "Epoch 121/10000\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.7592 - val_loss: 0.7989\n",
      "Epoch 122/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7591 - val_loss: 0.7989\n",
      "Epoch 123/10000\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.7589 - val_loss: 0.7988\n",
      "Epoch 124/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7587 - val_loss: 0.7987\n",
      "Epoch 125/10000\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.7586 - val_loss: 0.7987\n",
      "Epoch 126/10000\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.7584 - val_loss: 0.7986\n",
      "Epoch 127/10000\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.7582 - val_loss: 0.7986\n",
      "Epoch 128/10000\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.7581 - val_loss: 0.7986\n",
      "Epoch 129/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7579 - val_loss: 0.7985\n",
      "Epoch 130/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7577 - val_loss: 0.7985\n",
      "Epoch 131/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7576 - val_loss: 0.7984\n",
      "Epoch 132/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7574 - val_loss: 0.7984\n",
      "Epoch 133/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7572 - val_loss: 0.7983\n",
      "Epoch 134/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7571 - val_loss: 0.7983\n",
      "Epoch 135/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7569 - val_loss: 0.7983\n",
      "Epoch 136/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7568 - val_loss: 0.7982\n",
      "Epoch 137/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7566 - val_loss: 0.7982\n",
      "Epoch 138/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7564 - val_loss: 0.7981\n",
      "Epoch 139/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7563 - val_loss: 0.7980\n",
      "Epoch 140/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7561 - val_loss: 0.7980\n",
      "Epoch 141/10000\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.7559 - val_loss: 0.7979\n",
      "Epoch 142/10000\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.7558 - val_loss: 0.7980\n",
      "Epoch 143/10000\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.7556 - val_loss: 0.7979\n",
      "Epoch 144/10000\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.7555 - val_loss: 0.7978\n",
      "Epoch 145/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7553 - val_loss: 0.7978\n",
      "Epoch 146/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7551 - val_loss: 0.7980\n",
      "Epoch 147/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7550 - val_loss: 0.7980\n",
      "Epoch 148/10000\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.7548 - val_loss: 0.7979\n",
      "Epoch 149/10000\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.7547 - val_loss: 0.7978\n",
      "Epoch 150/10000\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.7545 - val_loss: 0.7976\n",
      "Epoch 151/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7543 - val_loss: 0.7975\n",
      "Epoch 152/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7542 - val_loss: 0.7975\n",
      "Epoch 153/10000\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.7540 - val_loss: 0.7974\n",
      "Epoch 154/10000\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.7539 - val_loss: 0.7974\n",
      "Epoch 155/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7537 - val_loss: 0.7974\n",
      "Epoch 156/10000\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.7536 - val_loss: 0.7975\n",
      "Epoch 157/10000\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.7534 - val_loss: 0.7976\n",
      "Epoch 158/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7532 - val_loss: 0.7976\n",
      "Epoch 159/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7531 - val_loss: 0.7978\n",
      "Epoch 160/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7529 - val_loss: 0.7978\n",
      "Epoch 161/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7528 - val_loss: 0.7978\n",
      "Epoch 162/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7526 - val_loss: 0.7977\n",
      "Epoch 163/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7525 - val_loss: 0.7977\n",
      "Epoch 164/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7523 - val_loss: 0.7976\n",
      "Epoch 165/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7522 - val_loss: 0.7976\n",
      "Epoch 166/10000\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.7520 - val_loss: 0.7975\n",
      "Epoch 167/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7519 - val_loss: 0.7974\n",
      "Epoch 168/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7517 - val_loss: 0.7973\n",
      "Epoch 169/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7516 - val_loss: 0.7972\n",
      "Epoch 170/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7514 - val_loss: 0.7973\n",
      "Epoch 171/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7513 - val_loss: 0.7973\n",
      "Epoch 172/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7511 - val_loss: 0.7972\n",
      "Epoch 173/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7510 - val_loss: 0.7971\n",
      "Epoch 174/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7508 - val_loss: 0.7973\n",
      "Epoch 175/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7507 - val_loss: 0.7974\n",
      "Epoch 176/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7505 - val_loss: 0.7975\n",
      "Epoch 177/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7504 - val_loss: 0.7976\n",
      "Epoch 178/10000\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.7502 - val_loss: 0.7976\n",
      "Epoch 179/10000\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.7501 - val_loss: 0.7977\n",
      "Epoch 180/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7499 - val_loss: 0.7976\n",
      "Epoch 181/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7498 - val_loss: 0.7975\n",
      "Epoch 182/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7496 - val_loss: 0.7976\n",
      "Epoch 183/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7495 - val_loss: 0.7976\n",
      "Epoch 184/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7493 - val_loss: 0.7975\n",
      "Epoch 185/10000\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.7492 - val_loss: 0.7973\n",
      "Epoch 186/10000\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.7490 - val_loss: 0.7972\n",
      "Epoch 187/10000\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.7489 - val_loss: 0.7971\n",
      "Epoch 188/10000\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.7487 - val_loss: 0.7970\n",
      "Epoch 189/10000\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.7486 - val_loss: 0.7969\n",
      "Epoch 190/10000\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.7484 - val_loss: 0.7969\n",
      "Epoch 191/10000\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.7483 - val_loss: 0.7969\n",
      "Epoch 192/10000\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.7481 - val_loss: 0.7968\n",
      "Epoch 193/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7480 - val_loss: 0.7967\n",
      "Epoch 194/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7478 - val_loss: 0.7966\n",
      "Epoch 195/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7477 - val_loss: 0.7965\n",
      "Epoch 196/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7476 - val_loss: 0.7964\n",
      "Epoch 197/10000\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.7474 - val_loss: 0.7962\n",
      "Epoch 198/10000\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.7473 - val_loss: 0.7961\n",
      "Epoch 199/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7471 - val_loss: 0.7961\n",
      "Epoch 200/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7470 - val_loss: 0.7961\n",
      "Epoch 201/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7468 - val_loss: 0.7962\n",
      "Epoch 202/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7467 - val_loss: 0.7963\n",
      "Epoch 203/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7465 - val_loss: 0.7966\n",
      "Epoch 204/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7464 - val_loss: 0.7966\n",
      "Epoch 205/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7463 - val_loss: 0.7967\n",
      "Epoch 206/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7461 - val_loss: 0.7967\n",
      "Epoch 207/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7460 - val_loss: 0.7967\n",
      "Epoch 208/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7458 - val_loss: 0.7966\n",
      "Epoch 209/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7457 - val_loss: 0.7966\n",
      "Epoch 210/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7456 - val_loss: 0.7967\n",
      "Epoch 211/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7454 - val_loss: 0.7967\n",
      "Epoch 212/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7453 - val_loss: 0.7968\n",
      "Epoch 213/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7451 - val_loss: 0.7968\n",
      "Epoch 214/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7450 - val_loss: 0.7968\n",
      "Epoch 215/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7449 - val_loss: 0.7968\n",
      "Epoch 216/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7447 - val_loss: 0.7968\n",
      "Epoch 217/10000\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.7446 - val_loss: 0.7968\n",
      "Epoch 218/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7444 - val_loss: 0.7969\n",
      "Epoch 219/10000\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.7443 - val_loss: 0.7970\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "oPredictiveModel = Long_Short_Term_Memory(\n",
    "    sSubModelName, \n",
    "    dfScaledInputTrain.shape[1], \n",
    "    1,\n",
    "    len(g_aBackwardTimeSteps), \n",
    "    1\n",
    "    )\n",
    "\n",
    "oPredictiveModel.Train(dfBalancedInputTrain, \n",
    "                       dfBalancedOutputTrain,\n",
    "                       dfScaledInputValidation,\n",
    "                       dfOutputValidation\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988c117a",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "862e6cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "aPredictionProb, _ = oPredictiveModel.aPredict(dfScaledInputTest)\n",
    "\n",
    "aPrediction = aPredictionProb.round()\n",
    "\n",
    "dfPrediction = pd.DataFrame(aPrediction, index = dfInputTest.index)\n",
    "dfPrediction.columns = aColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "26914599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)\n",
       "0.0       692\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfPrediction.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7936f2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.45      1.00      0.62       310\n",
      "         1.0       0.00      0.00      0.00       382\n",
      "\n",
      "    accuracy                           0.45       692\n",
      "   macro avg       0.22      0.50      0.31       692\n",
      "weighted avg       0.20      0.45      0.28       692\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yunus\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\yunus\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\yunus\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(dfOutputTest, dfPrediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc84d4e1",
   "metadata": {},
   "source": [
    "# UPWARD REGRESSION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "404f4f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "sModelName = os.path.join(sSymbol , \"__upward regression model__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ae829f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "sSubModelName = os.path.join(sModelName , \"__\"+ str(i) +\"  \" + str(j) + \"__\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6be5a7",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fee2856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInputUpward = dfInput.copy()\n",
    "dfOutputUpward = dfOutput.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "74cdef0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfOutputUpward.loc[dfMaskInvestable & dfMaskUpward, i] = dfReturn.loc[dfOutputUpward.index].loc[dfMaskInvestable & dfMaskUpward].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "97cc4af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>From</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>To</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-01-05 02:00:00</th>\n",
       "      <td>0.005027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-05 02:30:00</th>\n",
       "      <td>0.004635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-05 03:00:00</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-05 03:30:00</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-05 04:00:00</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-21 21:30:00</th>\n",
       "      <td>0.001237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-21 22:00:00</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-21 22:30:00</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-21 23:00:00</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-21 23:30:00</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8463 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "From                        1\n",
       "To                          3\n",
       "time                         \n",
       "2021-01-05 02:00:00  0.005027\n",
       "2021-01-05 02:30:00  0.004635\n",
       "2021-01-05 03:00:00       NaN\n",
       "2021-01-05 03:30:00       NaN\n",
       "2021-01-05 04:00:00       NaN\n",
       "...                       ...\n",
       "2021-09-21 21:30:00  0.001237\n",
       "2021-09-21 22:00:00       NaN\n",
       "2021-09-21 22:30:00       NaN\n",
       "2021-09-21 23:00:00       NaN\n",
       "2021-09-21 23:30:00       NaN\n",
       "\n",
       "[8463 rows x 1 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfOutputUpward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d44f80",
   "metadata": {},
   "source": [
    "### Remove Empty Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "abb7f719",
   "metadata": {},
   "outputs": [],
   "source": [
    "aMissingOutputs = dfOutputUpward[dfOutputUpward.isna().any(axis=1)].index\n",
    "dfInputUpward.drop(aMissingOutputs, inplace = True)\n",
    "dfOutputUpward.drop(aMissingOutputs, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c37bee4",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f95b1eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInputTrain, dfInputValidation, dfInputTest, dfOutputTrain, dfOutputValidation, dfOutputTest = dfSplitData(dfInputUpward, \n",
    "                                                                                                            dfOutputUpward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b62fcf",
   "metadata": {},
   "source": [
    "### Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "275904ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfScaledInputTrain, dfScaledInputValidation, dfScaledInputTest = dfScaleData(\"input\", dfInputTrain,dfInputValidation, dfInputTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05556bb1",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c7f620de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "18/18 [==============================] - 3s 41ms/step - loss: 0.1270 - val_loss: 0.7213\n",
      "Epoch 2/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.1241 - val_loss: 0.6776\n",
      "Epoch 3/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.1215 - val_loss: 0.6379\n",
      "Epoch 4/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.1190 - val_loss: 0.5968\n",
      "Epoch 5/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.1166 - val_loss: 0.5613\n",
      "Epoch 6/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.1145 - val_loss: 0.5296\n",
      "Epoch 7/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.1125 - val_loss: 0.4992\n",
      "Epoch 8/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.1107 - val_loss: 0.4710\n",
      "Epoch 9/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.1089 - val_loss: 0.4450\n",
      "Epoch 10/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.1073 - val_loss: 0.4205\n",
      "Epoch 11/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.1059 - val_loss: 0.3959\n",
      "Epoch 12/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.1044 - val_loss: 0.3737\n",
      "Epoch 13/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.1031 - val_loss: 0.3526\n",
      "Epoch 14/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.1019 - val_loss: 0.3343\n",
      "Epoch 15/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.1008 - val_loss: 0.3163\n",
      "Epoch 16/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0997 - val_loss: 0.3002\n",
      "Epoch 17/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0988 - val_loss: 0.2848\n",
      "Epoch 18/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0979 - val_loss: 0.2705\n",
      "Epoch 19/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0971 - val_loss: 0.2573\n",
      "Epoch 20/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0963 - val_loss: 0.2452\n",
      "Epoch 21/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0956 - val_loss: 0.2331\n",
      "Epoch 22/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0949 - val_loss: 0.2213\n",
      "Epoch 23/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0943 - val_loss: 0.2105\n",
      "Epoch 24/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0937 - val_loss: 0.2020\n",
      "Epoch 25/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0932 - val_loss: 0.1940\n",
      "Epoch 26/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0927 - val_loss: 0.1867\n",
      "Epoch 27/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0923 - val_loss: 0.1797\n",
      "Epoch 28/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0918 - val_loss: 0.1728\n",
      "Epoch 29/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0915 - val_loss: 0.1669\n",
      "Epoch 30/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0911 - val_loss: 0.1616\n",
      "Epoch 31/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0908 - val_loss: 0.1564\n",
      "Epoch 32/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0905 - val_loss: 0.1519\n",
      "Epoch 33/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0902 - val_loss: 0.1478\n",
      "Epoch 34/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0899 - val_loss: 0.1439\n",
      "Epoch 35/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0896 - val_loss: 0.1400\n",
      "Epoch 36/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0894 - val_loss: 0.1365\n",
      "Epoch 37/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0891 - val_loss: 0.1333\n",
      "Epoch 38/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0889 - val_loss: 0.1303\n",
      "Epoch 39/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0887 - val_loss: 0.1276\n",
      "Epoch 40/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0885 - val_loss: 0.1253\n",
      "Epoch 41/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0883 - val_loss: 0.1231\n",
      "Epoch 42/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0881 - val_loss: 0.1210\n",
      "Epoch 43/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0879 - val_loss: 0.1191\n",
      "Epoch 44/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0878 - val_loss: 0.1173\n",
      "Epoch 45/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0876 - val_loss: 0.1156\n",
      "Epoch 46/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0874 - val_loss: 0.1142\n",
      "Epoch 47/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0873 - val_loss: 0.1128\n",
      "Epoch 48/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0871 - val_loss: 0.1116\n",
      "Epoch 49/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0870 - val_loss: 0.1103\n",
      "Epoch 50/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0868 - val_loss: 0.1089\n",
      "Epoch 51/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0867 - val_loss: 0.1077\n",
      "Epoch 52/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0865 - val_loss: 0.1068\n",
      "Epoch 53/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0864 - val_loss: 0.1059\n",
      "Epoch 54/10000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.0863 - val_loss: 0.1050\n",
      "Epoch 55/10000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.0861 - val_loss: 0.1041\n",
      "Epoch 56/10000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.0860 - val_loss: 0.1034\n",
      "Epoch 57/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0859 - val_loss: 0.1027\n",
      "Epoch 58/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0857 - val_loss: 0.1018\n",
      "Epoch 59/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0856 - val_loss: 0.1012\n",
      "Epoch 60/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0855 - val_loss: 0.1005\n",
      "Epoch 61/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0853 - val_loss: 0.1000\n",
      "Epoch 62/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0852 - val_loss: 0.0994\n",
      "Epoch 63/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0851 - val_loss: 0.0989\n",
      "Epoch 64/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0849 - val_loss: 0.0984\n",
      "Epoch 65/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0848 - val_loss: 0.0980\n",
      "Epoch 66/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0847 - val_loss: 0.0975\n",
      "Epoch 67/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0846 - val_loss: 0.0971\n",
      "Epoch 68/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0844 - val_loss: 0.0967\n",
      "Epoch 69/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0843 - val_loss: 0.0963\n",
      "Epoch 70/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0842 - val_loss: 0.0957\n",
      "Epoch 71/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0841 - val_loss: 0.0954\n",
      "Epoch 72/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0839 - val_loss: 0.0952\n",
      "Epoch 73/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0838 - val_loss: 0.0948\n",
      "Epoch 74/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0837 - val_loss: 0.0944\n",
      "Epoch 75/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0836 - val_loss: 0.0942\n",
      "Epoch 76/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0835 - val_loss: 0.0940\n",
      "Epoch 77/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0833 - val_loss: 0.0937\n",
      "Epoch 78/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0832 - val_loss: 0.0933\n",
      "Epoch 79/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0831 - val_loss: 0.0929\n",
      "Epoch 80/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0830 - val_loss: 0.0927\n",
      "Epoch 81/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0829 - val_loss: 0.0924\n",
      "Epoch 82/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0827 - val_loss: 0.0920\n",
      "Epoch 83/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0826 - val_loss: 0.0917\n",
      "Epoch 84/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0825 - val_loss: 0.0915\n",
      "Epoch 85/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0824 - val_loss: 0.0912\n",
      "Epoch 86/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0823 - val_loss: 0.0909\n",
      "Epoch 87/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0821 - val_loss: 0.0906\n",
      "Epoch 88/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0820 - val_loss: 0.0904\n",
      "Epoch 89/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0819 - val_loss: 0.0902\n",
      "Epoch 90/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0818 - val_loss: 0.0900\n",
      "Epoch 91/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0817 - val_loss: 0.0897\n",
      "Epoch 92/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0816 - val_loss: 0.0895\n",
      "Epoch 93/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0814 - val_loss: 0.0893\n",
      "Epoch 94/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0813 - val_loss: 0.0890\n",
      "Epoch 95/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0812 - val_loss: 0.0888\n",
      "Epoch 96/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0811 - val_loss: 0.0886\n",
      "Epoch 97/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0810 - val_loss: 0.0883\n",
      "Epoch 98/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0808 - val_loss: 0.0881\n",
      "Epoch 99/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0807 - val_loss: 0.0879\n",
      "Epoch 100/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0806 - val_loss: 0.0877\n",
      "Epoch 101/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0805 - val_loss: 0.0875\n",
      "Epoch 102/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0804 - val_loss: 0.0873\n",
      "Epoch 103/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0803 - val_loss: 0.0871\n",
      "Epoch 104/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0801 - val_loss: 0.0869\n",
      "Epoch 105/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0800 - val_loss: 0.0867\n",
      "Epoch 106/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0799 - val_loss: 0.0865\n",
      "Epoch 107/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0798 - val_loss: 0.0863\n",
      "Epoch 108/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0797 - val_loss: 0.0861\n",
      "Epoch 109/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0795 - val_loss: 0.0858\n",
      "Epoch 110/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0794 - val_loss: 0.0856\n",
      "Epoch 111/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0793 - val_loss: 0.0854\n",
      "Epoch 112/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0792 - val_loss: 0.0853\n",
      "Epoch 113/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0791 - val_loss: 0.0850\n",
      "Epoch 114/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0789 - val_loss: 0.0848\n",
      "Epoch 115/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0788 - val_loss: 0.0846\n",
      "Epoch 116/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0787 - val_loss: 0.0844\n",
      "Epoch 117/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0786 - val_loss: 0.0842\n",
      "Epoch 118/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0785 - val_loss: 0.0840\n",
      "Epoch 119/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0784 - val_loss: 0.0839\n",
      "Epoch 120/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0782 - val_loss: 0.0837\n",
      "Epoch 121/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0781 - val_loss: 0.0835\n",
      "Epoch 122/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0780 - val_loss: 0.0832\n",
      "Epoch 123/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0779 - val_loss: 0.0830\n",
      "Epoch 124/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0778 - val_loss: 0.0829\n",
      "Epoch 125/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0776 - val_loss: 0.0828\n",
      "Epoch 126/10000\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.0775 - val_loss: 0.0826\n",
      "Epoch 127/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0774 - val_loss: 0.0824\n",
      "Epoch 128/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0773 - val_loss: 0.0822\n",
      "Epoch 129/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0772 - val_loss: 0.0821\n",
      "Epoch 130/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0770 - val_loss: 0.0819\n",
      "Epoch 131/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0769 - val_loss: 0.0818\n",
      "Epoch 132/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0768 - val_loss: 0.0817\n",
      "Epoch 133/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0767 - val_loss: 0.0815\n",
      "Epoch 134/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0766 - val_loss: 0.0813\n",
      "Epoch 135/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0764 - val_loss: 0.0811\n",
      "Epoch 136/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0763 - val_loss: 0.0809\n",
      "Epoch 137/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0762 - val_loss: 0.0808\n",
      "Epoch 138/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0761 - val_loss: 0.0806\n",
      "Epoch 139/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0760 - val_loss: 0.0804\n",
      "Epoch 140/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0758 - val_loss: 0.0802\n",
      "Epoch 141/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0757 - val_loss: 0.0800\n",
      "Epoch 142/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0756 - val_loss: 0.0799\n",
      "Epoch 143/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0755 - val_loss: 0.0797\n",
      "Epoch 144/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0754 - val_loss: 0.0794\n",
      "Epoch 145/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0752 - val_loss: 0.0793\n",
      "Epoch 146/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0751 - val_loss: 0.0790\n",
      "Epoch 147/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0750 - val_loss: 0.0789\n",
      "Epoch 148/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0749 - val_loss: 0.0787\n",
      "Epoch 149/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0748 - val_loss: 0.0786\n",
      "Epoch 150/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0746 - val_loss: 0.0785\n",
      "Epoch 151/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0745 - val_loss: 0.0783\n",
      "Epoch 152/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0744 - val_loss: 0.0781\n",
      "Epoch 153/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0743 - val_loss: 0.0780\n",
      "Epoch 154/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0741 - val_loss: 0.0778\n",
      "Epoch 155/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0740 - val_loss: 0.0777\n",
      "Epoch 156/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0739 - val_loss: 0.0776\n",
      "Epoch 157/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0738 - val_loss: 0.0774\n",
      "Epoch 158/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0737 - val_loss: 0.0771\n",
      "Epoch 159/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0735 - val_loss: 0.0770\n",
      "Epoch 160/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0734 - val_loss: 0.0769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0733 - val_loss: 0.0767\n",
      "Epoch 162/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0732 - val_loss: 0.0766\n",
      "Epoch 163/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0731 - val_loss: 0.0764\n",
      "Epoch 164/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0729 - val_loss: 0.0763\n",
      "Epoch 165/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0728 - val_loss: 0.0761\n",
      "Epoch 166/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0727 - val_loss: 0.0759\n",
      "Epoch 167/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0726 - val_loss: 0.0758\n",
      "Epoch 168/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0724 - val_loss: 0.0756\n",
      "Epoch 169/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0723 - val_loss: 0.0755\n",
      "Epoch 170/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0722 - val_loss: 0.0753\n",
      "Epoch 171/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0721 - val_loss: 0.0751\n",
      "Epoch 172/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0719 - val_loss: 0.0750\n",
      "Epoch 173/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0718 - val_loss: 0.0748\n",
      "Epoch 174/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0717 - val_loss: 0.0746\n",
      "Epoch 175/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0716 - val_loss: 0.0745\n",
      "Epoch 176/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0715 - val_loss: 0.0743\n",
      "Epoch 177/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0713 - val_loss: 0.0742\n",
      "Epoch 178/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0712 - val_loss: 0.0741\n",
      "Epoch 179/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0711 - val_loss: 0.0739\n",
      "Epoch 180/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0710 - val_loss: 0.0737\n",
      "Epoch 181/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0708 - val_loss: 0.0736\n",
      "Epoch 182/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0707 - val_loss: 0.0735\n",
      "Epoch 183/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0706 - val_loss: 0.0733\n",
      "Epoch 184/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0705 - val_loss: 0.0732\n",
      "Epoch 185/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0703 - val_loss: 0.0730\n",
      "Epoch 186/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0702 - val_loss: 0.0729\n",
      "Epoch 187/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0701 - val_loss: 0.0727\n",
      "Epoch 188/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0700 - val_loss: 0.0726\n",
      "Epoch 189/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0698 - val_loss: 0.0724\n",
      "Epoch 190/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0697 - val_loss: 0.0723\n",
      "Epoch 191/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0696 - val_loss: 0.0721\n",
      "Epoch 192/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0695 - val_loss: 0.0719\n",
      "Epoch 193/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0693 - val_loss: 0.0718\n",
      "Epoch 194/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0692 - val_loss: 0.0716\n",
      "Epoch 195/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0691 - val_loss: 0.0715\n",
      "Epoch 196/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0690 - val_loss: 0.0714\n",
      "Epoch 197/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0689 - val_loss: 0.0712\n",
      "Epoch 198/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0687 - val_loss: 0.0710\n",
      "Epoch 199/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0686 - val_loss: 0.0709\n",
      "Epoch 200/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0685 - val_loss: 0.0707\n",
      "Epoch 201/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0684 - val_loss: 0.0706\n",
      "Epoch 202/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0682 - val_loss: 0.0704\n",
      "Epoch 203/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0681 - val_loss: 0.0703\n",
      "Epoch 204/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0680 - val_loss: 0.0702\n",
      "Epoch 205/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0679 - val_loss: 0.0700\n",
      "Epoch 206/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0677 - val_loss: 0.0699\n",
      "Epoch 207/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0676 - val_loss: 0.0697\n",
      "Epoch 208/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0675 - val_loss: 0.0696\n",
      "Epoch 209/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0674 - val_loss: 0.0694\n",
      "Epoch 210/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0672 - val_loss: 0.0693\n",
      "Epoch 211/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0671 - val_loss: 0.0691\n",
      "Epoch 212/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0670 - val_loss: 0.0690\n",
      "Epoch 213/10000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.0669 - val_loss: 0.0689\n",
      "Epoch 214/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0667 - val_loss: 0.0687\n",
      "Epoch 215/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0666 - val_loss: 0.0686\n",
      "Epoch 216/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0665 - val_loss: 0.0684\n",
      "Epoch 217/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0664 - val_loss: 0.0682\n",
      "Epoch 218/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0662 - val_loss: 0.0681\n",
      "Epoch 219/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0661 - val_loss: 0.0680\n",
      "Epoch 220/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0660 - val_loss: 0.0678\n",
      "Epoch 221/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0659 - val_loss: 0.0677\n",
      "Epoch 222/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0657 - val_loss: 0.0676\n",
      "Epoch 223/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0656 - val_loss: 0.0674\n",
      "Epoch 224/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0655 - val_loss: 0.0673\n",
      "Epoch 225/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0654 - val_loss: 0.0671\n",
      "Epoch 226/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0652 - val_loss: 0.0670\n",
      "Epoch 227/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0651 - val_loss: 0.0669\n",
      "Epoch 228/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0650 - val_loss: 0.0667\n",
      "Epoch 229/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0649 - val_loss: 0.0666\n",
      "Epoch 230/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0647 - val_loss: 0.0664\n",
      "Epoch 231/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0646 - val_loss: 0.0663\n",
      "Epoch 232/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0645 - val_loss: 0.0661\n",
      "Epoch 233/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0644 - val_loss: 0.0660\n",
      "Epoch 234/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0642 - val_loss: 0.0659\n",
      "Epoch 235/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0641 - val_loss: 0.0657\n",
      "Epoch 236/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0640 - val_loss: 0.0656\n",
      "Epoch 237/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0639 - val_loss: 0.0654\n",
      "Epoch 238/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0637 - val_loss: 0.0653\n",
      "Epoch 239/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0636 - val_loss: 0.0652\n",
      "Epoch 240/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0635 - val_loss: 0.0651\n",
      "Epoch 241/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0634 - val_loss: 0.0649\n",
      "Epoch 242/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0632 - val_loss: 0.0648\n",
      "Epoch 243/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0631 - val_loss: 0.0647\n",
      "Epoch 244/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0630 - val_loss: 0.0645\n",
      "Epoch 245/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0629 - val_loss: 0.0644\n",
      "Epoch 246/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0627 - val_loss: 0.0642\n",
      "Epoch 247/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0626 - val_loss: 0.0641\n",
      "Epoch 248/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0625 - val_loss: 0.0639\n",
      "Epoch 249/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0624 - val_loss: 0.0638\n",
      "Epoch 250/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0622 - val_loss: 0.0636\n",
      "Epoch 251/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0621 - val_loss: 0.0635\n",
      "Epoch 252/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0620 - val_loss: 0.0633\n",
      "Epoch 253/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0619 - val_loss: 0.0632\n",
      "Epoch 254/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0617 - val_loss: 0.0631\n",
      "Epoch 255/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0616 - val_loss: 0.0630\n",
      "Epoch 256/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0615 - val_loss: 0.0628\n",
      "Epoch 257/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0614 - val_loss: 0.0627\n",
      "Epoch 258/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0612 - val_loss: 0.0626\n",
      "Epoch 259/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0611 - val_loss: 0.0624\n",
      "Epoch 260/10000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.0610 - val_loss: 0.0623\n",
      "Epoch 261/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0609 - val_loss: 0.0621\n",
      "Epoch 262/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0607 - val_loss: 0.0620\n",
      "Epoch 263/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0606 - val_loss: 0.0619\n",
      "Epoch 264/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0605 - val_loss: 0.0618\n",
      "Epoch 265/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0604 - val_loss: 0.0616\n",
      "Epoch 266/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0602 - val_loss: 0.0615\n",
      "Epoch 267/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0601 - val_loss: 0.0614\n",
      "Epoch 268/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0600 - val_loss: 0.0612\n",
      "Epoch 269/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0599 - val_loss: 0.0611\n",
      "Epoch 270/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0597 - val_loss: 0.0610\n",
      "Epoch 271/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0596 - val_loss: 0.0608\n",
      "Epoch 272/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0595 - val_loss: 0.0607\n",
      "Epoch 273/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0594 - val_loss: 0.0605\n",
      "Epoch 274/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0592 - val_loss: 0.0605\n",
      "Epoch 275/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0591 - val_loss: 0.0603\n",
      "Epoch 276/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0590 - val_loss: 0.0601\n",
      "Epoch 277/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0589 - val_loss: 0.0600\n",
      "Epoch 278/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0587 - val_loss: 0.0599\n",
      "Epoch 279/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0586 - val_loss: 0.0597\n",
      "Epoch 280/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0585 - val_loss: 0.0596\n",
      "Epoch 281/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0584 - val_loss: 0.0595\n",
      "Epoch 282/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0583 - val_loss: 0.0594\n",
      "Epoch 283/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0581 - val_loss: 0.0593\n",
      "Epoch 284/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0580 - val_loss: 0.0591\n",
      "Epoch 285/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0579 - val_loss: 0.0590\n",
      "Epoch 286/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0578 - val_loss: 0.0588\n",
      "Epoch 287/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0576 - val_loss: 0.0587\n",
      "Epoch 288/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0575 - val_loss: 0.0586\n",
      "Epoch 289/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0574 - val_loss: 0.0584\n",
      "Epoch 290/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0573 - val_loss: 0.0583\n",
      "Epoch 291/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0571 - val_loss: 0.0582\n",
      "Epoch 292/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0570 - val_loss: 0.0581\n",
      "Epoch 293/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0569 - val_loss: 0.0579\n",
      "Epoch 294/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0568 - val_loss: 0.0578\n",
      "Epoch 295/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0567 - val_loss: 0.0576\n",
      "Epoch 296/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0565 - val_loss: 0.0575\n",
      "Epoch 297/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0564 - val_loss: 0.0574\n",
      "Epoch 298/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0563 - val_loss: 0.0573\n",
      "Epoch 299/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0562 - val_loss: 0.0571\n",
      "Epoch 300/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0560 - val_loss: 0.0570\n",
      "Epoch 301/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0559 - val_loss: 0.0569\n",
      "Epoch 302/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0558 - val_loss: 0.0568\n",
      "Epoch 303/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0557 - val_loss: 0.0566\n",
      "Epoch 304/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0556 - val_loss: 0.0565\n",
      "Epoch 305/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0554 - val_loss: 0.0564\n",
      "Epoch 306/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0553 - val_loss: 0.0562\n",
      "Epoch 307/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0552 - val_loss: 0.0561\n",
      "Epoch 308/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0551 - val_loss: 0.0560\n",
      "Epoch 309/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0550 - val_loss: 0.0558\n",
      "Epoch 310/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0548 - val_loss: 0.0557\n",
      "Epoch 311/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0547 - val_loss: 0.0556\n",
      "Epoch 312/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0546 - val_loss: 0.0554\n",
      "Epoch 313/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0545 - val_loss: 0.0553\n",
      "Epoch 314/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0543 - val_loss: 0.0552\n",
      "Epoch 315/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0542 - val_loss: 0.0551\n",
      "Epoch 316/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0541 - val_loss: 0.0550\n",
      "Epoch 317/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0540 - val_loss: 0.0548\n",
      "Epoch 318/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0539 - val_loss: 0.0547\n",
      "Epoch 319/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0537 - val_loss: 0.0546\n",
      "Epoch 320/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0536 - val_loss: 0.0545\n",
      "Epoch 321/10000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.0535 - val_loss: 0.0543\n",
      "Epoch 322/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0534 - val_loss: 0.0542\n",
      "Epoch 323/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0533 - val_loss: 0.0541\n",
      "Epoch 324/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0531 - val_loss: 0.0540\n",
      "Epoch 325/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0530 - val_loss: 0.0538\n",
      "Epoch 326/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0529 - val_loss: 0.0537\n",
      "Epoch 327/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0528 - val_loss: 0.0536\n",
      "Epoch 328/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0527 - val_loss: 0.0534\n",
      "Epoch 329/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0526 - val_loss: 0.0533\n",
      "Epoch 330/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0524 - val_loss: 0.0532\n",
      "Epoch 331/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0523 - val_loss: 0.0531\n",
      "Epoch 332/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0522 - val_loss: 0.0529\n",
      "Epoch 333/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0521 - val_loss: 0.0528\n",
      "Epoch 334/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0520 - val_loss: 0.0527\n",
      "Epoch 335/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0518 - val_loss: 0.0526\n",
      "Epoch 336/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0517 - val_loss: 0.0525\n",
      "Epoch 337/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0516 - val_loss: 0.0523\n",
      "Epoch 338/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0515 - val_loss: 0.0522\n",
      "Epoch 339/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0514 - val_loss: 0.0521\n",
      "Epoch 340/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0512 - val_loss: 0.0520\n",
      "Epoch 341/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0511 - val_loss: 0.0518\n",
      "Epoch 342/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0510 - val_loss: 0.0517\n",
      "Epoch 343/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0509 - val_loss: 0.0516\n",
      "Epoch 344/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0508 - val_loss: 0.0515\n",
      "Epoch 345/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0507 - val_loss: 0.0513\n",
      "Epoch 346/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0505 - val_loss: 0.0512\n",
      "Epoch 347/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0504 - val_loss: 0.0511\n",
      "Epoch 348/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0503 - val_loss: 0.0510\n",
      "Epoch 349/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0502 - val_loss: 0.0509\n",
      "Epoch 350/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0501 - val_loss: 0.0507\n",
      "Epoch 351/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0500 - val_loss: 0.0506\n",
      "Epoch 352/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0498 - val_loss: 0.0505\n",
      "Epoch 353/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0497 - val_loss: 0.0504\n",
      "Epoch 354/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0496 - val_loss: 0.0503\n",
      "Epoch 355/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0495 - val_loss: 0.0501\n",
      "Epoch 356/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0494 - val_loss: 0.0500\n",
      "Epoch 357/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0493 - val_loss: 0.0499\n",
      "Epoch 358/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0491 - val_loss: 0.0498\n",
      "Epoch 359/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0490 - val_loss: 0.0496\n",
      "Epoch 360/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0489 - val_loss: 0.0495\n",
      "Epoch 361/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0488 - val_loss: 0.0494\n",
      "Epoch 362/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0487 - val_loss: 0.0493\n",
      "Epoch 363/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0486 - val_loss: 0.0492\n",
      "Epoch 364/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0485 - val_loss: 0.0490\n",
      "Epoch 365/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0483 - val_loss: 0.0489\n",
      "Epoch 366/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0482 - val_loss: 0.0488\n",
      "Epoch 367/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0481 - val_loss: 0.0487\n",
      "Epoch 368/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0480 - val_loss: 0.0486\n",
      "Epoch 369/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0479 - val_loss: 0.0484\n",
      "Epoch 370/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0478 - val_loss: 0.0483\n",
      "Epoch 371/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0477 - val_loss: 0.0482\n",
      "Epoch 372/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0475 - val_loss: 0.0481\n",
      "Epoch 373/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0474 - val_loss: 0.0480\n",
      "Epoch 374/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0473 - val_loss: 0.0479\n",
      "Epoch 375/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0472 - val_loss: 0.0477\n",
      "Epoch 376/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0471 - val_loss: 0.0476\n",
      "Epoch 377/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0470 - val_loss: 0.0475\n",
      "Epoch 378/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0469 - val_loss: 0.0474\n",
      "Epoch 379/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0467 - val_loss: 0.0473\n",
      "Epoch 380/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0466 - val_loss: 0.0472\n",
      "Epoch 381/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0465 - val_loss: 0.0470\n",
      "Epoch 382/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0464 - val_loss: 0.0469\n",
      "Epoch 383/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0463 - val_loss: 0.0468\n",
      "Epoch 384/10000\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.0462 - val_loss: 0.0467\n",
      "Epoch 385/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0461 - val_loss: 0.0466\n",
      "Epoch 386/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0460 - val_loss: 0.0465\n",
      "Epoch 387/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0459 - val_loss: 0.0464\n",
      "Epoch 388/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0457 - val_loss: 0.0462\n",
      "Epoch 389/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0456 - val_loss: 0.0461\n",
      "Epoch 390/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0455 - val_loss: 0.0460\n",
      "Epoch 391/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0454 - val_loss: 0.0459\n",
      "Epoch 392/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0453 - val_loss: 0.0458\n",
      "Epoch 393/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0452 - val_loss: 0.0457\n",
      "Epoch 394/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0451 - val_loss: 0.0455\n",
      "Epoch 395/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0450 - val_loss: 0.0454\n",
      "Epoch 396/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0449 - val_loss: 0.0453\n",
      "Epoch 397/10000\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.0447 - val_loss: 0.0452\n",
      "Epoch 398/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 11ms/step - loss: 0.0446 - val_loss: 0.0451\n",
      "Epoch 399/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0445 - val_loss: 0.0450\n",
      "Epoch 400/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0444 - val_loss: 0.0449\n",
      "Epoch 401/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0443 - val_loss: 0.0448\n",
      "Epoch 402/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0442 - val_loss: 0.0446\n",
      "Epoch 403/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0441 - val_loss: 0.0445\n",
      "Epoch 404/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0440 - val_loss: 0.0444\n",
      "Epoch 405/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0439 - val_loss: 0.0443\n",
      "Epoch 406/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0438 - val_loss: 0.0442\n",
      "Epoch 407/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0436 - val_loss: 0.0441\n",
      "Epoch 408/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0435 - val_loss: 0.0440\n",
      "Epoch 409/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0434 - val_loss: 0.0439\n",
      "Epoch 410/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0433 - val_loss: 0.0438\n",
      "Epoch 411/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0432 - val_loss: 0.0436\n",
      "Epoch 412/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0431 - val_loss: 0.0435\n",
      "Epoch 413/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0430 - val_loss: 0.0434\n",
      "Epoch 414/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0429 - val_loss: 0.0433\n",
      "Epoch 415/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0428 - val_loss: 0.0432\n",
      "Epoch 416/10000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.0427 - val_loss: 0.0431\n",
      "Epoch 417/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0426 - val_loss: 0.0430\n",
      "Epoch 418/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0425 - val_loss: 0.0429\n",
      "Epoch 419/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0424 - val_loss: 0.0428\n",
      "Epoch 420/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0422 - val_loss: 0.0426\n",
      "Epoch 421/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0421 - val_loss: 0.0425\n",
      "Epoch 422/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0420 - val_loss: 0.0424\n",
      "Epoch 423/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0419 - val_loss: 0.0423\n",
      "Epoch 424/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0418 - val_loss: 0.0422\n",
      "Epoch 425/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0417 - val_loss: 0.0421\n",
      "Epoch 426/10000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.0416 - val_loss: 0.0420\n",
      "Epoch 427/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0415 - val_loss: 0.0419\n",
      "Epoch 428/10000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.0414 - val_loss: 0.0418\n",
      "Epoch 429/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0413 - val_loss: 0.0417\n",
      "Epoch 430/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0412 - val_loss: 0.0416\n",
      "Epoch 431/10000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.0411 - val_loss: 0.0415\n",
      "Epoch 432/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0410 - val_loss: 0.0413\n",
      "Epoch 433/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0409 - val_loss: 0.0412\n",
      "Epoch 434/10000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.0408 - val_loss: 0.0411\n",
      "Epoch 435/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0407 - val_loss: 0.0410\n",
      "Epoch 436/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0406 - val_loss: 0.0409\n",
      "Epoch 437/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0405 - val_loss: 0.0408\n",
      "Epoch 438/10000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.0404 - val_loss: 0.0407\n",
      "Epoch 439/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0403 - val_loss: 0.0406\n",
      "Epoch 440/10000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.0401 - val_loss: 0.0405\n",
      "Epoch 441/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0400 - val_loss: 0.0404\n",
      "Epoch 442/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0399 - val_loss: 0.0403\n",
      "Epoch 443/10000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.0398 - val_loss: 0.0402\n",
      "Epoch 444/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0397 - val_loss: 0.0401\n",
      "Epoch 445/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0396 - val_loss: 0.0400\n",
      "Epoch 446/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0395 - val_loss: 0.0399\n",
      "Epoch 447/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0394 - val_loss: 0.0398\n",
      "Epoch 448/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0393 - val_loss: 0.0397\n",
      "Epoch 449/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0392 - val_loss: 0.0396\n",
      "Epoch 450/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0391 - val_loss: 0.0394\n",
      "Epoch 451/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0390 - val_loss: 0.0393\n",
      "Epoch 452/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0389 - val_loss: 0.0392\n",
      "Epoch 453/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0388 - val_loss: 0.0391\n",
      "Epoch 454/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0387 - val_loss: 0.0390\n",
      "Epoch 455/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0386 - val_loss: 0.0389\n",
      "Epoch 456/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0385 - val_loss: 0.0388\n",
      "Epoch 457/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0384 - val_loss: 0.0387\n",
      "Epoch 458/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0383 - val_loss: 0.0386\n",
      "Epoch 459/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0382 - val_loss: 0.0385\n",
      "Epoch 460/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0381 - val_loss: 0.0384\n",
      "Epoch 461/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0380 - val_loss: 0.0383\n",
      "Epoch 462/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0379 - val_loss: 0.0382\n",
      "Epoch 463/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0378 - val_loss: 0.0381\n",
      "Epoch 464/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0377 - val_loss: 0.0380\n",
      "Epoch 465/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0376 - val_loss: 0.0379\n",
      "Epoch 466/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0375 - val_loss: 0.0378\n",
      "Epoch 467/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0374 - val_loss: 0.0377\n",
      "Epoch 468/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0373 - val_loss: 0.0376\n",
      "Epoch 469/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0372 - val_loss: 0.0375\n",
      "Epoch 470/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0371 - val_loss: 0.0374\n",
      "Epoch 471/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0370 - val_loss: 0.0373\n",
      "Epoch 472/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0369 - val_loss: 0.0372\n",
      "Epoch 473/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0368 - val_loss: 0.0371\n",
      "Epoch 474/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0367 - val_loss: 0.0370\n",
      "Epoch 475/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0366 - val_loss: 0.0369\n",
      "Epoch 476/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0365 - val_loss: 0.0368\n",
      "Epoch 477/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0364 - val_loss: 0.0367\n",
      "Epoch 478/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0363 - val_loss: 0.0366\n",
      "Epoch 479/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0362 - val_loss: 0.0365\n",
      "Epoch 480/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0361 - val_loss: 0.0364\n",
      "Epoch 481/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0360 - val_loss: 0.0363\n",
      "Epoch 482/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0360 - val_loss: 0.0362\n",
      "Epoch 483/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0359 - val_loss: 0.0361\n",
      "Epoch 484/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0358 - val_loss: 0.0360\n",
      "Epoch 485/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0357 - val_loss: 0.0359\n",
      "Epoch 486/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0356 - val_loss: 0.0358\n",
      "Epoch 487/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0355 - val_loss: 0.0357\n",
      "Epoch 488/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0354 - val_loss: 0.0356\n",
      "Epoch 489/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0353 - val_loss: 0.0355\n",
      "Epoch 490/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0352 - val_loss: 0.0354\n",
      "Epoch 491/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0351 - val_loss: 0.0353\n",
      "Epoch 492/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0350 - val_loss: 0.0352\n",
      "Epoch 493/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0349 - val_loss: 0.0351\n",
      "Epoch 494/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0348 - val_loss: 0.0350\n",
      "Epoch 495/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0347 - val_loss: 0.0349\n",
      "Epoch 496/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0346 - val_loss: 0.0348\n",
      "Epoch 497/10000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.0345 - val_loss: 0.0348\n",
      "Epoch 498/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0344 - val_loss: 0.0347\n",
      "Epoch 499/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0343 - val_loss: 0.0346\n",
      "Epoch 500/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0342 - val_loss: 0.0345\n",
      "Epoch 501/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0341 - val_loss: 0.0344\n",
      "Epoch 502/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0341 - val_loss: 0.0343\n",
      "Epoch 503/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0340 - val_loss: 0.0342\n",
      "Epoch 504/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0339 - val_loss: 0.0341\n",
      "Epoch 505/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0338 - val_loss: 0.0340\n",
      "Epoch 506/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0337 - val_loss: 0.0339\n",
      "Epoch 507/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0336 - val_loss: 0.0338\n",
      "Epoch 508/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0335 - val_loss: 0.0337\n",
      "Epoch 509/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0334 - val_loss: 0.0336\n",
      "Epoch 510/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0333 - val_loss: 0.0335\n",
      "Epoch 511/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0332 - val_loss: 0.0334\n",
      "Epoch 512/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0331 - val_loss: 0.0333\n",
      "Epoch 513/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0330 - val_loss: 0.0332\n",
      "Epoch 514/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0330 - val_loss: 0.0332\n",
      "Epoch 515/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0329 - val_loss: 0.0331\n",
      "Epoch 516/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0328 - val_loss: 0.0330\n",
      "Epoch 517/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0327 - val_loss: 0.0329\n",
      "Epoch 518/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0326 - val_loss: 0.0328\n",
      "Epoch 519/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0325 - val_loss: 0.0327\n",
      "Epoch 520/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0324 - val_loss: 0.0326\n",
      "Epoch 521/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0323 - val_loss: 0.0325\n",
      "Epoch 522/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0322 - val_loss: 0.0324\n",
      "Epoch 523/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0321 - val_loss: 0.0323\n",
      "Epoch 524/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0320 - val_loss: 0.0322\n",
      "Epoch 525/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0320 - val_loss: 0.0321\n",
      "Epoch 526/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0319 - val_loss: 0.0321\n",
      "Epoch 527/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0318 - val_loss: 0.0320\n",
      "Epoch 528/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0317 - val_loss: 0.0319\n",
      "Epoch 529/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0316 - val_loss: 0.0318\n",
      "Epoch 530/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0315 - val_loss: 0.0317\n",
      "Epoch 531/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0314 - val_loss: 0.0316\n",
      "Epoch 532/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0313 - val_loss: 0.0315\n",
      "Epoch 533/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0312 - val_loss: 0.0314\n",
      "Epoch 534/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0312 - val_loss: 0.0313\n",
      "Epoch 535/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0311 - val_loss: 0.0312\n",
      "Epoch 536/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0310 - val_loss: 0.0312\n",
      "Epoch 537/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0309 - val_loss: 0.0311\n",
      "Epoch 538/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0308 - val_loss: 0.0310\n",
      "Epoch 539/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0307 - val_loss: 0.0309\n",
      "Epoch 540/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0306 - val_loss: 0.0308\n",
      "Epoch 541/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0305 - val_loss: 0.0307\n",
      "Epoch 542/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0305 - val_loss: 0.0306\n",
      "Epoch 543/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0304 - val_loss: 0.0305\n",
      "Epoch 544/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0303 - val_loss: 0.0305\n",
      "Epoch 545/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0302 - val_loss: 0.0304\n",
      "Epoch 546/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0301 - val_loss: 0.0303\n",
      "Epoch 547/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0300 - val_loss: 0.0302\n",
      "Epoch 548/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0299 - val_loss: 0.0301\n",
      "Epoch 549/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0299 - val_loss: 0.0300\n",
      "Epoch 550/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0298 - val_loss: 0.0299\n",
      "Epoch 551/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0297 - val_loss: 0.0298\n",
      "Epoch 552/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0296 - val_loss: 0.0298\n",
      "Epoch 553/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0295 - val_loss: 0.0297\n",
      "Epoch 554/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0294 - val_loss: 0.0296\n",
      "Epoch 555/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0294 - val_loss: 0.0295\n",
      "Epoch 556/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0293 - val_loss: 0.0294\n",
      "Epoch 557/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0292 - val_loss: 0.0293\n",
      "Epoch 558/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0291 - val_loss: 0.0292\n",
      "Epoch 559/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0290 - val_loss: 0.0292\n",
      "Epoch 560/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0289 - val_loss: 0.0291\n",
      "Epoch 561/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0288 - val_loss: 0.0290\n",
      "Epoch 562/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0288 - val_loss: 0.0289\n",
      "Epoch 563/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0287 - val_loss: 0.0288\n",
      "Epoch 564/10000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.0286 - val_loss: 0.0287\n",
      "Epoch 565/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0285 - val_loss: 0.0287\n",
      "Epoch 566/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0284 - val_loss: 0.0286\n",
      "Epoch 567/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0284 - val_loss: 0.0285\n",
      "Epoch 568/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0283 - val_loss: 0.0284\n",
      "Epoch 569/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0282 - val_loss: 0.0283\n",
      "Epoch 570/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0281 - val_loss: 0.0282\n",
      "Epoch 571/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0280 - val_loss: 0.0282\n",
      "Epoch 572/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0279 - val_loss: 0.0281\n",
      "Epoch 573/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0279 - val_loss: 0.0280\n",
      "Epoch 574/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0278 - val_loss: 0.0279\n",
      "Epoch 575/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0277 - val_loss: 0.0278\n",
      "Epoch 576/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0276 - val_loss: 0.0277\n",
      "Epoch 577/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0275 - val_loss: 0.0277\n",
      "Epoch 578/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0275 - val_loss: 0.0276\n",
      "Epoch 579/10000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.0274 - val_loss: 0.0275\n",
      "Epoch 580/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0273 - val_loss: 0.0274\n",
      "Epoch 581/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0272 - val_loss: 0.0273\n",
      "Epoch 582/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0271 - val_loss: 0.0273\n",
      "Epoch 583/10000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.0271 - val_loss: 0.0272\n",
      "Epoch 584/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0270 - val_loss: 0.0271\n",
      "Epoch 585/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0269 - val_loss: 0.0270\n",
      "Epoch 586/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0268 - val_loss: 0.0269\n",
      "Epoch 587/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0267 - val_loss: 0.0269\n",
      "Epoch 588/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0267 - val_loss: 0.0268\n",
      "Epoch 589/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0266 - val_loss: 0.0267\n",
      "Epoch 590/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0265 - val_loss: 0.0266\n",
      "Epoch 591/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0264 - val_loss: 0.0265\n",
      "Epoch 592/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0263 - val_loss: 0.0265\n",
      "Epoch 593/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0263 - val_loss: 0.0264\n",
      "Epoch 594/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0262 - val_loss: 0.0263\n",
      "Epoch 595/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0261 - val_loss: 0.0262\n",
      "Epoch 596/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0260 - val_loss: 0.0261\n",
      "Epoch 597/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0259 - val_loss: 0.0261\n",
      "Epoch 598/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0259 - val_loss: 0.0260\n",
      "Epoch 599/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0258 - val_loss: 0.0259\n",
      "Epoch 600/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0257 - val_loss: 0.0258\n",
      "Epoch 601/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0256 - val_loss: 0.0257\n",
      "Epoch 602/10000\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 0.0256 - val_loss: 0.0257\n",
      "Epoch 603/10000\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.0255 - val_loss: 0.0256\n",
      "Epoch 604/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0254 - val_loss: 0.0255\n",
      "Epoch 605/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0253 - val_loss: 0.0254\n",
      "Epoch 606/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0253 - val_loss: 0.0254\n",
      "Epoch 607/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0252 - val_loss: 0.0253\n",
      "Epoch 608/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0251 - val_loss: 0.0252\n",
      "Epoch 609/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0250 - val_loss: 0.0251\n",
      "Epoch 610/10000\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.0250 - val_loss: 0.0251\n",
      "Epoch 611/10000\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.0249 - val_loss: 0.0250\n",
      "Epoch 612/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0248 - val_loss: 0.0249\n",
      "Epoch 613/10000\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.0247 - val_loss: 0.0248\n",
      "Epoch 614/10000\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.0247 - val_loss: 0.0247\n",
      "Epoch 615/10000\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.0246 - val_loss: 0.0247\n",
      "Epoch 616/10000\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.0245 - val_loss: 0.0246\n",
      "Epoch 617/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0244 - val_loss: 0.0245\n",
      "Epoch 618/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0244 - val_loss: 0.0244\n",
      "Epoch 619/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0243 - val_loss: 0.0244\n",
      "Epoch 620/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0242 - val_loss: 0.0243\n",
      "Epoch 621/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0241 - val_loss: 0.0242\n",
      "Epoch 622/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0241 - val_loss: 0.0241\n",
      "Epoch 623/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0240 - val_loss: 0.0241\n",
      "Epoch 624/10000\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.0239 - val_loss: 0.0240\n",
      "Epoch 625/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0238 - val_loss: 0.0239\n",
      "Epoch 626/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0238 - val_loss: 0.0239\n",
      "Epoch 627/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0237 - val_loss: 0.0238\n",
      "Epoch 628/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0236 - val_loss: 0.0237\n",
      "Epoch 629/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0235 - val_loss: 0.0236\n",
      "Epoch 630/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0235 - val_loss: 0.0236\n",
      "Epoch 631/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0234 - val_loss: 0.0235\n",
      "Epoch 632/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0233 - val_loss: 0.0234\n",
      "Epoch 633/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0233 - val_loss: 0.0233\n",
      "Epoch 634/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0232 - val_loss: 0.0233\n",
      "Epoch 635/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0231 - val_loss: 0.0232\n",
      "Epoch 636/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0230 - val_loss: 0.0231\n",
      "Epoch 637/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0230 - val_loss: 0.0230\n",
      "Epoch 638/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0229 - val_loss: 0.0230\n",
      "Epoch 639/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0228 - val_loss: 0.0229\n",
      "Epoch 640/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0228 - val_loss: 0.0228\n",
      "Epoch 641/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0227 - val_loss: 0.0228\n",
      "Epoch 642/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0226 - val_loss: 0.0227\n",
      "Epoch 643/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0225 - val_loss: 0.0226\n",
      "Epoch 644/10000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.0225 - val_loss: 0.0225\n",
      "Epoch 645/10000\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.0224 - val_loss: 0.0225\n",
      "Epoch 646/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0223 - val_loss: 0.0224\n",
      "Epoch 647/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0223 - val_loss: 0.0223\n",
      "Epoch 648/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0222 - val_loss: 0.0223\n",
      "Epoch 649/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0221 - val_loss: 0.0222\n",
      "Epoch 650/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0221 - val_loss: 0.0221\n",
      "Epoch 651/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0220 - val_loss: 0.0221\n",
      "Epoch 652/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0219 - val_loss: 0.0220\n",
      "Epoch 653/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0218 - val_loss: 0.0219\n",
      "Epoch 654/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0218 - val_loss: 0.0218\n",
      "Epoch 655/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0217 - val_loss: 0.0218\n",
      "Epoch 656/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0216 - val_loss: 0.0217\n",
      "Epoch 657/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0216 - val_loss: 0.0216\n",
      "Epoch 658/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0215 - val_loss: 0.0216\n",
      "Epoch 659/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0214 - val_loss: 0.0215\n",
      "Epoch 660/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0214 - val_loss: 0.0214\n",
      "Epoch 661/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0213 - val_loss: 0.0214\n",
      "Epoch 662/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0212 - val_loss: 0.0213\n",
      "Epoch 663/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0212 - val_loss: 0.0212\n",
      "Epoch 664/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0211 - val_loss: 0.0212\n",
      "Epoch 665/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0210 - val_loss: 0.0211\n",
      "Epoch 666/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0210 - val_loss: 0.0210\n",
      "Epoch 667/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0209 - val_loss: 0.0210\n",
      "Epoch 668/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0208 - val_loss: 0.0209\n",
      "Epoch 669/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0208 - val_loss: 0.0208\n",
      "Epoch 670/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0207 - val_loss: 0.0208\n",
      "Epoch 671/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0206 - val_loss: 0.0207\n",
      "Epoch 672/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0206 - val_loss: 0.0206\n",
      "Epoch 673/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0205 - val_loss: 0.0206\n",
      "Epoch 674/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0204 - val_loss: 0.0205\n",
      "Epoch 675/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0204 - val_loss: 0.0204\n",
      "Epoch 676/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0203 - val_loss: 0.0204\n",
      "Epoch 677/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0202 - val_loss: 0.0203\n",
      "Epoch 678/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0202 - val_loss: 0.0202\n",
      "Epoch 679/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0201 - val_loss: 0.0202\n",
      "Epoch 680/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0200 - val_loss: 0.0201\n",
      "Epoch 681/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0200 - val_loss: 0.0200\n",
      "Epoch 682/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0199 - val_loss: 0.0200\n",
      "Epoch 683/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0198 - val_loss: 0.0199\n",
      "Epoch 684/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0198 - val_loss: 0.0198\n",
      "Epoch 685/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0197 - val_loss: 0.0198\n",
      "Epoch 686/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0197 - val_loss: 0.0197\n",
      "Epoch 687/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0196 - val_loss: 0.0196\n",
      "Epoch 688/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0195 - val_loss: 0.0196\n",
      "Epoch 689/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0195 - val_loss: 0.0195\n",
      "Epoch 690/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0194 - val_loss: 0.0194\n",
      "Epoch 691/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0193 - val_loss: 0.0194\n",
      "Epoch 692/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0193 - val_loss: 0.0193\n",
      "Epoch 693/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0192 - val_loss: 0.0193\n",
      "Epoch 694/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0191 - val_loss: 0.0192\n",
      "Epoch 695/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0191 - val_loss: 0.0191\n",
      "Epoch 696/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0190 - val_loss: 0.0191\n",
      "Epoch 697/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0190 - val_loss: 0.0190\n",
      "Epoch 698/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0189 - val_loss: 0.0189\n",
      "Epoch 699/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0188 - val_loss: 0.0189\n",
      "Epoch 700/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0188 - val_loss: 0.0188\n",
      "Epoch 701/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0187 - val_loss: 0.0188\n",
      "Epoch 702/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0186 - val_loss: 0.0187\n",
      "Epoch 703/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0186 - val_loss: 0.0186\n",
      "Epoch 704/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0185 - val_loss: 0.0186\n",
      "Epoch 705/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0185 - val_loss: 0.0185\n",
      "Epoch 706/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0184 - val_loss: 0.0184\n",
      "Epoch 707/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0183 - val_loss: 0.0184\n",
      "Epoch 708/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0183 - val_loss: 0.0183\n",
      "Epoch 709/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0182 - val_loss: 0.0183\n",
      "Epoch 710/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0182 - val_loss: 0.0182\n",
      "Epoch 711/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0181 - val_loss: 0.0181\n",
      "Epoch 712/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0180 - val_loss: 0.0181\n",
      "Epoch 713/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0180 - val_loss: 0.0180\n",
      "Epoch 714/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0179 - val_loss: 0.0180\n",
      "Epoch 715/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0179 - val_loss: 0.0179\n",
      "Epoch 716/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0178 - val_loss: 0.0178\n",
      "Epoch 717/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0177 - val_loss: 0.0178\n",
      "Epoch 718/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0177 - val_loss: 0.0177\n",
      "Epoch 719/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0176 - val_loss: 0.0177\n",
      "Epoch 720/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0176 - val_loss: 0.0176\n",
      "Epoch 721/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0175 - val_loss: 0.0175\n",
      "Epoch 722/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0174 - val_loss: 0.0175\n",
      "Epoch 723/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0174 - val_loss: 0.0174\n",
      "Epoch 724/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0173 - val_loss: 0.0174\n",
      "Epoch 725/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0173 - val_loss: 0.0173\n",
      "Epoch 726/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0172 - val_loss: 0.0172\n",
      "Epoch 727/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0171 - val_loss: 0.0172\n",
      "Epoch 728/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0171 - val_loss: 0.0171\n",
      "Epoch 729/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0170 - val_loss: 0.0171\n",
      "Epoch 730/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0170 - val_loss: 0.0170\n",
      "Epoch 731/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0169 - val_loss: 0.0169\n",
      "Epoch 732/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0169 - val_loss: 0.0169\n",
      "Epoch 733/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 734/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 735/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 736/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 737/10000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 738/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0165 - val_loss: 0.0165\n",
      "Epoch 739/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0165 - val_loss: 0.0165\n",
      "Epoch 740/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0164 - val_loss: 0.0164\n",
      "Epoch 741/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0163 - val_loss: 0.0164\n",
      "Epoch 742/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0163 - val_loss: 0.0163\n",
      "Epoch 743/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0162 - val_loss: 0.0163\n",
      "Epoch 744/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0162 - val_loss: 0.0162\n",
      "Epoch 745/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0161 - val_loss: 0.0161\n",
      "Epoch 746/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0161 - val_loss: 0.0161\n",
      "Epoch 747/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0160 - val_loss: 0.0160\n",
      "Epoch 748/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0159 - val_loss: 0.0160\n",
      "Epoch 749/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0159 - val_loss: 0.0159\n",
      "Epoch 750/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0158 - val_loss: 0.0159\n",
      "Epoch 751/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0158 - val_loss: 0.0158\n",
      "Epoch 752/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0157 - val_loss: 0.0158\n",
      "Epoch 753/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0157 - val_loss: 0.0157\n",
      "Epoch 754/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0156 - val_loss: 0.0156\n",
      "Epoch 755/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0156 - val_loss: 0.0156\n",
      "Epoch 756/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0155 - val_loss: 0.0155\n",
      "Epoch 757/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0155 - val_loss: 0.0155\n",
      "Epoch 758/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0154 - val_loss: 0.0154\n",
      "Epoch 759/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0153 - val_loss: 0.0154\n",
      "Epoch 760/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0153 - val_loss: 0.0153\n",
      "Epoch 761/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0152 - val_loss: 0.0153\n",
      "Epoch 762/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0152 - val_loss: 0.0152\n",
      "Epoch 763/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0151 - val_loss: 0.0152\n",
      "Epoch 764/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0151 - val_loss: 0.0151\n",
      "Epoch 765/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0150 - val_loss: 0.0150\n",
      "Epoch 766/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0150 - val_loss: 0.0150\n",
      "Epoch 767/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0149 - val_loss: 0.0149\n",
      "Epoch 768/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0149 - val_loss: 0.0149\n",
      "Epoch 769/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0148 - val_loss: 0.0148\n",
      "Epoch 770/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0148 - val_loss: 0.0148\n",
      "Epoch 771/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0147 - val_loss: 0.0147\n",
      "Epoch 772/10000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.0147 - val_loss: 0.0147\n",
      "Epoch 773/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0146 - val_loss: 0.0146\n",
      "Epoch 774/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0146 - val_loss: 0.0146\n",
      "Epoch 775/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0145 - val_loss: 0.0145\n",
      "Epoch 776/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0144 - val_loss: 0.0145\n",
      "Epoch 777/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0144 - val_loss: 0.0144\n",
      "Epoch 778/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0143 - val_loss: 0.0144\n",
      "Epoch 779/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0143 - val_loss: 0.0143\n",
      "Epoch 780/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0142 - val_loss: 0.0143\n",
      "Epoch 781/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0142 - val_loss: 0.0142\n",
      "Epoch 782/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0141 - val_loss: 0.0142\n",
      "Epoch 783/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0141 - val_loss: 0.0141\n",
      "Epoch 784/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0140 - val_loss: 0.0141\n",
      "Epoch 785/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0140 - val_loss: 0.0140\n",
      "Epoch 786/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0139 - val_loss: 0.0140\n",
      "Epoch 787/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0139 - val_loss: 0.0139\n",
      "Epoch 788/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0138 - val_loss: 0.0139\n",
      "Epoch 789/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0138 - val_loss: 0.0138\n",
      "Epoch 790/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0137 - val_loss: 0.0138\n",
      "Epoch 791/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0137 - val_loss: 0.0137\n",
      "Epoch 792/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0136 - val_loss: 0.0137\n",
      "Epoch 793/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0136 - val_loss: 0.0136\n",
      "Epoch 794/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0135 - val_loss: 0.0136\n",
      "Epoch 795/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0135 - val_loss: 0.0135\n",
      "Epoch 796/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0134 - val_loss: 0.0135\n",
      "Epoch 797/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0134 - val_loss: 0.0134\n",
      "Epoch 798/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0133 - val_loss: 0.0134\n",
      "Epoch 799/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0133 - val_loss: 0.0133\n",
      "Epoch 800/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0132 - val_loss: 0.0133\n",
      "Epoch 801/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0132 - val_loss: 0.0132\n",
      "Epoch 802/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0131 - val_loss: 0.0132\n",
      "Epoch 803/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0131 - val_loss: 0.0131\n",
      "Epoch 804/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0131 - val_loss: 0.0131\n",
      "Epoch 805/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0130 - val_loss: 0.0130\n",
      "Epoch 806/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0130 - val_loss: 0.0130\n",
      "Epoch 807/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0129 - val_loss: 0.0129\n",
      "Epoch 808/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0129 - val_loss: 0.0129\n",
      "Epoch 809/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0128 - val_loss: 0.0128\n",
      "Epoch 810/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0128 - val_loss: 0.0128\n",
      "Epoch 811/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0127 - val_loss: 0.0127\n",
      "Epoch 812/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0127 - val_loss: 0.0127\n",
      "Epoch 813/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0126 - val_loss: 0.0126\n",
      "Epoch 814/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0126 - val_loss: 0.0126\n",
      "Epoch 815/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0125 - val_loss: 0.0125\n",
      "Epoch 816/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0125 - val_loss: 0.0125\n",
      "Epoch 817/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0124 - val_loss: 0.0124\n",
      "Epoch 818/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0124 - val_loss: 0.0124\n",
      "Epoch 819/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0123 - val_loss: 0.0123\n",
      "Epoch 820/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0123 - val_loss: 0.0123\n",
      "Epoch 821/10000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.0122 - val_loss: 0.0123\n",
      "Epoch 822/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0122 - val_loss: 0.0122\n",
      "Epoch 823/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0122 - val_loss: 0.0122\n",
      "Epoch 824/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0121 - val_loss: 0.0121\n",
      "Epoch 825/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0121 - val_loss: 0.0121\n",
      "Epoch 826/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0120 - val_loss: 0.0120\n",
      "Epoch 827/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0120 - val_loss: 0.0120\n",
      "Epoch 828/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0119 - val_loss: 0.0119\n",
      "Epoch 829/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0119 - val_loss: 0.0119\n",
      "Epoch 830/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 831/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 832/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 833/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 834/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 835/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0116 - val_loss: 0.0116\n",
      "Epoch 836/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0116 - val_loss: 0.0116\n",
      "Epoch 837/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0115 - val_loss: 0.0115\n",
      "Epoch 838/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0115 - val_loss: 0.0115\n",
      "Epoch 839/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0114 - val_loss: 0.0114\n",
      "Epoch 840/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0114 - val_loss: 0.0114\n",
      "Epoch 841/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0114 - val_loss: 0.0114\n",
      "Epoch 842/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0113 - val_loss: 0.0113\n",
      "Epoch 843/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0113 - val_loss: 0.0113\n",
      "Epoch 844/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0112 - val_loss: 0.0112\n",
      "Epoch 845/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0112 - val_loss: 0.0112\n",
      "Epoch 846/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0111 - val_loss: 0.0111\n",
      "Epoch 847/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0111 - val_loss: 0.0111\n",
      "Epoch 848/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0110 - val_loss: 0.0111\n",
      "Epoch 849/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0110 - val_loss: 0.0110\n",
      "Epoch 850/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0110 - val_loss: 0.0110\n",
      "Epoch 851/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0109 - val_loss: 0.0109\n",
      "Epoch 852/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0109 - val_loss: 0.0109\n",
      "Epoch 853/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0108 - val_loss: 0.0108\n",
      "Epoch 854/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0108 - val_loss: 0.0108\n",
      "Epoch 855/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0108 - val_loss: 0.0108\n",
      "Epoch 856/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0107 - val_loss: 0.0107\n",
      "Epoch 857/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0107 - val_loss: 0.0107\n",
      "Epoch 858/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0106 - val_loss: 0.0106\n",
      "Epoch 859/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0106 - val_loss: 0.0106\n",
      "Epoch 860/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0105 - val_loss: 0.0105\n",
      "Epoch 861/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0105 - val_loss: 0.0105\n",
      "Epoch 862/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0105 - val_loss: 0.0105\n",
      "Epoch 863/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0104 - val_loss: 0.0104\n",
      "Epoch 864/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0104 - val_loss: 0.0104\n",
      "Epoch 865/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0103 - val_loss: 0.0103\n",
      "Epoch 866/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0103 - val_loss: 0.0103\n",
      "Epoch 867/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0103 - val_loss: 0.0103\n",
      "Epoch 868/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0102 - val_loss: 0.0102\n",
      "Epoch 869/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0102 - val_loss: 0.0102\n",
      "Epoch 870/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0101 - val_loss: 0.0101\n",
      "Epoch 871/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0101 - val_loss: 0.0101\n",
      "Epoch 872/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0101 - val_loss: 0.0101\n",
      "Epoch 873/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0100 - val_loss: 0.0100\n",
      "Epoch 874/10000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.0100 - val_loss: 0.0100\n",
      "Epoch 875/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0099 - val_loss: 0.0099\n",
      "Epoch 876/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0099 - val_loss: 0.0099\n",
      "Epoch 877/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0099 - val_loss: 0.0099\n",
      "Epoch 878/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0098 - val_loss: 0.0098\n",
      "Epoch 879/10000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.0098 - val_loss: 0.0098\n",
      "Epoch 880/10000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0097\n",
      "Epoch 881/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0097 - val_loss: 0.0097\n",
      "Epoch 882/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0097 - val_loss: 0.0097\n",
      "Epoch 883/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0096 - val_loss: 0.0096\n",
      "Epoch 884/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0096 - val_loss: 0.0096\n",
      "Epoch 885/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0095 - val_loss: 0.0095\n",
      "Epoch 886/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0095 - val_loss: 0.0095\n",
      "Epoch 887/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0095 - val_loss: 0.0095\n",
      "Epoch 888/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0094 - val_loss: 0.0094\n",
      "Epoch 889/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0094 - val_loss: 0.0094\n",
      "Epoch 890/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0094 - val_loss: 0.0093\n",
      "Epoch 891/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0093 - val_loss: 0.0093\n",
      "Epoch 892/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0093 - val_loss: 0.0093\n",
      "Epoch 893/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0092 - val_loss: 0.0092\n",
      "Epoch 894/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0092 - val_loss: 0.0092\n",
      "Epoch 895/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0092 - val_loss: 0.0092\n",
      "Epoch 896/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0091 - val_loss: 0.0091\n",
      "Epoch 897/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0091 - val_loss: 0.0091\n",
      "Epoch 898/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0090 - val_loss: 0.0090\n",
      "Epoch 899/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0090 - val_loss: 0.0090\n",
      "Epoch 900/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0090 - val_loss: 0.0090\n",
      "Epoch 901/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0089 - val_loss: 0.0089\n",
      "Epoch 902/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0089 - val_loss: 0.0089\n",
      "Epoch 903/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0089 - val_loss: 0.0089\n",
      "Epoch 904/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0088 - val_loss: 0.0088\n",
      "Epoch 905/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0088 - val_loss: 0.0088\n",
      "Epoch 906/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0088 - val_loss: 0.0087\n",
      "Epoch 907/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0087 - val_loss: 0.0087\n",
      "Epoch 908/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0087 - val_loss: 0.0087\n",
      "Epoch 909/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0086 - val_loss: 0.0086\n",
      "Epoch 910/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0086 - val_loss: 0.0086\n",
      "Epoch 911/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0086 - val_loss: 0.0086\n",
      "Epoch 912/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0085 - val_loss: 0.0085\n",
      "Epoch 913/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0085 - val_loss: 0.0085\n",
      "Epoch 914/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0085 - val_loss: 0.0085\n",
      "Epoch 915/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0084 - val_loss: 0.0084\n",
      "Epoch 916/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0084 - val_loss: 0.0084\n",
      "Epoch 917/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0084 - val_loss: 0.0084\n",
      "Epoch 918/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0083 - val_loss: 0.0083\n",
      "Epoch 919/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0083 - val_loss: 0.0083\n",
      "Epoch 920/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0083 - val_loss: 0.0082\n",
      "Epoch 921/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0082 - val_loss: 0.0082\n",
      "Epoch 922/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0082 - val_loss: 0.0082\n",
      "Epoch 923/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0082 - val_loss: 0.0081\n",
      "Epoch 924/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0081 - val_loss: 0.0081\n",
      "Epoch 925/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0081 - val_loss: 0.0081\n",
      "Epoch 926/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0080 - val_loss: 0.0080\n",
      "Epoch 927/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0080 - val_loss: 0.0080\n",
      "Epoch 928/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0080 - val_loss: 0.0080\n",
      "Epoch 929/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0079 - val_loss: 0.0079\n",
      "Epoch 930/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0079 - val_loss: 0.0079\n",
      "Epoch 931/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0079 - val_loss: 0.0079\n",
      "Epoch 932/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0078 - val_loss: 0.0078\n",
      "Epoch 933/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0078 - val_loss: 0.0078\n",
      "Epoch 934/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0078 - val_loss: 0.0078\n",
      "Epoch 935/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0077 - val_loss: 0.0077\n",
      "Epoch 936/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0077 - val_loss: 0.0077\n",
      "Epoch 937/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0077 - val_loss: 0.0077\n",
      "Epoch 938/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0076 - val_loss: 0.0076\n",
      "Epoch 939/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0076 - val_loss: 0.0076\n",
      "Epoch 940/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0076 - val_loss: 0.0076\n",
      "Epoch 941/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0075 - val_loss: 0.0075\n",
      "Epoch 942/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0075 - val_loss: 0.0075\n",
      "Epoch 943/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0075 - val_loss: 0.0075\n",
      "Epoch 944/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0074 - val_loss: 0.0074\n",
      "Epoch 945/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0074 - val_loss: 0.0074\n",
      "Epoch 946/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0074 - val_loss: 0.0074\n",
      "Epoch 947/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0074 - val_loss: 0.0073\n",
      "Epoch 948/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0073 - val_loss: 0.0073\n",
      "Epoch 949/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0073 - val_loss: 0.0073\n",
      "Epoch 950/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0073 - val_loss: 0.0072\n",
      "Epoch 951/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 952/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 953/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0072 - val_loss: 0.0071\n",
      "Epoch 954/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0071 - val_loss: 0.0071\n",
      "Epoch 955/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0071 - val_loss: 0.0071\n",
      "Epoch 956/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0071 - val_loss: 0.0071\n",
      "Epoch 957/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 958/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 959/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 960/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 961/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 962/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 963/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 964/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 965/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 966/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0068 - val_loss: 0.0067\n",
      "Epoch 967/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0067 - val_loss: 0.0067\n",
      "Epoch 968/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0067 - val_loss: 0.0067\n",
      "Epoch 969/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0067 - val_loss: 0.0067\n",
      "Epoch 970/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0066 - val_loss: 0.0066\n",
      "Epoch 971/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0066 - val_loss: 0.0066\n",
      "Epoch 972/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0066 - val_loss: 0.0066\n",
      "Epoch 973/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0065 - val_loss: 0.0065\n",
      "Epoch 974/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0065 - val_loss: 0.0065\n",
      "Epoch 975/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0065 - val_loss: 0.0065\n",
      "Epoch 976/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0065 - val_loss: 0.0065\n",
      "Epoch 977/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0064 - val_loss: 0.0064\n",
      "Epoch 978/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0064 - val_loss: 0.0064\n",
      "Epoch 979/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0064 - val_loss: 0.0064\n",
      "Epoch 980/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0063 - val_loss: 0.0063\n",
      "Epoch 981/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0063 - val_loss: 0.0063\n",
      "Epoch 982/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0063 - val_loss: 0.0063\n",
      "Epoch 983/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0063 - val_loss: 0.0062\n",
      "Epoch 984/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0062 - val_loss: 0.0062\n",
      "Epoch 985/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0062 - val_loss: 0.0062\n",
      "Epoch 986/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0062 - val_loss: 0.0062\n",
      "Epoch 987/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0061 - val_loss: 0.0061\n",
      "Epoch 988/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0061 - val_loss: 0.0061\n",
      "Epoch 989/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0061 - val_loss: 0.0061\n",
      "Epoch 990/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0061 - val_loss: 0.0060\n",
      "Epoch 991/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0060 - val_loss: 0.0060\n",
      "Epoch 992/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0060 - val_loss: 0.0060\n",
      "Epoch 993/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0060 - val_loss: 0.0060\n",
      "Epoch 994/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0059 - val_loss: 0.0059\n",
      "Epoch 995/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0059 - val_loss: 0.0059\n",
      "Epoch 996/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0059 - val_loss: 0.0059\n",
      "Epoch 997/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0059 - val_loss: 0.0059\n",
      "Epoch 998/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 999/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 1000/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 1001/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 1002/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0057 - val_loss: 0.0057\n",
      "Epoch 1003/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0057 - val_loss: 0.0057\n",
      "Epoch 1004/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0057 - val_loss: 0.0057\n",
      "Epoch 1005/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0056 - val_loss: 0.0056\n",
      "Epoch 1006/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0056 - val_loss: 0.0056\n",
      "Epoch 1007/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0056 - val_loss: 0.0056\n",
      "Epoch 1008/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0056 - val_loss: 0.0056\n",
      "Epoch 1009/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0055 - val_loss: 0.0055\n",
      "Epoch 1010/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0055 - val_loss: 0.0055\n",
      "Epoch 1011/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0055 - val_loss: 0.0055\n",
      "Epoch 1012/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0055 - val_loss: 0.0055\n",
      "Epoch 1013/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0054 - val_loss: 0.0054\n",
      "Epoch 1014/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0054 - val_loss: 0.0054\n",
      "Epoch 1015/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0054 - val_loss: 0.0054\n",
      "Epoch 1016/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0054 - val_loss: 0.0053\n",
      "Epoch 1017/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0053 - val_loss: 0.0053\n",
      "Epoch 1018/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0053 - val_loss: 0.0053\n",
      "Epoch 1019/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0053 - val_loss: 0.0053\n",
      "Epoch 1020/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0053 - val_loss: 0.0052\n",
      "Epoch 1021/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0052 - val_loss: 0.0052\n",
      "Epoch 1022/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0052 - val_loss: 0.0052\n",
      "Epoch 1023/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0052 - val_loss: 0.0052\n",
      "Epoch 1024/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0052 - val_loss: 0.0051\n",
      "Epoch 1025/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0051 - val_loss: 0.0051\n",
      "Epoch 1026/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0051 - val_loss: 0.0051\n",
      "Epoch 1027/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0051 - val_loss: 0.0051\n",
      "Epoch 1028/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0051 - val_loss: 0.0050\n",
      "Epoch 1029/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0050 - val_loss: 0.0050\n",
      "Epoch 1030/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0050 - val_loss: 0.0050\n",
      "Epoch 1031/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0050 - val_loss: 0.0050\n",
      "Epoch 1032/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0050 - val_loss: 0.0049\n",
      "Epoch 1033/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0049 - val_loss: 0.0049\n",
      "Epoch 1034/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0049 - val_loss: 0.0049\n",
      "Epoch 1035/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0049 - val_loss: 0.0049\n",
      "Epoch 1036/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0049 - val_loss: 0.0049\n",
      "Epoch 1037/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0048 - val_loss: 0.0048\n",
      "Epoch 1038/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0048 - val_loss: 0.0048\n",
      "Epoch 1039/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0048 - val_loss: 0.0048\n",
      "Epoch 1040/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0048 - val_loss: 0.0048\n",
      "Epoch 1041/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0047 - val_loss: 0.0047\n",
      "Epoch 1042/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0047 - val_loss: 0.0047\n",
      "Epoch 1043/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0047 - val_loss: 0.0047\n",
      "Epoch 1044/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0047 - val_loss: 0.0047\n",
      "Epoch 1045/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0047 - val_loss: 0.0046\n",
      "Epoch 1046/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 1047/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 1048/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 1049/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0046 - val_loss: 0.0045\n",
      "Epoch 1050/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0045 - val_loss: 0.0045\n",
      "Epoch 1051/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0045 - val_loss: 0.0045\n",
      "Epoch 1052/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0045 - val_loss: 0.0045\n",
      "Epoch 1053/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0045 - val_loss: 0.0045\n",
      "Epoch 1054/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 1055/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 1056/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 1057/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 1058/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0044 - val_loss: 0.0043\n",
      "Epoch 1059/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 1060/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 1061/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 1062/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 1063/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 1064/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 1065/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 1066/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 1067/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0042 - val_loss: 0.0041\n",
      "Epoch 1068/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 1069/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 1070/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 1071/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 1072/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0041 - val_loss: 0.0040\n",
      "Epoch 1073/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 1074/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 1075/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 1076/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 1077/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0040 - val_loss: 0.0039\n",
      "Epoch 1078/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 1079/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 1080/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 1081/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 1082/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0039 - val_loss: 0.0038\n",
      "Epoch 1083/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 1084/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 1085/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 1086/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 1087/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 1088/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 1089/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 1090/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 1091/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 1092/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 1093/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 1094/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 1095/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 1096/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 1097/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 1098/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 1099/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 1100/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 1101/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 1102/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 1103/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 1104/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 1105/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 1106/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 1107/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 1108/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0034 - val_loss: 0.0033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1109/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 1110/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 1111/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 1112/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 1113/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 1114/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 1115/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 1116/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 1117/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 1118/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 1119/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 1120/10000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 1121/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 1122/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 1123/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 1124/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 1125/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 1126/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 1127/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 1128/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 1129/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 1130/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 1131/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 1132/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 1133/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 1134/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 1135/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 1136/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 1137/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0029 - val_loss: 0.0028\n",
      "Epoch 1138/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 1139/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 1140/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 1141/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 1142/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 1143/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0028 - val_loss: 0.0027\n",
      "Epoch 1144/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 1145/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 1146/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 1147/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 1148/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 1149/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0027 - val_loss: 0.0026\n",
      "Epoch 1150/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 1151/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 1152/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 1153/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 1154/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 1155/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 1156/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 1157/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 1158/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 1159/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 1160/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 1161/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 1162/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 1163/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 1164/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 1165/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 1166/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 1167/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 1168/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 1169/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 1170/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 1171/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 1172/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 1173/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 1174/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 1175/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 1176/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 1177/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0023 - val_loss: 0.0022\n",
      "Epoch 1178/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 1179/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 1180/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 1181/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 1182/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 1183/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 1184/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0022 - val_loss: 0.0021\n",
      "Epoch 1185/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 1186/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 1187/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 1188/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 1189/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 1190/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 1191/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 1192/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 1193/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 1194/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 1195/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 1196/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 1197/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 1198/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 1199/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 1200/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 1201/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 1202/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 1203/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 1204/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 1205/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 1206/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 1207/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 1208/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 1209/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 1210/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 1211/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 1212/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 1213/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 1214/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 1215/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 1216/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 1217/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 1218/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 1219/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 1220/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 1221/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 1222/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 1223/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 1224/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 1225/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 1226/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 1227/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 1228/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 1229/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 1230/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 1231/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 1232/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 1233/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 1234/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 1235/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 1236/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 1237/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 1238/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 1239/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 1240/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 1241/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 1242/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 1243/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 1244/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 1245/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 1246/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 1247/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 1248/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 1249/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 1250/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 1251/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 1252/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 1253/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 1254/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 1255/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 1256/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 1257/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 1258/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 1259/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 1260/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 1261/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 1262/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 1263/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 1264/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 1265/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1266/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 1267/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 1268/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 1269/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 1270/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 1271/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 1272/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 1273/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 1274/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 1275/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 1276/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 1277/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 1278/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 1279/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 1280/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 1281/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 1282/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 1283/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 1284/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 1285/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 1286/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 1287/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 1288/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 1289/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 1290/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 1291/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 1292/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 1293/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 1294/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 1295/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 1296/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 1297/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 1298/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0010 - val_loss: 9.9644e-04\n",
      "Epoch 1299/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0010 - val_loss: 9.8941e-04\n",
      "Epoch 1300/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 9.9501e-04 - val_loss: 9.8180e-04\n",
      "Epoch 1301/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 9.8780e-04 - val_loss: 9.7462e-04\n",
      "Epoch 1302/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 9.8054e-04 - val_loss: 9.6739e-04\n",
      "Epoch 1303/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 9.7335e-04 - val_loss: 9.6053e-04\n",
      "Epoch 1304/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 9.6625e-04 - val_loss: 9.5306e-04\n",
      "Epoch 1305/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 9.5908e-04 - val_loss: 9.4594e-04\n",
      "Epoch 1306/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 9.5206e-04 - val_loss: 9.3896e-04\n",
      "Epoch 1307/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 9.4504e-04 - val_loss: 9.3220e-04\n",
      "Epoch 1308/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 9.3812e-04 - val_loss: 9.2507e-04\n",
      "Epoch 1309/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 9.3111e-04 - val_loss: 9.1798e-04\n",
      "Epoch 1310/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 9.2419e-04 - val_loss: 9.1113e-04\n",
      "Epoch 1311/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 9.1733e-04 - val_loss: 9.0425e-04\n",
      "Epoch 1312/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 9.1053e-04 - val_loss: 8.9756e-04\n",
      "Epoch 1313/10000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 9.0375e-04 - val_loss: 8.9081e-04\n",
      "Epoch 1314/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 8.9699e-04 - val_loss: 8.8410e-04\n",
      "Epoch 1315/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 8.9032e-04 - val_loss: 8.7736e-04\n",
      "Epoch 1316/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 8.8365e-04 - val_loss: 8.7068e-04\n",
      "Epoch 1317/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 8.7701e-04 - val_loss: 8.6410e-04\n",
      "Epoch 1318/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 8.7042e-04 - val_loss: 8.5762e-04\n",
      "Epoch 1319/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 8.6388e-04 - val_loss: 8.5114e-04\n",
      "Epoch 1320/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 8.5740e-04 - val_loss: 8.4480e-04\n",
      "Epoch 1321/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 8.5094e-04 - val_loss: 8.3839e-04\n",
      "Epoch 1322/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 8.4451e-04 - val_loss: 8.3166e-04\n",
      "Epoch 1323/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 8.3810e-04 - val_loss: 8.2559e-04\n",
      "Epoch 1324/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 8.3180e-04 - val_loss: 8.2006e-04\n",
      "Epoch 1325/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 8.2555e-04 - val_loss: 8.1304e-04\n",
      "Epoch 1326/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 8.1917e-04 - val_loss: 8.0673e-04\n",
      "Epoch 1327/10000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 8.1297e-04 - val_loss: 8.0032e-04\n",
      "Epoch 1328/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 8.0674e-04 - val_loss: 7.9397e-04\n",
      "Epoch 1329/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 8.0059e-04 - val_loss: 7.8789e-04\n",
      "Epoch 1330/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 7.9447e-04 - val_loss: 7.8179e-04\n",
      "Epoch 1331/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 7.8838e-04 - val_loss: 7.7577e-04\n",
      "Epoch 1332/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 7.8236e-04 - val_loss: 7.6960e-04\n",
      "Epoch 1333/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 7.7635e-04 - val_loss: 7.6360e-04\n",
      "Epoch 1334/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 7.7036e-04 - val_loss: 7.5758e-04\n",
      "Epoch 1335/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 7.6442e-04 - val_loss: 7.5176e-04\n",
      "Epoch 1336/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 7.5853e-04 - val_loss: 7.4577e-04\n",
      "Epoch 1337/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 7.5266e-04 - val_loss: 7.3996e-04\n",
      "Epoch 1338/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 7.4684e-04 - val_loss: 7.3409e-04\n",
      "Epoch 1339/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 7.4107e-04 - val_loss: 7.2856e-04\n",
      "Epoch 1340/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 7.3527e-04 - val_loss: 7.2260e-04\n",
      "Epoch 1341/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 8ms/step - loss: 7.2960e-04 - val_loss: 7.1701e-04\n",
      "Epoch 1342/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 7.2389e-04 - val_loss: 7.1126e-04\n",
      "Epoch 1343/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 7.1825e-04 - val_loss: 7.0560e-04\n",
      "Epoch 1344/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 7.1264e-04 - val_loss: 7.0009e-04\n",
      "Epoch 1345/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 7.0706e-04 - val_loss: 6.9443e-04\n",
      "Epoch 1346/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 7.0155e-04 - val_loss: 6.8905e-04\n",
      "Epoch 1347/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 6.9603e-04 - val_loss: 6.8360e-04\n",
      "Epoch 1348/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 6.9056e-04 - val_loss: 6.7804e-04\n",
      "Epoch 1349/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 6.8513e-04 - val_loss: 6.7266e-04\n",
      "Epoch 1350/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 6.7971e-04 - val_loss: 6.6726e-04\n",
      "Epoch 1351/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 6.7433e-04 - val_loss: 6.6193e-04\n",
      "Epoch 1352/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 6.6901e-04 - val_loss: 6.5649e-04\n",
      "Epoch 1353/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 6.6371e-04 - val_loss: 6.5119e-04\n",
      "Epoch 1354/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 6.5847e-04 - val_loss: 6.4596e-04\n",
      "Epoch 1355/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 6.5323e-04 - val_loss: 6.4075e-04\n",
      "Epoch 1356/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 6.4802e-04 - val_loss: 6.3556e-04\n",
      "Epoch 1357/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 6.4291e-04 - val_loss: 6.3041e-04\n",
      "Epoch 1358/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 6.3771e-04 - val_loss: 6.2532e-04\n",
      "Epoch 1359/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 6.3263e-04 - val_loss: 6.2031e-04\n",
      "Epoch 1360/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 6.2763e-04 - val_loss: 6.1518e-04\n",
      "Epoch 1361/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 6.2255e-04 - val_loss: 6.1018e-04\n",
      "Epoch 1362/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 6.1756e-04 - val_loss: 6.0523e-04\n",
      "Epoch 1363/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 6.1262e-04 - val_loss: 6.0041e-04\n",
      "Epoch 1364/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 6.0775e-04 - val_loss: 5.9532e-04\n",
      "Epoch 1365/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 6.0280e-04 - val_loss: 5.9047e-04\n",
      "Epoch 1366/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 5.9794e-04 - val_loss: 5.8560e-04\n",
      "Epoch 1367/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 5.9310e-04 - val_loss: 5.8076e-04\n",
      "Epoch 1368/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 5.8829e-04 - val_loss: 5.7597e-04\n",
      "Epoch 1369/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 5.8355e-04 - val_loss: 5.7123e-04\n",
      "Epoch 1370/10000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 5.7878e-04 - val_loss: 5.6648e-04\n",
      "Epoch 1371/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 5.7407e-04 - val_loss: 5.6182e-04\n",
      "Epoch 1372/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 5.6944e-04 - val_loss: 5.5714e-04\n",
      "Epoch 1373/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 5.6476e-04 - val_loss: 5.5252e-04\n",
      "Epoch 1374/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 5.6016e-04 - val_loss: 5.4794e-04\n",
      "Epoch 1375/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 5.5562e-04 - val_loss: 5.4335e-04\n",
      "Epoch 1376/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 5.5105e-04 - val_loss: 5.3883e-04\n",
      "Epoch 1377/10000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 5.4656e-04 - val_loss: 5.3437e-04\n",
      "Epoch 1378/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 5.4203e-04 - val_loss: 5.2986e-04\n",
      "Epoch 1379/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 5.3756e-04 - val_loss: 5.2542e-04\n",
      "Epoch 1380/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 5.3313e-04 - val_loss: 5.2105e-04\n",
      "Epoch 1381/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 5.2874e-04 - val_loss: 5.1662e-04\n",
      "Epoch 1382/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 5.2443e-04 - val_loss: 5.1233e-04\n",
      "Epoch 1383/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 5.2004e-04 - val_loss: 5.0801e-04\n",
      "Epoch 1384/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 5.1571e-04 - val_loss: 5.0386e-04\n",
      "Epoch 1385/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 5.1152e-04 - val_loss: 4.9962e-04\n",
      "Epoch 1386/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 5.0726e-04 - val_loss: 4.9532e-04\n",
      "Epoch 1387/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 5.0306e-04 - val_loss: 4.9105e-04\n",
      "Epoch 1388/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.9882e-04 - val_loss: 4.8697e-04\n",
      "Epoch 1389/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.9477e-04 - val_loss: 4.8276e-04\n",
      "Epoch 1390/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.9057e-04 - val_loss: 4.7859e-04\n",
      "Epoch 1391/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.8644e-04 - val_loss: 4.7451e-04\n",
      "Epoch 1392/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.8238e-04 - val_loss: 4.7049e-04\n",
      "Epoch 1393/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.7834e-04 - val_loss: 4.6641e-04\n",
      "Epoch 1394/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.7435e-04 - val_loss: 4.6249e-04\n",
      "Epoch 1395/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.7038e-04 - val_loss: 4.5841e-04\n",
      "Epoch 1396/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.6640e-04 - val_loss: 4.5448e-04\n",
      "Epoch 1397/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.6247e-04 - val_loss: 4.5059e-04\n",
      "Epoch 1398/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.5856e-04 - val_loss: 4.4664e-04\n",
      "Epoch 1399/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.5469e-04 - val_loss: 4.4278e-04\n",
      "Epoch 1400/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.5091e-04 - val_loss: 4.3895e-04\n",
      "Epoch 1401/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.4707e-04 - val_loss: 4.3516e-04\n",
      "Epoch 1402/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.4327e-04 - val_loss: 4.3140e-04\n",
      "Epoch 1403/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.3951e-04 - val_loss: 4.2768e-04\n",
      "Epoch 1404/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.3578e-04 - val_loss: 4.2393e-04\n",
      "Epoch 1405/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.3206e-04 - val_loss: 4.2029e-04\n",
      "Epoch 1406/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.2841e-04 - val_loss: 4.1667e-04\n",
      "Epoch 1407/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.2477e-04 - val_loss: 4.1300e-04\n",
      "Epoch 1408/10000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 4.2110e-04 - val_loss: 4.0933e-04\n",
      "Epoch 1409/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.1751e-04 - val_loss: 4.0575e-04\n",
      "Epoch 1410/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.1394e-04 - val_loss: 4.0225e-04\n",
      "Epoch 1411/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.1041e-04 - val_loss: 3.9873e-04\n",
      "Epoch 1412/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4.0690e-04 - val_loss: 3.9521e-04\n",
      "Epoch 1413/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.0340e-04 - val_loss: 3.9167e-04\n",
      "Epoch 1414/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 8ms/step - loss: 3.9998e-04 - val_loss: 3.8832e-04\n",
      "Epoch 1415/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.9650e-04 - val_loss: 3.8500e-04\n",
      "Epoch 1416/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.9310e-04 - val_loss: 3.8148e-04\n",
      "Epoch 1417/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.8968e-04 - val_loss: 3.7808e-04\n",
      "Epoch 1418/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.8630e-04 - val_loss: 3.7472e-04\n",
      "Epoch 1419/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.8298e-04 - val_loss: 3.7149e-04\n",
      "Epoch 1420/10000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 3.7966e-04 - val_loss: 3.6802e-04\n",
      "Epoch 1421/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.7635e-04 - val_loss: 3.6476e-04\n",
      "Epoch 1422/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.7309e-04 - val_loss: 3.6176e-04\n",
      "Epoch 1423/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.6995e-04 - val_loss: 3.5827e-04\n",
      "Epoch 1424/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.6665e-04 - val_loss: 3.5504e-04\n",
      "Epoch 1425/10000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 3.6348e-04 - val_loss: 3.5188e-04\n",
      "Epoch 1426/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.6030e-04 - val_loss: 3.4869e-04\n",
      "Epoch 1427/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.5711e-04 - val_loss: 3.4557e-04\n",
      "Epoch 1428/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.5405e-04 - val_loss: 3.4244e-04\n",
      "Epoch 1429/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.5092e-04 - val_loss: 3.3950e-04\n",
      "Epoch 1430/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.4797e-04 - val_loss: 3.3632e-04\n",
      "Epoch 1431/10000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 3.4483e-04 - val_loss: 3.3330e-04\n",
      "Epoch 1432/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.4180e-04 - val_loss: 3.3029e-04\n",
      "Epoch 1433/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.3880e-04 - val_loss: 3.2732e-04\n",
      "Epoch 1434/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.3589e-04 - val_loss: 3.2436e-04\n",
      "Epoch 1435/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.3288e-04 - val_loss: 3.2147e-04\n",
      "Epoch 1436/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2995e-04 - val_loss: 3.1849e-04\n",
      "Epoch 1437/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2706e-04 - val_loss: 3.1557e-04\n",
      "Epoch 1438/10000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 3.2414e-04 - val_loss: 3.1272e-04\n",
      "Epoch 1439/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2128e-04 - val_loss: 3.0984e-04\n",
      "Epoch 1440/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.1844e-04 - val_loss: 3.0702e-04\n",
      "Epoch 1441/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.1563e-04 - val_loss: 3.0428e-04\n",
      "Epoch 1442/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.1284e-04 - val_loss: 3.0140e-04\n",
      "Epoch 1443/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.1006e-04 - val_loss: 2.9863e-04\n",
      "Epoch 1444/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.0729e-04 - val_loss: 2.9590e-04\n",
      "Epoch 1445/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.0458e-04 - val_loss: 2.9317e-04\n",
      "Epoch 1446/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.0188e-04 - val_loss: 2.9048e-04\n",
      "Epoch 1447/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.9919e-04 - val_loss: 2.8784e-04\n",
      "Epoch 1448/10000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 2.9651e-04 - val_loss: 2.8515e-04\n",
      "Epoch 1449/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.9386e-04 - val_loss: 2.8260e-04\n",
      "Epoch 1450/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.9127e-04 - val_loss: 2.8020e-04\n",
      "Epoch 1451/10000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 2.8874e-04 - val_loss: 2.7734e-04\n",
      "Epoch 1452/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.8610e-04 - val_loss: 2.7476e-04\n",
      "Epoch 1453/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.8353e-04 - val_loss: 2.7224e-04\n",
      "Epoch 1454/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.8099e-04 - val_loss: 2.6966e-04\n",
      "Epoch 1455/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.7848e-04 - val_loss: 2.6715e-04\n",
      "Epoch 1456/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.7597e-04 - val_loss: 2.6468e-04\n",
      "Epoch 1457/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.7351e-04 - val_loss: 2.6224e-04\n",
      "Epoch 1458/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.7105e-04 - val_loss: 2.5977e-04\n",
      "Epoch 1459/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.6860e-04 - val_loss: 2.5733e-04\n",
      "Epoch 1460/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.6618e-04 - val_loss: 2.5494e-04\n",
      "Epoch 1461/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.6380e-04 - val_loss: 2.5259e-04\n",
      "Epoch 1462/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.6140e-04 - val_loss: 2.5021e-04\n",
      "Epoch 1463/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.5908e-04 - val_loss: 2.4784e-04\n",
      "Epoch 1464/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.5674e-04 - val_loss: 2.4549e-04\n",
      "Epoch 1465/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.5443e-04 - val_loss: 2.4316e-04\n",
      "Epoch 1466/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.5211e-04 - val_loss: 2.4089e-04\n",
      "Epoch 1467/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.4983e-04 - val_loss: 2.3861e-04\n",
      "Epoch 1468/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.4758e-04 - val_loss: 2.3636e-04\n",
      "Epoch 1469/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.4537e-04 - val_loss: 2.3412e-04\n",
      "Epoch 1470/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.4310e-04 - val_loss: 2.3191e-04\n",
      "Epoch 1471/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.4093e-04 - val_loss: 2.2975e-04\n",
      "Epoch 1472/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.3873e-04 - val_loss: 2.2756e-04\n",
      "Epoch 1473/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 2.3657e-04 - val_loss: 2.2537e-04\n",
      "Epoch 1474/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.3441e-04 - val_loss: 2.2325e-04\n",
      "Epoch 1475/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.3232e-04 - val_loss: 2.2114e-04\n",
      "Epoch 1476/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.3021e-04 - val_loss: 2.1912e-04\n",
      "Epoch 1477/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.2811e-04 - val_loss: 2.1693e-04\n",
      "Epoch 1478/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.2601e-04 - val_loss: 2.1494e-04\n",
      "Epoch 1479/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.2397e-04 - val_loss: 2.1291e-04\n",
      "Epoch 1480/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 2.2192e-04 - val_loss: 2.1082e-04\n",
      "Epoch 1481/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.1990e-04 - val_loss: 2.0877e-04\n",
      "Epoch 1482/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.1791e-04 - val_loss: 2.0678e-04\n",
      "Epoch 1483/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.1593e-04 - val_loss: 2.0486e-04\n",
      "Epoch 1484/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.1396e-04 - val_loss: 2.0294e-04\n",
      "Epoch 1485/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.1203e-04 - val_loss: 2.0089e-04\n",
      "Epoch 1486/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 2.1007e-04 - val_loss: 1.9895e-04\n",
      "Epoch 1487/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 8ms/step - loss: 2.0815e-04 - val_loss: 1.9703e-04\n",
      "Epoch 1488/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.0625e-04 - val_loss: 1.9515e-04\n",
      "Epoch 1489/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.0435e-04 - val_loss: 1.9369e-04\n",
      "Epoch 1490/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.0267e-04 - val_loss: 1.9143e-04\n",
      "Epoch 1491/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2.0062e-04 - val_loss: 1.8960e-04\n",
      "Epoch 1492/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.9879e-04 - val_loss: 1.8776e-04\n",
      "Epoch 1493/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.9697e-04 - val_loss: 1.8595e-04\n",
      "Epoch 1494/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.9517e-04 - val_loss: 1.8419e-04\n",
      "Epoch 1495/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.9339e-04 - val_loss: 1.8243e-04\n",
      "Epoch 1496/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.9163e-04 - val_loss: 1.8061e-04\n",
      "Epoch 1497/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.8986e-04 - val_loss: 1.7884e-04\n",
      "Epoch 1498/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.8811e-04 - val_loss: 1.7717e-04\n",
      "Epoch 1499/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.8640e-04 - val_loss: 1.7538e-04\n",
      "Epoch 1500/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.8469e-04 - val_loss: 1.7369e-04\n",
      "Epoch 1501/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.8301e-04 - val_loss: 1.7200e-04\n",
      "Epoch 1502/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.8133e-04 - val_loss: 1.7032e-04\n",
      "Epoch 1503/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.7965e-04 - val_loss: 1.6873e-04\n",
      "Epoch 1504/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.7803e-04 - val_loss: 1.6702e-04\n",
      "Epoch 1505/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.7642e-04 - val_loss: 1.6540e-04\n",
      "Epoch 1506/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.7474e-04 - val_loss: 1.6394e-04\n",
      "Epoch 1507/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.7319e-04 - val_loss: 1.6233e-04\n",
      "Epoch 1508/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.7158e-04 - val_loss: 1.6073e-04\n",
      "Epoch 1509/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.7000e-04 - val_loss: 1.5906e-04\n",
      "Epoch 1510/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.6844e-04 - val_loss: 1.5748e-04\n",
      "Epoch 1511/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.6691e-04 - val_loss: 1.5607e-04\n",
      "Epoch 1512/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.6551e-04 - val_loss: 1.5444e-04\n",
      "Epoch 1513/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.6384e-04 - val_loss: 1.5297e-04\n",
      "Epoch 1514/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.6233e-04 - val_loss: 1.5145e-04\n",
      "Epoch 1515/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.6085e-04 - val_loss: 1.4995e-04\n",
      "Epoch 1516/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.5939e-04 - val_loss: 1.4855e-04\n",
      "Epoch 1517/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.5796e-04 - val_loss: 1.4706e-04\n",
      "Epoch 1518/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.5648e-04 - val_loss: 1.4562e-04\n",
      "Epoch 1519/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.5503e-04 - val_loss: 1.4416e-04\n",
      "Epoch 1520/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.5361e-04 - val_loss: 1.4282e-04\n",
      "Epoch 1521/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.5222e-04 - val_loss: 1.4138e-04\n",
      "Epoch 1522/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.5082e-04 - val_loss: 1.3994e-04\n",
      "Epoch 1523/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.4943e-04 - val_loss: 1.3857e-04\n",
      "Epoch 1524/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.4806e-04 - val_loss: 1.3732e-04\n",
      "Epoch 1525/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.4671e-04 - val_loss: 1.3595e-04\n",
      "Epoch 1526/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.4537e-04 - val_loss: 1.3459e-04\n",
      "Epoch 1527/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.4404e-04 - val_loss: 1.3323e-04\n",
      "Epoch 1528/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.4283e-04 - val_loss: 1.3189e-04\n",
      "Epoch 1529/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.4143e-04 - val_loss: 1.3059e-04\n",
      "Epoch 1530/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.4013e-04 - val_loss: 1.2931e-04\n",
      "Epoch 1531/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.3888e-04 - val_loss: 1.2805e-04\n",
      "Epoch 1532/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.3761e-04 - val_loss: 1.2681e-04\n",
      "Epoch 1533/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.3641e-04 - val_loss: 1.2556e-04\n",
      "Epoch 1534/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.3516e-04 - val_loss: 1.2431e-04\n",
      "Epoch 1535/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.3391e-04 - val_loss: 1.2308e-04\n",
      "Epoch 1536/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.3265e-04 - val_loss: 1.2188e-04\n",
      "Epoch 1537/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.3144e-04 - val_loss: 1.2066e-04\n",
      "Epoch 1538/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.3022e-04 - val_loss: 1.1944e-04\n",
      "Epoch 1539/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.2908e-04 - val_loss: 1.1827e-04\n",
      "Epoch 1540/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.2794e-04 - val_loss: 1.1710e-04\n",
      "Epoch 1541/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.2673e-04 - val_loss: 1.1596e-04\n",
      "Epoch 1542/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.2562e-04 - val_loss: 1.1482e-04\n",
      "Epoch 1543/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.2447e-04 - val_loss: 1.1366e-04\n",
      "Epoch 1544/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.2333e-04 - val_loss: 1.1256e-04\n",
      "Epoch 1545/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.2220e-04 - val_loss: 1.1142e-04\n",
      "Epoch 1546/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.2113e-04 - val_loss: 1.1032e-04\n",
      "Epoch 1547/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.2003e-04 - val_loss: 1.0928e-04\n",
      "Epoch 1548/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.1894e-04 - val_loss: 1.0816e-04\n",
      "Epoch 1549/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 1.1785e-04 - val_loss: 1.0717e-04\n",
      "Epoch 1550/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.1681e-04 - val_loss: 1.0604e-04\n",
      "Epoch 1551/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.1575e-04 - val_loss: 1.0498e-04\n",
      "Epoch 1552/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 1.1469e-04 - val_loss: 1.0396e-04\n",
      "Epoch 1553/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.1368e-04 - val_loss: 1.0300e-04\n",
      "Epoch 1554/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.1266e-04 - val_loss: 1.0200e-04\n",
      "Epoch 1555/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.1166e-04 - val_loss: 1.0094e-04\n",
      "Epoch 1556/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.1066e-04 - val_loss: 1.0004e-04\n",
      "Epoch 1557/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.0971e-04 - val_loss: 9.8952e-05\n",
      "Epoch 1558/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.0871e-04 - val_loss: 9.7972e-05\n",
      "Epoch 1559/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.0773e-04 - val_loss: 9.7010e-05\n",
      "Epoch 1560/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 8ms/step - loss: 1.0676e-04 - val_loss: 9.6068e-05\n",
      "Epoch 1561/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.0581e-04 - val_loss: 9.5114e-05\n",
      "Epoch 1562/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.0489e-04 - val_loss: 9.4189e-05\n",
      "Epoch 1563/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 1.0395e-04 - val_loss: 9.3278e-05\n",
      "Epoch 1564/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.0304e-04 - val_loss: 9.2357e-05\n",
      "Epoch 1565/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.0214e-04 - val_loss: 9.1433e-05\n",
      "Epoch 1566/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.0121e-04 - val_loss: 9.0539e-05\n",
      "Epoch 1567/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.0033e-04 - val_loss: 8.9640e-05\n",
      "Epoch 1568/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 9.9458e-05 - val_loss: 8.8759e-05\n",
      "Epoch 1569/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 9.8597e-05 - val_loss: 8.8016e-05\n",
      "Epoch 1570/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 9.7820e-05 - val_loss: 8.7053e-05\n",
      "Epoch 1571/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 9.6894e-05 - val_loss: 8.6267e-05\n",
      "Epoch 1572/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 9.6056e-05 - val_loss: 8.5363e-05\n",
      "Epoch 1573/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 9.5188e-05 - val_loss: 8.4566e-05\n",
      "Epoch 1574/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 9.4422e-05 - val_loss: 8.3821e-05\n",
      "Epoch 1575/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 9.3556e-05 - val_loss: 8.2900e-05\n",
      "Epoch 1576/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 9.2758e-05 - val_loss: 8.2074e-05\n",
      "Epoch 1577/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 9.1969e-05 - val_loss: 8.1285e-05\n",
      "Epoch 1578/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 9.1171e-05 - val_loss: 8.0505e-05\n",
      "Epoch 1579/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 9.0405e-05 - val_loss: 7.9721e-05\n",
      "Epoch 1580/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 8.9587e-05 - val_loss: 7.8940e-05\n",
      "Epoch 1581/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 8.8832e-05 - val_loss: 7.8186e-05\n",
      "Epoch 1582/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 8.8110e-05 - val_loss: 7.7431e-05\n",
      "Epoch 1583/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 8.7341e-05 - val_loss: 7.6841e-05\n",
      "Epoch 1584/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 8.6615e-05 - val_loss: 7.6108e-05\n",
      "Epoch 1585/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 8.5919e-05 - val_loss: 7.5315e-05\n",
      "Epoch 1586/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 8.5162e-05 - val_loss: 7.4491e-05\n",
      "Epoch 1587/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 8.4441e-05 - val_loss: 7.3772e-05\n",
      "Epoch 1588/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 8.3690e-05 - val_loss: 7.3117e-05\n",
      "Epoch 1589/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 8.3048e-05 - val_loss: 7.2379e-05\n",
      "Epoch 1590/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 8.2283e-05 - val_loss: 7.1706e-05\n",
      "Epoch 1591/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 8.1629e-05 - val_loss: 7.1023e-05\n",
      "Epoch 1592/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 8.0937e-05 - val_loss: 7.0339e-05\n",
      "Epoch 1593/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 8.0261e-05 - val_loss: 6.9683e-05\n",
      "Epoch 1594/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 7.9593e-05 - val_loss: 6.8993e-05\n",
      "Epoch 1595/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 7.8957e-05 - val_loss: 6.8472e-05\n",
      "Epoch 1596/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 7.8349e-05 - val_loss: 6.7727e-05\n",
      "Epoch 1597/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 7.7667e-05 - val_loss: 6.7054e-05\n",
      "Epoch 1598/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 7.7022e-05 - val_loss: 6.6480e-05\n",
      "Epoch 1599/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 7.6390e-05 - val_loss: 6.5815e-05\n",
      "Epoch 1600/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 7.5774e-05 - val_loss: 6.5226e-05\n",
      "Epoch 1601/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 7.5183e-05 - val_loss: 6.4570e-05\n",
      "Epoch 1602/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 7.4546e-05 - val_loss: 6.3966e-05\n",
      "Epoch 1603/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 7.3954e-05 - val_loss: 6.3356e-05\n",
      "Epoch 1604/10000\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 7.3351e-05 - val_loss: 6.2772e-05\n",
      "Epoch 1605/10000\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 7.2778e-05 - val_loss: 6.2190e-05\n",
      "Epoch 1606/10000\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 7.2246e-05 - val_loss: 6.1615e-05\n",
      "Epoch 1607/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 7.1606e-05 - val_loss: 6.1208e-05\n",
      "Epoch 1608/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 7.1121e-05 - val_loss: 6.0529e-05\n",
      "Epoch 1609/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 7.0543e-05 - val_loss: 5.9951e-05\n",
      "Epoch 1610/10000\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 6.9965e-05 - val_loss: 5.9372e-05\n",
      "Epoch 1611/10000\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 6.9417e-05 - val_loss: 5.8828e-05\n",
      "Epoch 1612/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 6.8864e-05 - val_loss: 5.8304e-05\n",
      "Epoch 1613/10000\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 6.8340e-05 - val_loss: 5.7780e-05\n",
      "Epoch 1614/10000\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 6.7805e-05 - val_loss: 5.7241e-05\n",
      "Epoch 1615/10000\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 6.7277e-05 - val_loss: 5.6710e-05\n",
      "Epoch 1616/10000\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.6772e-05 - val_loss: 5.6238e-05\n",
      "Epoch 1617/10000\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 6.6307e-05 - val_loss: 5.5770e-05\n",
      "Epoch 1618/10000\n",
      "18/18 [==============================] - 0s 18ms/step - loss: 6.5784e-05 - val_loss: 5.5227e-05\n",
      "Epoch 1619/10000\n",
      "18/18 [==============================] - 0s 17ms/step - loss: 6.5275e-05 - val_loss: 5.4820e-05\n",
      "Epoch 1620/10000\n",
      "18/18 [==============================] - 0s 18ms/step - loss: 6.4826e-05 - val_loss: 5.4229e-05\n",
      "Epoch 1621/10000\n",
      "18/18 [==============================] - 0s 18ms/step - loss: 6.4316e-05 - val_loss: 5.3731e-05\n",
      "Epoch 1622/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 6.3818e-05 - val_loss: 5.3256e-05\n",
      "Epoch 1623/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 6.3336e-05 - val_loss: 5.3156e-05\n",
      "Epoch 1624/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 6.3062e-05 - val_loss: 5.2324e-05\n",
      "Epoch 1625/10000\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 6.2445e-05 - val_loss: 5.1890e-05\n",
      "Epoch 1626/10000\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 6.1972e-05 - val_loss: 5.1424e-05\n",
      "Epoch 1627/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 6.1492e-05 - val_loss: 5.1065e-05\n",
      "Epoch 1628/10000\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 6.1079e-05 - val_loss: 5.0579e-05\n",
      "Epoch 1629/10000\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 6.0666e-05 - val_loss: 5.0075e-05\n",
      "Epoch 1630/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 6.0196e-05 - val_loss: 4.9642e-05\n",
      "Epoch 1631/10000\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 5.9743e-05 - val_loss: 4.9229e-05\n",
      "Epoch 1632/10000\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 5.9353e-05 - val_loss: 4.8860e-05\n",
      "Epoch 1633/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 10ms/step - loss: 5.8955e-05 - val_loss: 4.8424e-05\n",
      "Epoch 1634/10000\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 5.8589e-05 - val_loss: 4.7977e-05\n",
      "Epoch 1635/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 5.8092e-05 - val_loss: 4.7586e-05\n",
      "Epoch 1636/10000\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 5.7703e-05 - val_loss: 4.7157e-05\n",
      "Epoch 1637/10000\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 5.7305e-05 - val_loss: 4.6782e-05\n",
      "Epoch 1638/10000\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 5.6913e-05 - val_loss: 4.6370e-05\n",
      "Epoch 1639/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 5.6500e-05 - val_loss: 4.5987e-05\n",
      "Epoch 1640/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 5.6125e-05 - val_loss: 4.5606e-05\n",
      "Epoch 1641/10000\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 5.5777e-05 - val_loss: 4.5239e-05\n",
      "Epoch 1642/10000\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 5.5405e-05 - val_loss: 4.4889e-05\n",
      "Epoch 1643/10000\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 5.5036e-05 - val_loss: 4.4486e-05\n",
      "Epoch 1644/10000\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 5.4642e-05 - val_loss: 4.4163e-05\n",
      "Epoch 1645/10000\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 5.4305e-05 - val_loss: 4.3806e-05\n",
      "Epoch 1646/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 5.3976e-05 - val_loss: 4.3413e-05\n",
      "Epoch 1647/10000\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 5.3610e-05 - val_loss: 4.3108e-05\n",
      "Epoch 1648/10000\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 5.3248e-05 - val_loss: 4.2726e-05\n",
      "Epoch 1649/10000\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 5.2898e-05 - val_loss: 4.2391e-05\n",
      "Epoch 1650/10000\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 5.2587e-05 - val_loss: 4.2098e-05\n",
      "Epoch 1651/10000\n",
      "18/18 [==============================] - 0s 16ms/step - loss: 5.2257e-05 - val_loss: 4.1761e-05\n",
      "Epoch 1652/10000\n",
      "18/18 [==============================] - 0s 16ms/step - loss: 5.1921e-05 - val_loss: 4.1399e-05\n",
      "Epoch 1653/10000\n",
      "18/18 [==============================] - 0s 16ms/step - loss: 5.1611e-05 - val_loss: 4.1085e-05\n",
      "Epoch 1654/10000\n",
      "18/18 [==============================] - 0s 18ms/step - loss: 5.1274e-05 - val_loss: 4.0760e-05\n",
      "Epoch 1655/10000\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 5.0957e-05 - val_loss: 4.0445e-05\n",
      "Epoch 1656/10000\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 5.0653e-05 - val_loss: 4.0182e-05\n",
      "Epoch 1657/10000\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 5.0366e-05 - val_loss: 3.9839e-05\n",
      "Epoch 1658/10000\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 5.0043e-05 - val_loss: 3.9530e-05\n",
      "Epoch 1659/10000\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 4.9748e-05 - val_loss: 3.9240e-05\n",
      "Epoch 1660/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 4.9454e-05 - val_loss: 3.8950e-05\n",
      "Epoch 1661/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 4.9181e-05 - val_loss: 3.8679e-05\n",
      "Epoch 1662/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 4.8884e-05 - val_loss: 3.8403e-05\n",
      "Epoch 1663/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 4.8627e-05 - val_loss: 3.8096e-05\n",
      "Epoch 1664/10000\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 4.8316e-05 - val_loss: 3.7822e-05\n",
      "Epoch 1665/10000\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 4.8071e-05 - val_loss: 3.7540e-05\n",
      "Epoch 1666/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 4.7766e-05 - val_loss: 3.7316e-05\n",
      "Epoch 1667/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4.7533e-05 - val_loss: 3.7021e-05\n",
      "Epoch 1668/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.7275e-05 - val_loss: 3.6743e-05\n",
      "Epoch 1669/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.6988e-05 - val_loss: 3.6496e-05\n",
      "Epoch 1670/10000\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 4.6753e-05 - val_loss: 3.6233e-05\n",
      "Epoch 1671/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 4.6486e-05 - val_loss: 3.5994e-05\n",
      "Epoch 1672/10000\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 4.6226e-05 - val_loss: 3.5787e-05\n",
      "Epoch 1673/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4.6013e-05 - val_loss: 3.5516e-05\n",
      "Epoch 1674/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4.5763e-05 - val_loss: 3.5260e-05\n",
      "Epoch 1675/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4.5515e-05 - val_loss: 3.5052e-05\n",
      "Epoch 1676/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.5292e-05 - val_loss: 3.4899e-05\n",
      "Epoch 1677/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4.5097e-05 - val_loss: 3.4562e-05\n",
      "Epoch 1678/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 4.4810e-05 - val_loss: 3.4334e-05\n",
      "Epoch 1679/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.4596e-05 - val_loss: 3.4143e-05\n",
      "Epoch 1680/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.4368e-05 - val_loss: 3.3944e-05\n",
      "Epoch 1681/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4.4176e-05 - val_loss: 3.3672e-05\n",
      "Epoch 1682/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4.3945e-05 - val_loss: 3.3463e-05\n",
      "Epoch 1683/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.3739e-05 - val_loss: 3.3501e-05\n",
      "Epoch 1684/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.3625e-05 - val_loss: 3.3097e-05\n",
      "Epoch 1685/10000\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 4.3343e-05 - val_loss: 3.2861e-05\n",
      "Epoch 1686/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 4.3131e-05 - val_loss: 3.2662e-05\n",
      "Epoch 1687/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.2931e-05 - val_loss: 3.2448e-05\n",
      "Epoch 1688/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.2719e-05 - val_loss: 3.2246e-05\n",
      "Epoch 1689/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.2536e-05 - val_loss: 3.2058e-05\n",
      "Epoch 1690/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.2335e-05 - val_loss: 3.1866e-05\n",
      "Epoch 1691/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4.2156e-05 - val_loss: 3.1680e-05\n",
      "Epoch 1692/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4.1958e-05 - val_loss: 3.1532e-05\n",
      "Epoch 1693/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 4.1795e-05 - val_loss: 3.1374e-05\n",
      "Epoch 1694/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.1647e-05 - val_loss: 3.1161e-05\n",
      "Epoch 1695/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 4.1446e-05 - val_loss: 3.0970e-05\n",
      "Epoch 1696/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4.1281e-05 - val_loss: 3.0805e-05\n",
      "Epoch 1697/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.1089e-05 - val_loss: 3.0629e-05\n",
      "Epoch 1698/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.0913e-05 - val_loss: 3.0464e-05\n",
      "Epoch 1699/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 4.0801e-05 - val_loss: 3.0351e-05\n",
      "Epoch 1700/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.0596e-05 - val_loss: 3.0133e-05\n",
      "Epoch 1701/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.0453e-05 - val_loss: 3.0168e-05\n",
      "Epoch 1702/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.0337e-05 - val_loss: 2.9820e-05\n",
      "Epoch 1703/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4.0147e-05 - val_loss: 2.9661e-05\n",
      "Epoch 1704/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.9965e-05 - val_loss: 2.9515e-05\n",
      "Epoch 1705/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.9867e-05 - val_loss: 2.9381e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1706/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.9686e-05 - val_loss: 2.9320e-05\n",
      "Epoch 1707/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.9574e-05 - val_loss: 2.9079e-05\n",
      "Epoch 1708/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.9399e-05 - val_loss: 2.8960e-05\n",
      "Epoch 1709/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.9260e-05 - val_loss: 2.8812e-05\n",
      "Epoch 1710/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.9150e-05 - val_loss: 2.8661e-05\n",
      "Epoch 1711/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.8984e-05 - val_loss: 2.8555e-05\n",
      "Epoch 1712/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.8822e-05 - val_loss: 2.8383e-05\n",
      "Epoch 1713/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.8748e-05 - val_loss: 2.8254e-05\n",
      "Epoch 1714/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.8582e-05 - val_loss: 2.8127e-05\n",
      "Epoch 1715/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.8447e-05 - val_loss: 2.8179e-05\n",
      "Epoch 1716/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.8367e-05 - val_loss: 2.8021e-05\n",
      "Epoch 1717/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.8237e-05 - val_loss: 2.7753e-05\n",
      "Epoch 1718/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.8101e-05 - val_loss: 2.7643e-05\n",
      "Epoch 1719/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.7979e-05 - val_loss: 2.7561e-05\n",
      "Epoch 1720/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.7848e-05 - val_loss: 2.7413e-05\n",
      "Epoch 1721/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.7734e-05 - val_loss: 2.7294e-05\n",
      "Epoch 1722/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3.7667e-05 - val_loss: 2.7178e-05\n",
      "Epoch 1723/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3.7550e-05 - val_loss: 2.7068e-05\n",
      "Epoch 1724/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.7421e-05 - val_loss: 2.6961e-05\n",
      "Epoch 1725/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.7308e-05 - val_loss: 2.6879e-05\n",
      "Epoch 1726/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.7207e-05 - val_loss: 2.6775e-05\n",
      "Epoch 1727/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.7104e-05 - val_loss: 2.6647e-05\n",
      "Epoch 1728/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.7001e-05 - val_loss: 2.6548e-05\n",
      "Epoch 1729/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.6928e-05 - val_loss: 2.6474e-05\n",
      "Epoch 1730/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.6801e-05 - val_loss: 2.6355e-05\n",
      "Epoch 1731/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3.6692e-05 - val_loss: 2.6294e-05\n",
      "Epoch 1732/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.6655e-05 - val_loss: 2.6231e-05\n",
      "Epoch 1733/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.6546e-05 - val_loss: 2.6092e-05\n",
      "Epoch 1734/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.6451e-05 - val_loss: 2.6010e-05\n",
      "Epoch 1735/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3.6345e-05 - val_loss: 2.5895e-05\n",
      "Epoch 1736/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.6274e-05 - val_loss: 2.5826e-05\n",
      "Epoch 1737/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.6150e-05 - val_loss: 2.5719e-05\n",
      "Epoch 1738/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.6098e-05 - val_loss: 2.5734e-05\n",
      "Epoch 1739/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.6049e-05 - val_loss: 2.5560e-05\n",
      "Epoch 1740/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3.5917e-05 - val_loss: 2.5474e-05\n",
      "Epoch 1741/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.5825e-05 - val_loss: 2.5417e-05\n",
      "Epoch 1742/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.5747e-05 - val_loss: 2.5339e-05\n",
      "Epoch 1743/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.5699e-05 - val_loss: 2.5293e-05\n",
      "Epoch 1744/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.5620e-05 - val_loss: 2.5220e-05\n",
      "Epoch 1745/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.5549e-05 - val_loss: 2.5114e-05\n",
      "Epoch 1746/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.5469e-05 - val_loss: 2.5026e-05\n",
      "Epoch 1747/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.5392e-05 - val_loss: 2.5118e-05\n",
      "Epoch 1748/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.5382e-05 - val_loss: 2.4946e-05\n",
      "Epoch 1749/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.5269e-05 - val_loss: 2.4840e-05\n",
      "Epoch 1750/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.5231e-05 - val_loss: 2.4750e-05\n",
      "Epoch 1751/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.5149e-05 - val_loss: 2.4685e-05\n",
      "Epoch 1752/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3.5078e-05 - val_loss: 2.4664e-05\n",
      "Epoch 1753/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.5006e-05 - val_loss: 2.4582e-05\n",
      "Epoch 1754/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.4944e-05 - val_loss: 2.4620e-05\n",
      "Epoch 1755/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3.4882e-05 - val_loss: 2.4531e-05\n",
      "Epoch 1756/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.4835e-05 - val_loss: 2.4476e-05\n",
      "Epoch 1757/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.4796e-05 - val_loss: 2.4325e-05\n",
      "Epoch 1758/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.4696e-05 - val_loss: 2.4265e-05\n",
      "Epoch 1759/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.4651e-05 - val_loss: 2.4206e-05\n",
      "Epoch 1760/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.4568e-05 - val_loss: 2.4324e-05\n",
      "Epoch 1761/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.4580e-05 - val_loss: 2.4184e-05\n",
      "Epoch 1762/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.4495e-05 - val_loss: 2.4546e-05\n",
      "Epoch 1763/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.4578e-05 - val_loss: 2.4031e-05\n",
      "Epoch 1764/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.4371e-05 - val_loss: 2.3969e-05\n",
      "Epoch 1765/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.4317e-05 - val_loss: 2.3896e-05\n",
      "Epoch 1766/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.4283e-05 - val_loss: 2.3864e-05\n",
      "Epoch 1767/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.4256e-05 - val_loss: 2.3824e-05\n",
      "Epoch 1768/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.4203e-05 - val_loss: 2.3754e-05\n",
      "Epoch 1769/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.4157e-05 - val_loss: 2.3721e-05\n",
      "Epoch 1770/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.4100e-05 - val_loss: 2.3770e-05\n",
      "Epoch 1771/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.4054e-05 - val_loss: 2.3623e-05\n",
      "Epoch 1772/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.4043e-05 - val_loss: 2.3586e-05\n",
      "Epoch 1773/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.3972e-05 - val_loss: 2.3610e-05\n",
      "Epoch 1774/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.3928e-05 - val_loss: 2.3561e-05\n",
      "Epoch 1775/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.3904e-05 - val_loss: 2.3470e-05\n",
      "Epoch 1776/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.3839e-05 - val_loss: 2.3466e-05\n",
      "Epoch 1777/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.3814e-05 - val_loss: 2.3414e-05\n",
      "Epoch 1778/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.3764e-05 - val_loss: 2.3508e-05\n",
      "Epoch 1779/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 8ms/step - loss: 3.3725e-05 - val_loss: 2.3313e-05\n",
      "Epoch 1780/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.3724e-05 - val_loss: 2.3322e-05\n",
      "Epoch 1781/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.3653e-05 - val_loss: 2.3240e-05\n",
      "Epoch 1782/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.3642e-05 - val_loss: 2.3284e-05\n",
      "Epoch 1783/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.3628e-05 - val_loss: 2.3387e-05\n",
      "Epoch 1784/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.3607e-05 - val_loss: 2.3291e-05\n",
      "Epoch 1785/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.3562e-05 - val_loss: 2.3206e-05\n",
      "Epoch 1786/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.3510e-05 - val_loss: 2.3087e-05\n",
      "Epoch 1787/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.3485e-05 - val_loss: 2.3066e-05\n",
      "Epoch 1788/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.3464e-05 - val_loss: 2.3028e-05\n",
      "Epoch 1789/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.3399e-05 - val_loss: 2.3128e-05\n",
      "Epoch 1790/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.3427e-05 - val_loss: 2.3015e-05\n",
      "Epoch 1791/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.3352e-05 - val_loss: 2.2963e-05\n",
      "Epoch 1792/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.3335e-05 - val_loss: 2.2910e-05\n",
      "Epoch 1793/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.3291e-05 - val_loss: 2.2981e-05\n",
      "Epoch 1794/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.3281e-05 - val_loss: 2.2877e-05\n",
      "Epoch 1795/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.3272e-05 - val_loss: 2.2822e-05\n",
      "Epoch 1796/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.3250e-05 - val_loss: 2.2811e-05\n",
      "Epoch 1797/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.3199e-05 - val_loss: 2.2844e-05\n",
      "Epoch 1798/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.3174e-05 - val_loss: 2.2831e-05\n",
      "Epoch 1799/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.3151e-05 - val_loss: 2.2728e-05\n",
      "Epoch 1800/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.3170e-05 - val_loss: 2.2719e-05\n",
      "Epoch 1801/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.3141e-05 - val_loss: 2.2698e-05\n",
      "Epoch 1802/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.3132e-05 - val_loss: 2.2672e-05\n",
      "Epoch 1803/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.3092e-05 - val_loss: 2.2641e-05\n",
      "Epoch 1804/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.3077e-05 - val_loss: 2.2620e-05\n",
      "Epoch 1805/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.3042e-05 - val_loss: 2.2612e-05\n",
      "Epoch 1806/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2997e-05 - val_loss: 2.2580e-05\n",
      "Epoch 1807/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.3009e-05 - val_loss: 2.2568e-05\n",
      "Epoch 1808/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2980e-05 - val_loss: 2.2597e-05\n",
      "Epoch 1809/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2949e-05 - val_loss: 2.2542e-05\n",
      "Epoch 1810/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2971e-05 - val_loss: 2.2523e-05\n",
      "Epoch 1811/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2952e-05 - val_loss: 2.2495e-05\n",
      "Epoch 1812/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3.2895e-05 - val_loss: 2.2473e-05\n",
      "Epoch 1813/10000\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 3.2903e-05 - val_loss: 2.2461e-05\n",
      "Epoch 1814/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3.2898e-05 - val_loss: 2.2459e-05\n",
      "Epoch 1815/10000\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 3.2881e-05 - val_loss: 2.2435e-05\n",
      "Epoch 1816/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2823e-05 - val_loss: 2.2417e-05\n",
      "Epoch 1817/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3.2806e-05 - val_loss: 2.2397e-05\n",
      "Epoch 1818/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3.2832e-05 - val_loss: 2.2699e-05\n",
      "Epoch 1819/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3.2828e-05 - val_loss: 2.2382e-05\n",
      "Epoch 1820/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2780e-05 - val_loss: 2.2352e-05\n",
      "Epoch 1821/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2778e-05 - val_loss: 2.2362e-05\n",
      "Epoch 1822/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2758e-05 - val_loss: 2.2606e-05\n",
      "Epoch 1823/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2800e-05 - val_loss: 2.2449e-05\n",
      "Epoch 1824/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3.2748e-05 - val_loss: 2.2301e-05\n",
      "Epoch 1825/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3.2712e-05 - val_loss: 2.2342e-05\n",
      "Epoch 1826/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3.2710e-05 - val_loss: 2.2453e-05\n",
      "Epoch 1827/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2713e-05 - val_loss: 2.2378e-05\n",
      "Epoch 1828/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2677e-05 - val_loss: 2.2264e-05\n",
      "Epoch 1829/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2654e-05 - val_loss: 2.2243e-05\n",
      "Epoch 1830/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2685e-05 - val_loss: 2.2242e-05\n",
      "Epoch 1831/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2655e-05 - val_loss: 2.2260e-05\n",
      "Epoch 1832/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2631e-05 - val_loss: 2.2222e-05\n",
      "Epoch 1833/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2612e-05 - val_loss: 2.2207e-05\n",
      "Epoch 1834/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2603e-05 - val_loss: 2.2661e-05\n",
      "Epoch 1835/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2750e-05 - val_loss: 2.2234e-05\n",
      "Epoch 1836/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2615e-05 - val_loss: 2.2176e-05\n",
      "Epoch 1837/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2621e-05 - val_loss: 2.2178e-05\n",
      "Epoch 1838/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2588e-05 - val_loss: 2.2160e-05\n",
      "Epoch 1839/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2595e-05 - val_loss: 2.2154e-05\n",
      "Epoch 1840/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2561e-05 - val_loss: 2.2145e-05\n",
      "Epoch 1841/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2590e-05 - val_loss: 2.2136e-05\n",
      "Epoch 1842/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2594e-05 - val_loss: 2.2167e-05\n",
      "Epoch 1843/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2545e-05 - val_loss: 2.2118e-05\n",
      "Epoch 1844/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2531e-05 - val_loss: 2.2166e-05\n",
      "Epoch 1845/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3.2522e-05 - val_loss: 2.2105e-05\n",
      "Epoch 1846/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2557e-05 - val_loss: 2.2102e-05\n",
      "Epoch 1847/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2576e-05 - val_loss: 2.2093e-05\n",
      "Epoch 1848/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2497e-05 - val_loss: 2.2086e-05\n",
      "Epoch 1849/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2521e-05 - val_loss: 2.2163e-05\n",
      "Epoch 1850/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2479e-05 - val_loss: 2.2080e-05\n",
      "Epoch 1851/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2502e-05 - val_loss: 2.2071e-05\n",
      "Epoch 1852/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2488e-05 - val_loss: 2.2085e-05\n",
      "Epoch 1853/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2478e-05 - val_loss: 2.2053e-05\n",
      "Epoch 1854/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2493e-05 - val_loss: 2.2091e-05\n",
      "Epoch 1855/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2487e-05 - val_loss: 2.2293e-05\n",
      "Epoch 1856/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2505e-05 - val_loss: 2.2062e-05\n",
      "Epoch 1857/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2438e-05 - val_loss: 2.2037e-05\n",
      "Epoch 1858/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2456e-05 - val_loss: 2.2043e-05\n",
      "Epoch 1859/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2485e-05 - val_loss: 2.2030e-05\n",
      "Epoch 1860/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2458e-05 - val_loss: 2.2038e-05\n",
      "Epoch 1861/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2424e-05 - val_loss: 2.2015e-05\n",
      "Epoch 1862/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2441e-05 - val_loss: 2.2043e-05\n",
      "Epoch 1863/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2437e-05 - val_loss: 2.2070e-05\n",
      "Epoch 1864/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2455e-05 - val_loss: 2.2013e-05\n",
      "Epoch 1865/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2441e-05 - val_loss: 2.1998e-05\n",
      "Epoch 1866/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3.2425e-05 - val_loss: 2.2010e-05\n",
      "Epoch 1867/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3.2456e-05 - val_loss: 2.2303e-05\n",
      "Epoch 1868/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2517e-05 - val_loss: 2.2038e-05\n",
      "Epoch 1869/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2393e-05 - val_loss: 2.2172e-05\n",
      "Epoch 1870/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2456e-05 - val_loss: 2.2168e-05\n",
      "Epoch 1871/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2449e-05 - val_loss: 2.2053e-05\n",
      "Epoch 1872/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2391e-05 - val_loss: 2.1984e-05\n",
      "Epoch 1873/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2381e-05 - val_loss: 2.1971e-05\n",
      "Epoch 1874/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2406e-05 - val_loss: 2.1967e-05\n",
      "Epoch 1875/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2402e-05 - val_loss: 2.1991e-05\n",
      "Epoch 1876/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2398e-05 - val_loss: 2.1981e-05\n",
      "Epoch 1877/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2415e-05 - val_loss: 2.1958e-05\n",
      "Epoch 1878/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2380e-05 - val_loss: 2.1957e-05\n",
      "Epoch 1879/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2383e-05 - val_loss: 2.1960e-05\n",
      "Epoch 1880/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2380e-05 - val_loss: 2.1988e-05\n",
      "Epoch 1881/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2361e-05 - val_loss: 2.1949e-05\n",
      "Epoch 1882/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2387e-05 - val_loss: 2.1948e-05\n",
      "Epoch 1883/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2391e-05 - val_loss: 2.1975e-05\n",
      "Epoch 1884/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2458e-05 - val_loss: 2.1942e-05\n",
      "Epoch 1885/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2356e-05 - val_loss: 2.1953e-05\n",
      "Epoch 1886/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2360e-05 - val_loss: 2.1941e-05\n",
      "Epoch 1887/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2350e-05 - val_loss: 2.1999e-05\n",
      "Epoch 1888/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2372e-05 - val_loss: 2.1937e-05\n",
      "Epoch 1889/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2349e-05 - val_loss: 2.1932e-05\n",
      "Epoch 1890/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2369e-05 - val_loss: 2.1949e-05\n",
      "Epoch 1891/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2397e-05 - val_loss: 2.1929e-05\n",
      "Epoch 1892/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2366e-05 - val_loss: 2.1938e-05\n",
      "Epoch 1893/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2340e-05 - val_loss: 2.1929e-05\n",
      "Epoch 1894/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2348e-05 - val_loss: 2.1953e-05\n",
      "Epoch 1895/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2328e-05 - val_loss: 2.2034e-05\n",
      "Epoch 1896/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2374e-05 - val_loss: 2.2202e-05\n",
      "Epoch 1897/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2392e-05 - val_loss: 2.1925e-05\n",
      "Epoch 1898/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2339e-05 - val_loss: 2.1952e-05\n",
      "Epoch 1899/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2344e-05 - val_loss: 2.1926e-05\n",
      "Epoch 1900/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2338e-05 - val_loss: 2.1932e-05\n",
      "Epoch 1901/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2322e-05 - val_loss: 2.2010e-05\n",
      "Epoch 1902/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2337e-05 - val_loss: 2.2090e-05\n",
      "Epoch 1903/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2365e-05 - val_loss: 2.1945e-05\n",
      "Epoch 1904/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2330e-05 - val_loss: 2.1994e-05\n",
      "Epoch 1905/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2347e-05 - val_loss: 2.1916e-05\n",
      "Epoch 1906/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2353e-05 - val_loss: 2.2088e-05\n",
      "Epoch 1907/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3.2324e-05 - val_loss: 2.1910e-05\n",
      "Epoch 1908/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2381e-05 - val_loss: 2.1970e-05\n",
      "Epoch 1909/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2328e-05 - val_loss: 2.1908e-05\n",
      "Epoch 1910/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2353e-05 - val_loss: 2.1923e-05\n",
      "Epoch 1911/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2366e-05 - val_loss: 2.1924e-05\n",
      "Epoch 1912/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2373e-05 - val_loss: 2.1936e-05\n",
      "Epoch 1913/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2323e-05 - val_loss: 2.1904e-05\n",
      "Epoch 1914/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2329e-05 - val_loss: 2.1906e-05\n",
      "Epoch 1915/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2330e-05 - val_loss: 2.1933e-05\n",
      "Epoch 1916/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2321e-05 - val_loss: 2.2465e-05\n",
      "Epoch 1917/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2465e-05 - val_loss: 2.1984e-05\n",
      "Epoch 1918/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2345e-05 - val_loss: 2.1901e-05\n",
      "Epoch 1919/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2326e-05 - val_loss: 2.1937e-05\n",
      "Epoch 1920/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2349e-05 - val_loss: 2.1909e-05\n",
      "Epoch 1921/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2320e-05 - val_loss: 2.2447e-05\n",
      "Epoch 1922/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2445e-05 - val_loss: 2.1929e-05\n",
      "Epoch 1923/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2297e-05 - val_loss: 2.1942e-05\n",
      "Epoch 1924/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2327e-05 - val_loss: 2.1930e-05\n",
      "Epoch 1925/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2303e-05 - val_loss: 2.1912e-05\n",
      "Epoch 1926/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3.2310e-05 - val_loss: 2.1900e-05\n",
      "Epoch 1927/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2344e-05 - val_loss: 2.1896e-05\n",
      "Epoch 1928/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2351e-05 - val_loss: 2.1895e-05\n",
      "Epoch 1929/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2324e-05 - val_loss: 2.1997e-05\n",
      "Epoch 1930/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2333e-05 - val_loss: 2.1909e-05\n",
      "Epoch 1931/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2309e-05 - val_loss: 2.1935e-05\n",
      "Epoch 1932/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2324e-05 - val_loss: 2.1961e-05\n",
      "Epoch 1933/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2308e-05 - val_loss: 2.1893e-05\n",
      "Epoch 1934/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2323e-05 - val_loss: 2.1894e-05\n",
      "Epoch 1935/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3.2316e-05 - val_loss: 2.1909e-05\n",
      "Epoch 1936/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3.2302e-05 - val_loss: 2.1955e-05\n",
      "Epoch 1937/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2313e-05 - val_loss: 2.1904e-05\n",
      "Epoch 1938/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2327e-05 - val_loss: 2.1915e-05\n",
      "Epoch 1939/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2327e-05 - val_loss: 2.1945e-05\n",
      "Epoch 1940/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2329e-05 - val_loss: 2.2232e-05\n",
      "Epoch 1941/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2404e-05 - val_loss: 2.1929e-05\n",
      "Epoch 1942/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3.2311e-05 - val_loss: 2.1969e-05\n",
      "Epoch 1943/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2318e-05 - val_loss: 2.1981e-05\n",
      "Epoch 1944/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2310e-05 - val_loss: 2.1894e-05\n",
      "Epoch 1945/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2315e-05 - val_loss: 2.1996e-05\n",
      "Epoch 1946/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2322e-05 - val_loss: 2.1893e-05\n",
      "Epoch 1947/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2312e-05 - val_loss: 2.1891e-05\n",
      "Epoch 1948/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2314e-05 - val_loss: 2.2042e-05\n",
      "Epoch 1949/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2337e-05 - val_loss: 2.2024e-05\n",
      "Epoch 1950/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2310e-05 - val_loss: 2.1894e-05\n",
      "Epoch 1951/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2337e-05 - val_loss: 2.1909e-05\n",
      "Epoch 1952/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2301e-05 - val_loss: 2.2452e-05\n",
      "Epoch 1953/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2450e-05 - val_loss: 2.1895e-05\n",
      "Epoch 1954/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2298e-05 - val_loss: 2.1905e-05\n",
      "Epoch 1955/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2312e-05 - val_loss: 2.1898e-05\n",
      "Epoch 1956/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2326e-05 - val_loss: 2.1921e-05\n",
      "Epoch 1957/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2320e-05 - val_loss: 2.1894e-05\n",
      "Epoch 1958/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3.2300e-05 - val_loss: 2.1914e-05\n",
      "Epoch 1959/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3.2300e-05 - val_loss: 2.1934e-05\n",
      "Epoch 1960/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2324e-05 - val_loss: 2.2072e-05\n",
      "Epoch 1961/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2335e-05 - val_loss: 2.1901e-05\n",
      "Epoch 1962/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2299e-05 - val_loss: 2.1892e-05\n",
      "Epoch 1963/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2320e-05 - val_loss: 2.1890e-05\n",
      "Epoch 1964/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2302e-05 - val_loss: 2.1909e-05\n",
      "Epoch 1965/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2314e-05 - val_loss: 2.2036e-05\n",
      "Epoch 1966/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2323e-05 - val_loss: 2.1893e-05\n",
      "Epoch 1967/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2302e-05 - val_loss: 2.1888e-05\n",
      "Epoch 1968/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3.2332e-05 - val_loss: 2.1887e-05\n",
      "Epoch 1969/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2294e-05 - val_loss: 2.1905e-05\n",
      "Epoch 1970/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3.2272e-05 - val_loss: 2.2027e-05\n",
      "Epoch 1971/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3.2311e-05 - val_loss: 2.1896e-05\n",
      "Epoch 1972/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3.2317e-05 - val_loss: 2.1887e-05\n",
      "Epoch 1973/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3.2313e-05 - val_loss: 2.1888e-05\n",
      "Epoch 1974/10000\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 3.2314e-05 - val_loss: 2.1896e-05\n",
      "Epoch 1975/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3.2357e-05 - val_loss: 2.1936e-05\n",
      "Epoch 1976/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3.2368e-05 - val_loss: 2.1887e-05\n",
      "Epoch 1977/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2293e-05 - val_loss: 2.1911e-05\n",
      "Epoch 1978/10000\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 3.2288e-05 - val_loss: 2.1920e-05\n",
      "Epoch 1979/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3.2339e-05 - val_loss: 2.1959e-05\n",
      "Epoch 1980/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3.2336e-05 - val_loss: 2.2071e-05\n",
      "Epoch 1981/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3.2332e-05 - val_loss: 2.2117e-05\n",
      "Epoch 1982/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3.2353e-05 - val_loss: 2.2001e-05\n",
      "Epoch 1983/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3.2324e-05 - val_loss: 2.1887e-05\n",
      "Epoch 1984/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3.2346e-05 - val_loss: 2.1888e-05\n",
      "Epoch 1985/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2376e-05 - val_loss: 2.1887e-05\n",
      "Epoch 1986/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2308e-05 - val_loss: 2.1931e-05\n",
      "Epoch 1987/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3.2305e-05 - val_loss: 2.1899e-05\n",
      "Epoch 1988/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3.2309e-05 - val_loss: 2.1992e-05\n",
      "Epoch 1989/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2303e-05 - val_loss: 2.1918e-05\n",
      "Epoch 1990/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3.2296e-05 - val_loss: 2.1887e-05\n",
      "Epoch 1991/10000\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3.2303e-05 - val_loss: 2.1892e-05\n",
      "Epoch 1992/10000\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3.2321e-05 - val_loss: 2.1892e-05\n",
      "Epoch 1993/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2327e-05 - val_loss: 2.1936e-05\n",
      "Epoch 1994/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2318e-05 - val_loss: 2.1936e-05\n",
      "Epoch 1995/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2325e-05 - val_loss: 2.1919e-05\n",
      "Epoch 1996/10000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3.2301e-05 - val_loss: 2.2059e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "oPredictiveModel = Long_Short_Term_Memory(\n",
    "    sSubModelName, \n",
    "    dfScaledInputTrain.shape[1], \n",
    "    1,\n",
    "    len(g_aBackwardTimeSteps), \n",
    "    1,\n",
    "    False\n",
    "    )\n",
    "\n",
    "oPredictiveModel.Train(dfScaledInputTrain, \n",
    "                       dfOutputTrain,\n",
    "                       dfScaledInputValidation,\n",
    "                       dfOutputValidation\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70079690",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "746311f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "aPrediction = oPredictiveModel.aPredict(dfScaledInputTest)\n",
    "dfPrediction = pd.DataFrame(aPrediction, index = dfScaledInputTest.index)\n",
    "dfPrediction.columns = aColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "aa48de7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for regression:\n",
      "      mean absolute error:                0.005\n",
      "    median absolute error:                0.004\n",
      "       mean squared error:                0.000\n",
      "                max error:                0.052\n",
      "                 r2 score:               -0.000\n",
      " explained variance score:                0.000\n"
     ]
    }
   ],
   "source": [
    "error = dfOutputTest - dfPrediction\n",
    "\n",
    "aMetrics = [\n",
    "        ('mean absolute error', mean_absolute_error(dfOutputTest, dfPrediction)),\n",
    "        ('median absolute error', median_absolute_error(dfOutputTest, dfPrediction)),\n",
    "        ('mean squared error', mean_squared_error(dfOutputTest, dfPrediction)),\n",
    "        ('max error', max_error(dfOutputTest, dfPrediction)),\n",
    "        ('r2 score', r2_score(dfOutputTest, dfPrediction)),\n",
    "        ('explained variance score', explained_variance_score(dfOutputTest, dfPrediction))\n",
    "    ]\n",
    "\n",
    "print('Metrics for regression:')\n",
    "for metric_name, metric_value in aMetrics:\n",
    "    print(f'{metric_name:>25s}: {metric_value: >20.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1127d583",
   "metadata": {},
   "source": [
    "# DOWNWARD REGRESSION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137167f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
