{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dbcf9b4-483a-40ea-8c03-bed78cda47c3",
   "metadata": {},
   "source": [
    "* https://towardsdatascience.com/transformers-141e32e69591\n",
    "* https://towardsdatascience.com/stock-predictions-with-state-of-the-art-transformer-and-time-embeddings-3a4485237de6\n",
    "* https://towardsdatascience.com/lstm-networks-a-detailed-explanation-8fae6aefc7f9#:~:text=LSTMs%20use%20a%20series%20of,each%20their%20own%20neural%20network.\n",
    "* https://stats.stackexchange.com/questions/365778/what-should-i-do-when-my-neural-network-doesnt-generalize-well\n",
    "* https://www.youtube.com/watch?v=ROLugVqjf00\n",
    "* https://lilianweng.github.io/posts/2019-03-14-overfit/    (very important)\n",
    "* https://atcold.github.io/pytorch-Deep-Learning/en/week11/11-3/\n",
    "* https://towardsdatascience.com/choosing-and-customizing-loss-functions-for-image-processing-a0e4bf665b0a\n",
    "* https://towardsdatascience.com/learning-to-rank-a-complete-guide-to-ranking-using-machine-learning-4c9688d370d4\n",
    "* https://embracingtherandom.com/machine-learning/tensorflow/ranking/deep-learning/learning-to-rank-part-2/#where-do-probabilities-fit-into-listnet\n",
    "* https://www.topbots.com/attention-for-time-series-forecasting-and-classification/\n",
    "* https://medium.com/@judopro/using-machine-learning-to-programmatically-determine-stock-support-and-resistance-levels-9bb70777cf8e  K-Means Algorithm used to identify the support and resistence levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90dc90ea-65c6-4440-8410-11d2f6afb4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NR_OF_BINS = 100\n",
    "BACKWARD_WINDOW_LENGTH = 3\n",
    "FORWARD_WINDOW_LENGTH = 1\n",
    "INTERPLOATION_POINTS = 1200\n",
    "EXCHANGE_RATES = ['EURUSD']\n",
    "\n",
    "FROM_TIMESTAMP = '2022-08-01 00:00'\n",
    "TO_TIMESTAMP = '2022-11-01 00:00'\n",
    "FFT_AC_COEFFICIENT  = 15 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3092ce-8ffd-42ce-bf10-41cf93cb035e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b11b8bc3-2e60-4f49-9359-a751c5d2e45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import MetaTrader5 as mt5\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy import interpolate\n",
    "from numpy.fft import fft,ifft\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler, KBinsDiscretizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593979c9-3fa4-4747-a198-d6cb2befa91e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# SOURCE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69889ffe-e4c5-470b-9417-328ef9998d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish connection to the MetaTrader 5 terminal\n",
    "if not mt5.initialize():\n",
    "    print(\"initialize() failed, error code =\",mt5.last_error())\n",
    "    quit()\n",
    "    \n",
    "tplSymbols = mt5.symbols_get()\n",
    "dfSymbols = pd.DataFrame(tplSymbols, columns = tplSymbols[0]._asdict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e6ee16d-0e2b-49e7-b75a-dccfa0889fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get OHLC data\n",
    "dfOhlc = pd.DataFrame()\n",
    "for sExchangeRate in EXCHANGE_RATES:\n",
    "    \n",
    "    df = pd.read_csv(r'Data\\{}_M1_202010010001_202210312359.csv'.format(sExchangeRate), delimiter = '\\t')\n",
    "    df.loc[:, 'PRICE_TIME_STAMP'] = pd.to_datetime(df['<DATE>'] + df['<TIME>'], format='%Y.%m.%d%H:%M:%S')\n",
    "    df.drop(['<DATE>', '<TIME>'], axis = 1, inplace = True)\n",
    "    df.loc[:, 'EXCHANGE_RATE'] = sExchangeRate\n",
    "    df.query('@FROM_TIMESTAMP<= PRICE_TIME_STAMP <= @TO_TIMESTAMP ', inplace = True)\n",
    "    dfOhlc = pd.concat([dfOhlc, df])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bcb69c-c19c-453f-995c-c5a067ec6d51",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8c8cee3-3c36-498f-80fa-dd2dc40f2e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GET_DATASETS_1(dfPreprocessed, dfTimes):\n",
    "\n",
    "    #Create PySpark SparkSession\n",
    "    oSparkSess = SparkSession.builder \\\n",
    "        .master(\"local[1]\") \\\n",
    "        .appName(\"SparkByExamples.com\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    oSparkSess.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "    def dfGetPriceAnalysis(df, iFrom, iTo, dfTimes):\n",
    "\n",
    "        df['from_time'] = df['PRICE_TIME_STAMP'] + pd.DateOffset(hours=iFrom)\n",
    "        df['to_time'] = df['PRICE_TIME_STAMP'] + pd.DateOffset(hours=iTo)\n",
    "\n",
    "        df = df[['to_time', 'from_time', 'PRICE_TIME_STAMP','<CLOSE>', '<HIGH>']]\n",
    "\n",
    "        sdf= oSparkSess.createDataFrame(df)\n",
    "        sdf.createOrReplaceTempView(\"sdf\")\n",
    "\n",
    "        dfTimes['FROM_TIME_STAMP'] = dfTimes['TIME_STAMP'] + pd.DateOffset(hours=iFrom)\n",
    "        dfTimes['TO_TIME_STAMP'] = dfTimes['TIME_STAMP'] + pd.DateOffset(hours=iTo)\n",
    "\n",
    "        sdfTimes= oSparkSess.createDataFrame(dfTimes)\n",
    "        sdfTimes.createOrReplaceTempView(\"sdfTimes\")\n",
    "\n",
    "        dfToReturn =  oSparkSess.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM\n",
    "            (\n",
    "                SELECT \n",
    "                t.*,\n",
    "                df.`<CLOSE>` AS CURRENT_CLOSE,\n",
    "                df2.`<HIGH>` AS HISTORY_HIGH,\n",
    "                (df2.`<HIGH>`-df.`<CLOSE>`)/(df.`<CLOSE>`) AS DIFF\n",
    "                FROM\n",
    "                (\n",
    "                    SELECT df1.*, df2.TIME_STAMP AS HISTORY_TIME_STAMP FROM sdfTimes df1\n",
    "                    INNER JOIN sdfTimes df2\n",
    "                    ON df2.TIME_STAMP >= df1.FROM_TIME_STAMP and df2.TIME_STAMP < df1.TO_TIME_STAMP\n",
    "                ) t\n",
    "                LEFT JOIN sdf df\n",
    "                ON t.TIME_STAMP = df.PRICE_TIME_STAMP\n",
    "                LEFT JOIN sdf df2\n",
    "                ON t.HISTORY_TIME_STAMP = df2.PRICE_TIME_STAMP\n",
    "            ) df\n",
    "            ORDER BY df.TIME_STAMP, df.HISTORY_TIME_STAMP\n",
    "        \"\"\").toPandas()\n",
    "        # find the unique timestamps where there is no historical or current data. and drop them from the dataset.\n",
    "        aTimeStampsToDrop = dfToReturn.query('CURRENT_CLOSE.isna() == True or HISTORY_HIGH.isna() == True')['TIME_STAMP'].unique()\n",
    "        dfToReturn.query('TIME_STAMP not in @aTimeStampsToDrop', inplace = True)\n",
    "        dfToReturn.reset_index(drop = True, inplace = True)\n",
    "\n",
    "        # drop the time stamps where there is no 60-mins data avaiable\n",
    "        aTimeStampsToDrop = dfToReturn.groupby(['TIME_STAMP']).count().reset_index().query('HISTORY_TIME_STAMP < 60')['TIME_STAMP'].unique()\n",
    "        dfToReturn.query('TIME_STAMP not in @aTimeStampsToDrop', inplace = True)\n",
    "        dfToReturn.reset_index(drop = True, inplace = True)    \n",
    "\n",
    "        dfToReturn.loc[:, 'MINUTE_DIFF'] = ((dfToReturn.loc[:, 'HISTORY_TIME_STAMP']-dfToReturn.loc[:, 'FROM_TIME_STAMP']).dt.seconds/60).astype(int)\n",
    "        return dfToReturn\n",
    "\n",
    "\n",
    "\n",
    "    dfPrep = dfPreprocessed.copy()\n",
    "\n",
    "    aExchangeRates = list(dfPrep['EXCHANGE_RATE'].unique())\n",
    "\n",
    "    dicDatasets = {\n",
    "        'INPUT':\n",
    "        {\n",
    "        },\n",
    "        'OUTPUT':\n",
    "        {   \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    \n",
    "    for sExchangeRate in aExchangeRates:\n",
    "        df_single_exc = dfPrep.query('EXCHANGE_RATE == @sExchangeRate')\n",
    "        df_single_exc = df_single_exc[['PRICE_TIME_STAMP','<CLOSE>', '<HIGH>']].fillna(0)\n",
    "\n",
    "        dic_input_single_exc = {}\n",
    "        for i in range(-BACKWARD_WINDOW_LENGTH, 0):\n",
    "            iFrom = i\n",
    "            iTo = i +1\n",
    "\n",
    "            print(iFrom)\n",
    "            dfPriceAnalysis = dfGetPriceAnalysis(df_single_exc, iFrom, iTo, dfTimes)\n",
    "\n",
    "            dic_input_single_exc[iFrom] = dfPriceAnalysis\n",
    "\n",
    "\n",
    "\n",
    "        dic_output_single_exc = {}\n",
    "        for i in range(0, FORWARD_WINDOW_LENGTH):\n",
    "            iFrom = i\n",
    "            iTo = i +1\n",
    "\n",
    "            print(iFrom)\n",
    "            dfPriceAnalysis = dfGetPriceAnalysis(df_single_exc, iFrom, iTo, dfTimes)\n",
    "\n",
    "            dic_output_single_exc[iFrom] = dfPriceAnalysis\n",
    "\n",
    "\n",
    "        dicDatasets['INPUT'][sExchangeRate] = dic_input_single_exc\n",
    "        dicDatasets['OUTPUT'][sExchangeRate] = dic_output_single_exc\n",
    "\n",
    "    \n",
    "    \n",
    "    # identify common time stamps\n",
    "    aCommonTimeStamps = list()\n",
    "    for i in dicDatasets:\n",
    "        for j in dicDatasets[i]:\n",
    "            for k in dicDatasets[i][j]:\n",
    "                df = dicDatasets[i][j][k]\n",
    "\n",
    "                if len(aCommonTimeStamps) == 0:\n",
    "                    aCommonTimeStamps = df['TIME_STAMP'].unique()\n",
    "                else:\n",
    "                    aCommonTimeStamps = np.intersect1d(aCommonTimeStamps, df['TIME_STAMP'].unique())\n",
    "\n",
    "\n",
    "    # drop the datafrom datasets that don't have common time stamps\n",
    "    for i in dicDatasets:\n",
    "        for j in dicDatasets[i]:\n",
    "            for k in dicDatasets[i][j]:\n",
    "                df = dicDatasets[i][j][k]\n",
    "                dicDatasets[i][j][k] = df.query('TIME_STAMP in @aCommonTimeStamps').reset_index(drop = True)\n",
    "\n",
    "\n",
    "\n",
    "    def dfCompileDic(p_dic, tplFormat,sKey):\n",
    "        aToReturn =  np.zeros(tplFormat)\n",
    "        dic = p_dic[sKey]\n",
    "        ixExcRate = 0\n",
    "        for i in dic:\n",
    "            ixTimeStep = 0\n",
    "            for j in dic[i]:\n",
    "                df = dic[i][j]\n",
    "                df = pd.pivot_table(\n",
    "                    data = df[['DIFF','MINUTE_DIFF', 'TIME_STAMP']], \n",
    "                    values='DIFF', index='TIME_STAMP',\n",
    "                    columns='MINUTE_DIFF', \n",
    "                    aggfunc=np.sum\n",
    "                )\n",
    "\n",
    "                aToReturn[:, ixTimeStep,:,ixExcRate] = df.values\n",
    "\n",
    "                ixTimeStep = ixTimeStep + 1\n",
    "\n",
    "            ixExcRate = ixExcRate  + 1\n",
    "\n",
    "\n",
    "        return aToReturn\n",
    "\n",
    "\n",
    "\n",
    "    X = dfCompileDic(dicDatasets, (len(aCommonTimeStamps), BACKWARD_WINDOW_LENGTH , 60 , len(aExchangeRates)), 'INPUT') # sample_size, time_steps, features, channel_size\n",
    "    Y = dfCompileDic(dicDatasets, (len(aCommonTimeStamps), FORWARD_WINDOW_LENGTH , 60 , len(aExchangeRates)), 'OUTPUT') # sample_size, time_steps, features, channel_size\n",
    "    \n",
    "    \n",
    "    return X,Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5be55d0-9c57-45b4-816e-3e6b3afdd292",
   "metadata": {
    "tags": []
   },
   "source": [
    "# VASWANI'S ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60be1d81-fa3b-4748-bc52-d3f9801f9e1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3\n"
     ]
    }
   ],
   "source": [
    "dfPrep = dfOhlc.copy()\n",
    "dfTimeStamps = pd.DataFrame(\n",
    "    data = pd.date_range(\n",
    "        start=FROM_TIMESTAMP, \n",
    "        end=TO_TIMESTAMP, freq = 'min'\n",
    "    ),\n",
    "    columns  = ['TIME_STAMP']\n",
    ")\n",
    "dfTimeStamps.query('TIME_STAMP.dt.day_of_week not in (6,7)', inplace = True)\n",
    "dfTimeStamps.reset_index(drop = True, inplace = True)\n",
    "\n",
    "X_ORIGINAL, Y_ORIGINAL = GET_DATASETS_1(dfPrep, dfTimeStamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac3b301-6b63-4554-a293-bbcb302f9d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce dimension of input dataset with fast fourier transform\n",
    "X = np.zeros((X_ORIGINAL.shape[0], X_ORIGINAL.shape[1], FFT_AC_COEFFICIENT, X_ORIGINAL.shape[3]))\n",
    "for i in range(X_ORIGINAL.shape[1]): #future time step\n",
    "    for j in range(X_ORIGINAL.shape[3]): #exchange rate\n",
    "        aTransformed = fft(X_ORIGINAL[:, i,:,j])\n",
    "        aTransformed = aTransformed[:, :FFT_AC_COEFFICIENT]\n",
    "        aInverseTransformed = ifft(aTransformed)\n",
    "        X[:, i,:,j] = aInverseTransformed\n",
    "        \n",
    "        \n",
    "Y= Y_ORIGINAL.copy()\n",
    "# handle outliers below 0.10th and above .90th quantiles\n",
    "fLowerBound = np.quantile(Y.reshape(-1,1), 0.10)\n",
    "fUpperBound =np.quantile(Y.reshape(-1,1), 0.90)\n",
    "\n",
    "Y[Y<fLowerBound] = fLowerBound\n",
    "Y[Y>fUpperBound] = fUpperBound\n",
    "\n",
    "oOutputScaler = MinMaxScaler()\n",
    "Y = oOutputScaler.fit_transform(Y.reshape(-1,1)).reshape(Y.shape)\n",
    "        \n",
    "Y_copy = Y.copy()\n",
    "# interplating Y dataset (as data augmentation ?)\n",
    "Y = np.zeros((Y_copy.shape[0], Y_copy.shape[1], INTERPLOATION_POINTS, Y_copy.shape[3]))\n",
    "for i in range(Y_copy.shape[1]): #future time step\n",
    "    \n",
    "    for j in range(Y_copy.shape[3]): #exchange rate\n",
    "        aX = list(range(0,60))\n",
    "        aY = Y_copy[:, i,:,j]\n",
    "\n",
    "        oInterpolate = interpolate.interp1d(aX, aY, kind = 'linear')\n",
    "\n",
    "        aX_new = np.linspace(0, 59, INTERPLOATION_POINTS)\n",
    "        aY_new = oInterpolate(aX_new)\n",
    "        \n",
    "        Y[:, i,:,j] = aY_new\n",
    "        \n",
    "\n",
    "Y = (Y /np.expand_dims(np.sum(Y, axis = 2), 2))        \n",
    "        \n",
    "# # # discritize output to histogram bin size\n",
    "# aHistogramBins = np.linspace(fLowerBound,fUpperBound, num = INTERPLOATION_POINTS).reshape(-1 ,1)\n",
    "# oOutputDiscritizer = KBinsDiscretizer(n_bins = NR_OF_BINS, encode = 'ordinal', strategy = 'uniform')\n",
    "# aBins = oOutputDiscritizer.fit_transform(aHistogramBins).astype(int)\n",
    "# Y = oOutputDiscritizer.transform(Y.reshape(-1,1)).reshape(Y.shape).astype(int)\n",
    "\n",
    "# Y = Y/NR_OF_BINS\n",
    "\n",
    "\n",
    "\n",
    "# Y_copy = Y.copy()\n",
    "# Y = np.zeros((Y_copy.shape[0], Y_copy.shape[1], NR_OF_BINS, Y_copy.shape[3]))\n",
    "# for i in range(Y_copy.shape[1]): #future time step\n",
    "#     for j in range(Y_copy.shape[3]): #exchange rate\n",
    "#         aOriginal = Y_copy[:, i,:,j]\n",
    "#         df = pd.DataFrame(aOriginal).melt(ignore_index = False)\n",
    "#         df.reset_index(inplace = True)\n",
    "#         df = pd.pivot_table(df , columns = 'value',a index = 'index', values = 'variable', aggfunc='count', fill_value=0)\n",
    "#         Y[:, i,:,j] = df.values\n",
    "        \n",
    "# # convert Y values to softmax format.\n",
    "# Y = Y/INTERPLOATION_POINTS\n",
    "\n",
    "\n",
    "# split data to train-validation-test\n",
    "c_fTrainingRatio = 0.70\n",
    "c_fValidationRatio = 0.28\n",
    "c_fTestRatio = 0.02\n",
    "\n",
    "X_train,X_test, Y_train, Y_test = train_test_split(\n",
    "    X,Y,\n",
    "    test_size=1-c_fTrainingRatio,\n",
    "    shuffle=False,\n",
    "    random_state = 1\n",
    ")\n",
    "\n",
    "X_validation, X_test,Y_validation, Y_test = train_test_split(\n",
    "    X_test,Y_test,\n",
    "    test_size=c_fTestRatio/(c_fTestRatio + c_fValidationRatio),\n",
    "    shuffle=False,\n",
    "    random_state = 1\n",
    ")\n",
    "\n",
    "X_train = X_train[:,:,:,0]\n",
    "X_validation = X_validation[:,:,:,0]\n",
    "X_test = X_test[:,:,:,0]\n",
    "\n",
    "Y_train = Y_train[:,0,:,0]\n",
    "Y_validation = Y_validation[:,0,:,0]\n",
    "Y_test = Y_test[:,0,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71d7c82-6b90-47ff-8fe4-489b1c8875f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "\n",
    "class Time2Vec(Layer):\n",
    "    def __init__(self, kernel_size=1):\n",
    "        super(Time2Vec, self).__init__(trainable=True, name='Time2VecLayer')\n",
    "        self.k = kernel_size\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # trend\n",
    "        self.wb = self.add_weight(name='wb',shape=(input_shape[1],),initializer='uniform',trainable=True)\n",
    "        self.bb = self.add_weight(name='bb',shape=(input_shape[1],),initializer='uniform',trainable=True)\n",
    "        # periodic\n",
    "        self.wa = self.add_weight(name='wa',shape=(1, input_shape[1], self.k),initializer='uniform',trainable=True)\n",
    "        self.ba = self.add_weight(name='ba',shape=(1, input_shape[1], self.k),initializer='uniform',trainable=True)\n",
    "        super(Time2Vec, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs, **kwargs):\n",
    "        bias = self.wb * inputs + self.bb\n",
    "        dp = K.dot(inputs, self.wa) + self.ba\n",
    "        wgts = K.sin(dp) # or K.cos(.)\n",
    "\n",
    "        ret = K.concatenate([K.expand_dims(bias, -1), wgts], -1)\n",
    "        ret = K.reshape(ret, (-1, inputs.shape[1]*(self.k+1)))\n",
    "        return ret\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1]*(self.k + 1))\n",
    "\n",
    "\n",
    "def transformer_encoder(inputs):\n",
    "    # Normalization and Attention\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = tf.keras.layers.MultiHeadAttention(key_dim=512, num_heads=4, dropout=0.1)(x, x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = tf.keras.layers.Conv1D(filters=1, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return res\n",
    "\n",
    "\n",
    "TermInput = tf.keras.Input(\n",
    "    shape=(BACKWARD_WINDOW_LENGTH, FFT_AC_COEFFICIENT))\n",
    "\n",
    "# TimeInput = tf.keras.Input(\n",
    "#     shape=(BACKWARD_WINDOW_LENGTH, X_TIME_train.shape[2]))\n",
    "\n",
    "# W = Time2Vec(1)(TimeInput)\n",
    "# W = tf.keras.layers.concatenate([TermInput, W], -1)\n",
    "\n",
    "W = TermInput\n",
    "# W = tf.keras.layers.Flatten()(W)\n",
    " \n",
    "# for _ in range(1):\n",
    "#     W = transformer_encoder(W)\n",
    "    \n",
    "# W = tf.keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(W)\n",
    "for _ in range(3):\n",
    "    W = tf.keras.layers.Dense(128)(W)\n",
    "    W = tf.keras.layers.ReLU()(W)\n",
    "    W = tf.keras.layers.Dropout(0.5)(W)\n",
    "\n",
    "W = tf.keras.layers.Flatten()(W)\n",
    "W = tf.keras.layers.Dense(INTERPLOATION_POINTS, activation = 'softmax')(W)\n",
    "# W = tf.keras.layers.Softmax(axis = 2)(W)\n",
    "# W = tf.keras.layers.Reshape((Y_train.shape[2]))(W)\n",
    "\n",
    "\n",
    "ModelOutput = W\n",
    "oModel = tf.keras.Model(TermInput, ModelOutput, name = 'TRANSFORMER_MODEL')\n",
    "\n",
    "oOptimizer = tf.keras.optimizers.Adam(learning_rate=1e-04)\n",
    "oModel.compile(\n",
    "    loss = tf.keras.losses.KLDivergence(), \n",
    "    optimizer=oOptimizer\n",
    ")\n",
    "\n",
    "# # print(oModel.summary())\n",
    "\n",
    "tf.keras.utils.plot_model(oModel, show_shapes=True)\n",
    "\n",
    "\n",
    "# fit model\n",
    "oEarlyStop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss', \n",
    "    mode = 'min', \n",
    "    verbose = 0 , \n",
    "    patience = 50, \n",
    "    restore_best_weights = True,\n",
    "    start_from_epoch= 8000\n",
    ")\n",
    "\n",
    "oModel.fit(\n",
    "    X_train, \n",
    "    Y_train, \n",
    "    epochs= 10**2, \n",
    "    batch_size=2**7, \n",
    "    verbose=1, \n",
    "    validation_data= (X_validation, Y_validation),\n",
    "    # callbacks=[oEarlyStop]\n",
    ")\n",
    "\n",
    "# show epoch history\n",
    "dfHistory = pd.DataFrame(oModel.history.history)\n",
    "\n",
    "plt.figure(figsize = (20, 8))\n",
    "sns.lineplot(data = dfHistory['loss'].iloc[1:], legend = True, label = 'Train')\n",
    "sns.lineplot(data = dfHistory['val_loss'].iloc[1:], legend = True, label = 'Validation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a72d84-3a17-43ec-8f55-39d1dc8b8e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "a_Y_datasets = [Y_train, Y_validation]\n",
    "a_X_datasets = [X_train, X_validation]\n",
    "a_labels = ['train', 'validation'] \n",
    "for X_to_visualzie,Y_to_visualize, sLabel in list(zip(a_X_datasets, a_Y_datasets, a_labels)):\n",
    "    pred = oModel.predict(X_to_visualzie)\n",
    "    ground_true = Y_to_visualize\n",
    "    for i in np.random.randint(low = 0,high = len(X_to_visualzie), size = 10):      \n",
    "        \n",
    "        fig, axs = plt.subplots(2,2, figsize=(20, 4))\n",
    "        fig.suptitle('{}--{}'.format(sLabel, i))\n",
    "        \n",
    "        axs[0, 0].imshow(X_to_visualzie[i], cmap='gray')\n",
    "        axs[0, 0].set_title( 'X')\n",
    "\n",
    "        axs[0, 1].imshow(X_to_visualzie[i], cmap='gray')\n",
    "        axs[0, 1].set_title( 'X')\n",
    "\n",
    "    \n",
    "        axs[1, 0].hist(ground_true[i])\n",
    "        axs[1, 0].set_title( 'GROUND TRUE')\n",
    "\n",
    "        axs[1, 1].hist(pred[i])\n",
    "        axs[1, 1].set_title( 'PREDICTION')\n",
    "\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
