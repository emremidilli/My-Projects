{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dbcf9b4-483a-40ea-8c03-bed78cda47c3",
   "metadata": {},
   "source": [
    "* https://towardsdatascience.com/transformers-141e32e69591\n",
    "* https://towardsdatascience.com/stock-predictions-with-state-of-the-art-transformer-and-time-embeddings-3a4485237de6\n",
    "* https://towardsdatascience.com/lstm-networks-a-detailed-explanation-8fae6aefc7f9#:~:text=LSTMs%20use%20a%20series%20of,each%20their%20own%20neural%20network.\n",
    "* https://stats.stackexchange.com/questions/365778/what-should-i-do-when-my-neural-network-doesnt-generalize-well\n",
    "* https://www.youtube.com/watch?v=ROLugVqjf00\n",
    "* https://lilianweng.github.io/posts/2019-03-14-overfit/    (very important)\n",
    "* https://atcold.github.io/pytorch-Deep-Learning/en/week11/11-3/\n",
    "* https://towardsdatascience.com/choosing-and-customizing-loss-functions-for-image-processing-a0e4bf665b0a\n",
    "* https://towardsdatascience.com/learning-to-rank-a-complete-guide-to-ranking-using-machine-learning-4c9688d370d4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90dc90ea-65c6-4440-8410-11d2f6afb4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NR_OF_PIXELS= 20\n",
    "BACKWARD_WINDOW_LENGTH = 19\n",
    "FORWARD_WINDOW_LENGTH = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3092ce-8ffd-42ce-bf10-41cf93cb035e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b11b8bc3-2e60-4f49-9359-a751c5d2e45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import MetaTrader5 as mt5\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import pytz\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import entropy\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_ranking as tfr\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e040057-d8d5-42e5-b0f5-91046b417cf0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e4ff210-342f-4879-82a6-3e6ee032b63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', '{:.5f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e03a0df7-f9f2-46a6-b110-2570b4fe1406",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_o_TIME_ZONE = pytz.timezone(\"Etc/UTC\")\n",
    "gc_dt_FROM = datetime(2022, 8, 1, tzinfo=gc_o_TIME_ZONE)\n",
    "gc_dt_TO = datetime(2022, 11, 1, tzinfo=gc_o_TIME_ZONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11878356-d9cb-4d52-a4d3-93ff8637487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sCategory = 'Stocks\\\\Germany\\\\Banking & Investment Services\\\\Banks\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593979c9-3fa4-4747-a198-d6cb2befa91e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# SOURCE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69889ffe-e4c5-470b-9417-328ef9998d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfFetchSampleDataFromMt(p_sSymbolName):\n",
    "    aOhlSample = mt5.copy_rates_range(\n",
    "        p_sSymbolName,\n",
    "        mt5.TIMEFRAME_M1,\n",
    "        gc_dt_FROM, \n",
    "        gc_dt_TO\n",
    "    )\n",
    "\n",
    "    dfToReturn = pd.DataFrame(aOhlSample)\n",
    "    if 'time' in list(dfToReturn.columns):\n",
    "        return dfToReturn\n",
    "    else:\n",
    "        return dfFetchSampleDataFromMt(p_sSymbolName)\n",
    "    \n",
    "    \n",
    "\n",
    "# establish connection to the MetaTrader 5 terminal\n",
    "if not mt5.initialize():\n",
    "    print(\"initialize() failed, error code =\",mt5.last_error())\n",
    "    quit()\n",
    "    \n",
    "tplSymbols = mt5.symbols_get()\n",
    "dfSymbols = pd.DataFrame(tplSymbols, columns = tplSymbols[0]._asdict().keys())\n",
    "\n",
    "\n",
    "def aGetUniqueCategories(tplSymbols):\n",
    "    aCategories = []\n",
    "    for i in range(0, len(tplSymbols)):\n",
    "        sCategory = '\\\\'.join(tplSymbols[i]._asdict()['path'].split('\\\\')[:-1]) +'\\\\'\n",
    "        if sCategory not in aCategories:\n",
    "            aCategories.append(sCategory)\n",
    "    return aCategories\n",
    "\n",
    "\n",
    "aCategories = aGetUniqueCategories(tplSymbols)\n",
    "\n",
    "\n",
    "dfFilteredSymbols = dfSymbols[dfSymbols['path'].str.contains(sCategory, regex=False) == True]\n",
    "\n",
    "dfOhlc = pd.DataFrame()\n",
    "\n",
    "for iIndex, srsRow in dfFilteredSymbols.iterrows():\n",
    "    sSymbolName = dfFilteredSymbols.loc[iIndex, 'name']\n",
    "    iDigit  = dfFilteredSymbols.loc[iIndex, 'digits']\n",
    "    \n",
    "    dfOhlcSample =  dfFetchSampleDataFromMt(sSymbolName)\n",
    "    \n",
    "    if len(dfOhlcSample) > 500:\n",
    "        dfOhlcSample['EXCHANGE_RATE'] =  sSymbolName\n",
    "        dfOhlcSample['DIGIT_SENSITIVITY'] =  iDigit\n",
    "        dfOhlc = dfOhlc.append(dfOhlcSample)\n",
    "        \n",
    "dfOhlc.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e6ee16d-0e2b-49e7-b75a-dccfa0889fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfOhlc['time'] = pd.to_datetime(dfOhlc['time'], unit = 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a179b0d-e256-49c5-b1f0-a69d0a969dc7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a5a98da-73a0-4a62-9b32-05185da38660",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPrep = dfOhlc.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450de93f-e6f5-4b78-944a-8b7354e0070a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Add Last Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "570d74e6-f7d1-47fa-97a7-2ec05ba498ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To-be-continued"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea44ade-6230-4f9d-9972-f4bccd0d3263",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Add Candlestick Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c64645d-9248-4584-83b6-94e47ecd5629",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPrep['RETURN'] =  (dfPrep['close']-dfPrep['open'])/dfPrep['open']\n",
    "\n",
    "dfPrep[\"UPPER_SHADOW\"] =( dfPrep[\"high\"] - dfPrep[['close', 'open']].max(axis=1))/ dfPrep[['close', 'open']].max(axis=1)\n",
    "dfPrep[\"LOWER_SHADOW\"] = (dfPrep[['close', 'open']].min(axis=1) - dfPrep[\"low\"])/dfPrep[\"low\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6e36ba-2b5a-4d1b-9399-5c97c86cd8e8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Add Seasonal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44f0aa6e-b2ee-43c6-88a7-b753f4f6cb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPrep.loc[:, 'MINUTE'] = pd.to_datetime(dfPrep.loc[:, 'time'],unit='s').dt.minute\n",
    "dfPrep.loc[:, 'HOUR'] = pd.to_datetime(dfPrep.loc[:, 'time'],unit='s').dt.hour\n",
    "dfPrep.loc[:, 'DAY_OF_WEEK'] = pd.to_datetime(dfPrep.loc[:, 'time'],unit='s').dt.day_of_week\n",
    "dfPrep.loc[:, 'DAY_OF_MONTH'] = pd.to_datetime(dfPrep.loc[:, 'time'],unit='s').dt.day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35311b2-208e-458b-b6b9-ee191cf8cbdc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Select Only One Exchange Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd5fd4ed-242f-4371-9a41-7f04b4d2fd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfPrep.query('EXCHANGE_RATE == \"CBKG.DE\"', inplace  = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038d7214-b6af-422a-9b8b-ad613093fa2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Calculate Support and Resistence Levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c259931-59ff-488b-99b6-ad2b40dde81c",
   "metadata": {},
   "source": [
    "K-Means Algorithm used to identify the support and resistence levels.\n",
    "* https://medium.com/@judopro/using-machine-learning-to-programmatically-determine-stock-support-and-resistance-levels-9bb70777cf8e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0af030f-9ac2-484a-84c6-916c97b0a070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bIsSupport(df, l, n1, n2):\n",
    "#     for i in range(l-n1+1, l+1):\n",
    "#         if (df['low'].iloc[i] > df['low'].iloc[i-1]):\n",
    "#             return 0\n",
    "        \n",
    "#     for i in range(l+1, l+n2+1):\n",
    "#         if (df['low'].iloc[i] < df['low'].iloc[i-1] ):\n",
    "#             return 0\n",
    "        \n",
    "#     return 1\n",
    "\n",
    "\n",
    "# def bIsResistence(df, l, n1, n2):\n",
    "#     for i in range(l-n1+1, l+1):\n",
    "#         if (df['high'].iloc[i] > df['high'].iloc[i-1]):\n",
    "#             return 0\n",
    "        \n",
    "#     for i in range(l+1, l+n2+1):\n",
    "#         if (df['high'].iloc[i] < df['high'].iloc[i-1] ):\n",
    "#             return 0\n",
    "        \n",
    "#     return 1\n",
    "\n",
    "\n",
    "\n",
    "# n1 = 4\n",
    "# n2 = 4\n",
    "# for i in range(0, len(dfPrep)):\n",
    "#     dfPrep.iloc[i].loc['IS_SUPPORT'] = bIsSupport(dfPrep, i, n1, n2)\n",
    "#     dfPrep.iloc[i].loc['IS_RESISTENCE'] = bIsResistence(dfPrep, i, n1, n2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bcb69c-c19c-453f-995c-c5a067ec6d51",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "255aa2a3-3b0d-480f-859f-726437023f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GET_DATASETS_1(dfPreprocessed):\n",
    "\n",
    "    dfPreprocessed.sort_values(by = ['EXCHANGE_RATE', 'time'], inplace = True)\n",
    "\n",
    "    # prepare input dataset\n",
    "    dfX = dfPreprocessed[['RETURN', 'UPPER_SHADOW', 'LOWER_SHADOW']].copy()\n",
    "\n",
    "    df = pd.DataFrame(index = dfX.index)\n",
    "\n",
    "    for i in range(-BACKWARD_WINDOW_LENGTH, 0):\n",
    "        df = df.merge(right  =dfX.shift(-i).add_prefix('{}|'.format(i)), left_index= True, right_index = True)\n",
    "\n",
    "\n",
    "    # add seasonal features to input dataset\n",
    "    df = df.merge(right = dfPreprocessed[['MINUTE', 'HOUR', 'DAY_OF_WEEK','DAY_OF_MONTH']],left_index = True, right_index =True, how = 'inner') \n",
    "\n",
    "    # drop due to transition from one exchange to another\n",
    "    ixToDrop = dfPreprocessed[['EXCHANGE_RATE']].merge(right = dfPreprocessed[['EXCHANGE_RATE']].shift(BACKWARD_WINDOW_LENGTH), left_index  = True, right_index = True, how = 'inner').query('EXCHANGE_RATE_x != EXCHANGE_RATE_y').index\n",
    "    df.drop(ixToDrop, inplace = True)\n",
    "\n",
    "    dfX = df\n",
    "\n",
    "    # prepare output dataset\n",
    "    dfY = dfPreprocessed[['RETURN']].copy()\n",
    "\n",
    "    df = pd.DataFrame(index = dfY.index)\n",
    "\n",
    "    for i in range(0, FORWARD_WINDOW_LENGTH):\n",
    "        df = df.merge(right  =dfY.shift(-i).add_prefix('{}|'.format(i)), left_index= True, right_index = True)\n",
    "\n",
    "    ixToDrop = dfPreprocessed[['EXCHANGE_RATE']].merge(right = dfPreprocessed[['EXCHANGE_RATE']].shift(-FORWARD_WINDOW_LENGTH), left_index  = True, right_index = True, how = 'inner').query('EXCHANGE_RATE_x != EXCHANGE_RATE_y').index\n",
    "    df.drop(ixToDrop, inplace = True)\n",
    "\n",
    "    dfY = df\n",
    "\n",
    "    # drop records that contain at least one nan\n",
    "    ixToDrop = list(dfX[dfX.isna().any(axis = 1)].index) + list(dfY[dfY.isna().any(axis = 1)].index) \n",
    "    dfX.drop(ixToDrop, inplace = True)\n",
    "    dfY.drop(ixToDrop, inplace = True)\n",
    "\n",
    "    \n",
    "    # use common timestamps\n",
    "    ixCommon = np.intersect1d(dfX.index, dfY.index)\n",
    "    dfX = dfX.loc[ixCommon]\n",
    "    dfY = dfY.loc[ixCommon]\n",
    "    \n",
    "    return dfX, dfY\n",
    "\n",
    "\n",
    "def GET_DATASETS_2(dfPreprocessed):\n",
    "\n",
    "    #Create PySpark SparkSession\n",
    "    oSparkSess = SparkSession.builder \\\n",
    "        .master(\"local[1]\") \\\n",
    "        .appName(\"SparkByExamples.com\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    oSparkSess.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "    def dfGetPixels(df, iFrom, iTo):\n",
    "        df['from_time'] = df['time'] + pd.DateOffset(hours=iFrom)\n",
    "        df['to_time'] = df['time'] + pd.DateOffset(hours=iTo)\n",
    "\n",
    "\n",
    "        df = df[['to_time', 'from_time', 'time','close']]\n",
    "\n",
    "\n",
    "        sdf= oSparkSess.createDataFrame(df)\n",
    "        sdf.createOrReplaceTempView(\"sdf\")\n",
    "\n",
    "        dfPriceAnalysis = oSparkSess.sql(\"\"\"\n",
    "            SELECT \n",
    "                t.from_time, \n",
    "                t.to_time,\n",
    "                t.time, \n",
    "                t.close, \n",
    "                t.HISTORICAL_CLOSE, \n",
    "                count(*) AS NR_OF_MINS\n",
    "            FROM\n",
    "            (\n",
    "                SELECT df1.*, df2.close as HISTORICAL_CLOSE FROM sdf df1\n",
    "                INNER JOIN sdf df2\n",
    "                ON df2.time >= df1.from_time and df2.time < df1.to_time\n",
    "            ) t\n",
    "            GROUP BY t.from_time, t.to_time, t.time, t.close, t.HISTORICAL_CLOSE\n",
    "            ORDER BY t.from_time, t.to_time, t.time, t.close, t.HISTORICAL_CLOSE\n",
    "        \"\"\").toPandas()\n",
    "\n",
    "        df.set_index(['time', 'close'], inplace = True)\n",
    "        dfToReturn = pd.DataFrame(\n",
    "            data =  np.zeros((len(df) , NR_OF_PIXELS)).astype(int), \n",
    "            columns = list(range(0,NR_OF_PIXELS)),\n",
    "            index = df.index\n",
    "        )\n",
    "\n",
    "\n",
    "        if len(dfPriceAnalysis)==0:\n",
    "            return dfToReturn\n",
    "        else:\n",
    "            dfPriceAnalysis.sort_values(['time', 'HISTORICAL_CLOSE'], inplace = True)\n",
    "            dfPriceAnalysis.loc[:, 'DIFF'] =  dfPriceAnalysis.loc[: , 'HISTORICAL_CLOSE']-dfPriceAnalysis.loc[:, 'close']\n",
    "            oDiscretizer = KBinsDiscretizer(n_bins=NR_OF_PIXELS, encode='ordinal', strategy='quantile')\n",
    "\n",
    "            dfSample = pd.DataFrame(\n",
    "                data = np.arange(-0.10, 0.10 , (0.20/NR_OF_PIXELS)),\n",
    "                columns = ['DIFF']\n",
    "            )\n",
    "            oDiscretizer.fit(dfSample)\n",
    "            dfPriceAnalysis['DIFF_BIN'] =  oDiscretizer.transform(dfPriceAnalysis[['DIFF']]).astype(int)\n",
    "            dfPixel = dfPriceAnalysis.pivot_table(index = ['time', 'close'], columns = 'DIFF_BIN', values = 'NR_OF_MINS', aggfunc = 'sum', fill_value = 0)\n",
    "\n",
    "            dfToReturn.loc[dfPixel.index] = dfPixel.values\n",
    "\n",
    "            return dfToReturn\n",
    "\n",
    "\n",
    "    df = dfPreprocessed.copy()\n",
    "\n",
    "    # build input dataset\n",
    "    X = []\n",
    "    for i in range(-BACKWARD_WINDOW_LENGTH, 0):\n",
    "        iFrom = i\n",
    "        iTo = i +1\n",
    "\n",
    "        print(iFrom)\n",
    "        dfPixel = dfGetPixels(df, iFrom, iTo)\n",
    "        dfPixel= dfPixel[BACKWARD_WINDOW_LENGTH * 60:]\n",
    "        dfPixel= dfPixel[:-FORWARD_WINDOW_LENGTH * 60]\n",
    "\n",
    "        if len(X) == 0:\n",
    "            X = dfPixel.values\n",
    "        else:\n",
    "            X = np.append(X, dfPixel.values, axis = 1)\n",
    "\n",
    "    X = X.reshape((-1, BACKWARD_WINDOW_LENGTH, dfPixel.shape[1]))\n",
    "\n",
    "    # build output dataset\n",
    "    Y = []\n",
    "    for i in range(0, FORWARD_WINDOW_LENGTH):\n",
    "        iFrom = i\n",
    "        iTo = i +1\n",
    "\n",
    "        print(iFrom)\n",
    "        dfPixel = dfGetPixels(df, iFrom, iTo)\n",
    "\n",
    "        dfPixel= dfPixel[BACKWARD_WINDOW_LENGTH * 60:]\n",
    "        dfPixel= dfPixel[:-FORWARD_WINDOW_LENGTH * 60]\n",
    "\n",
    "        if len(Y) == 0:\n",
    "            Y = dfPixel.values\n",
    "        else:\n",
    "            Y = np.append(Y, dfPixel.values, axis = 1)\n",
    "\n",
    "    Y = Y.reshape((-1, FORWARD_WINDOW_LENGTH, dfPixel.shape[1]))\n",
    "\n",
    "    # reshape the image in following format (batch_size, NR_OF_PIXELS, TIME_STEP)\n",
    "    X = np.transpose(X, (0,2,1))\n",
    "    Y = np.transpose(Y, (0,2,1))\n",
    "\n",
    "    # # divide all the minutes to 60 to normalize between 0-1 (1hr contains maximum 60 1m candlesticks)\n",
    "    X = X/60\n",
    "    Y = Y/60\n",
    "\n",
    "    df= df.iloc[BACKWARD_WINDOW_LENGTH * 60:]\n",
    "    df= df.iloc[:-FORWARD_WINDOW_LENGTH * 60]\n",
    "    \n",
    "    return X, Y, df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8504b026-173d-426d-8285-f90b89eab751",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3aafa4f-310f-46c2-bcf1-30a2eb4cd207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfX, dfY = GET_DATASETS_1(dfPrep)\n",
    "\n",
    "# # # independent component analysis\n",
    "# # df = dfX.copy()\n",
    "# # oIca = FastICA(n_components=7, random_state=0, whiten='unit-variance')\n",
    "\n",
    "# # df = pd.DataFrame(\n",
    "# #     data = oIca.fit_transform(df),\n",
    "# #     index = df.index\n",
    "# # )\n",
    "# # df = df.add_prefix('ICA_')\n",
    "# # dfX = df\n",
    "\n",
    "# # # remove outliers from X\n",
    "# # ixOutliers = dfX[(((dfX-dfX.mean())/dfX.std()).abs() > 3 ).any(axis = 1) == True].index\n",
    "# # dfX.drop(ixOutliers, inplace = True)\n",
    "\n",
    "# # # remove outliers from Y\n",
    "# # ixOutliers = dfY[(((dfY-dfY.mean())/dfY.std()).abs() > 3 ).any(axis = 1) == True].index\n",
    "# # dfY.drop(ixOutliers, inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "# # standarize input\n",
    "# oScalerInput = StandardScaler()\n",
    "# df =  dfX.copy()\n",
    "\n",
    "# df = pd.DataFrame(\n",
    "#     data = oScalerInput.fit_transform(df),\n",
    "#     columns =  df.columns,\n",
    "#     index = df.index\n",
    "# )\n",
    "\n",
    "# dfX = df.copy()\n",
    "\n",
    "\n",
    "# # standardize output\n",
    "# oScalerOutput = StandardScaler()\n",
    "# df =  dfY.copy()\n",
    "\n",
    "# df = pd.DataFrame(\n",
    "#     data = oScalerOutput.fit_transform(df),\n",
    "#     columns =  df.columns,\n",
    "#     index = df.index\n",
    "# )\n",
    "\n",
    "# dfY = df.copy()\n",
    "\n",
    "\n",
    "# # # PCA input\n",
    "# # oPca = PCA(0.90)\n",
    "# # df = dfX.copy()\n",
    "\n",
    "# # df = pd.DataFrame(\n",
    "# #     data = oPca.fit_transform(df),\n",
    "# #     index = df.index\n",
    "# # )\n",
    "# # df = df.add_prefix('PCA_')\n",
    "# # dfX = df.copy()\n",
    "\n",
    "\n",
    "# # split data to train-validation-test\n",
    "\n",
    "# c_fTrainingRatio = 0.70\n",
    "# c_fValidationRatio = 0.28\n",
    "# c_fTestRatio = 0.02\n",
    "\n",
    "# ixTrain,ixTest = train_test_split(\n",
    "#     dfX.index,\n",
    "#     test_size=1-c_fTrainingRatio,\n",
    "#     shuffle=False,\n",
    "#     random_state = 1\n",
    "# )\n",
    "\n",
    "# ixValidation,ixTest= train_test_split(\n",
    "#     ixTest,\n",
    "#     test_size=c_fTestRatio/(c_fTestRatio + c_fValidationRatio),\n",
    "#     shuffle=False,\n",
    "#     random_state = 1\n",
    "# )\n",
    "\n",
    "\n",
    "# dfX_train = dfX.loc[ixTrain]\n",
    "# dfX_validation = dfX.loc[ixValidation]\n",
    "# dfX_test = dfX.loc[ixTest]\n",
    "\n",
    "# dfY_train = dfY.loc[ixTrain]\n",
    "# dfY_validation = dfY.loc[ixValidation]\n",
    "# dfY_test = dfY.loc[ixTest]\n",
    "\n",
    "\n",
    "# # check identicality of distributions\n",
    "# df = dfX.copy()\n",
    "# dfDistAnalysis = pd.DataFrame(columns = ['FEATURE', 'TRAIN_VAL', 'TRAIN_TEST', 'VAL_TEST'])\n",
    "# for sCol in df.columns:\n",
    "#     # The null hypothesis is that the two distributions are identical\n",
    "#     # If the KS statistic is small or the p-value is high, then we cannot reject the null hypothesis in favor of the alternative.\n",
    "#     fStatsTrainVal, fPValueTrainVal = ks_2samp(\n",
    "#         df.loc[ixTrain, sCol],\n",
    "#         df.loc[ixValidation, sCol]\n",
    "#     )\n",
    "\n",
    "#     fStatsTrainTest, fPValueTrainTest = ks_2samp(\n",
    "#         df.loc[ixTrain, sCol],\n",
    "#         df.loc[ixTest, sCol]\n",
    "#     )\n",
    "\n",
    "#     fStatsValTest, fPValueValTest = ks_2samp(\n",
    "#         df.loc[ixValidation, sCol],\n",
    "#         df.loc[ixTest, sCol]\n",
    "#     )\n",
    "    \n",
    "#     dfDistAnalysis = pd.concat(\n",
    "#          [dfDistAnalysis,\n",
    "#          pd.DataFrame(data = [[sCol, fPValueTrainVal,  fPValueTrainTest, fPValueValTest]], columns = dfDistAnalysis.columns)\n",
    "#          ],\n",
    "#         ignore_index=False\n",
    "#     )\n",
    "\n",
    "# print('identicality of distributions: \\n\\n {}'.format(dfDistAnalysis))\n",
    "\n",
    "\n",
    "\n",
    "# # model development\n",
    "# X_train = dfX_train.values\n",
    "# X_validation = dfX_validation.values\n",
    "# X_test = dfX_test.values\n",
    "\n",
    "\n",
    "# Y_train = dfY_train.values\n",
    "# Y_validation = dfY_validation.values\n",
    "# Y_test = dfY_test.values\n",
    "\n",
    "\n",
    "# # compile model\n",
    "# aInput = tf.keras.Input(\n",
    "#     shape =  X_train.shape[1]\n",
    "# )\n",
    "\n",
    "# aHidden1 = tf.keras.layers.Dense(\n",
    "#     units = 300, \n",
    "#     kernel_initializer='normal',\n",
    "#     # activity_regularizer = tf.keras.regularizers.L2(0.1),\n",
    "#     use_bias = False\n",
    "# )(aInput)\n",
    "\n",
    "# aHidden1 = tf.keras.layers.BatchNormalization()(aHidden1)\n",
    "# aHidden1 = tf.keras.layers.ReLU()(aHidden1)\n",
    "# # aHidden1 = tf.keras.layers.Dropout(0.5)(aHidden1)\n",
    "\n",
    "\n",
    "# aHidden2 = aHidden1\n",
    "# # aHidden2 = tf.keras.layers.Dense(\n",
    "# #     units = 300, \n",
    "# #      kernel_initializer='normal',\n",
    "# #     activity_regularizer = tf.keras.regularizers.L2(0.1),  \n",
    "# #     use_bias = False\n",
    "# # )(aHidden1)\n",
    "\n",
    "# # aHidden2 = tf.keras.layers.BatchNormalization()(aHidden2)\n",
    "# # aHidden2 = tf.keras.layers.ReLU()(aHidden2)\n",
    "# # aHidden2 = tf.keras.layers.Dropout(0.5)(aHidden2)\n",
    "\n",
    "# aOutput = tf.keras.layers.Dense(\n",
    "#     units = Y_train.shape[1],\n",
    "#     kernel_initializer='normal',\n",
    "#     # activation = 'ReLU'\n",
    "# )(aHidden2)\n",
    "\n",
    "# oModel = tf.keras.Model(inputs=aInput, outputs=aOutput)\n",
    "\n",
    "# oLearningRateSchedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#     initial_learning_rate = 1e-03 * 2,\n",
    "#     decay_steps=100000,\n",
    "#     decay_rate=1e-02,\n",
    "#     staircase=True)\n",
    "\n",
    "\n",
    "# oOptimizer = tf.keras.optimizers.Adam(learning_rate=oLearningRateSchedule)\n",
    "\n",
    "# oModel.compile(optimizer=oOptimizer,loss= tf.keras.losses.MeanSquaredError())\n",
    "\n",
    "# tf.keras.utils.plot_model(oModel, show_shapes=True)\n",
    "\n",
    "# print(oModel.summary())\n",
    "\n",
    "\n",
    "# # fit model\n",
    "# oEarlyStop = tf.keras.callbacks.EarlyStopping(\n",
    "#     monitor = 'val_loss', \n",
    "#     mode = 'min', \n",
    "#     verbose = 0 , \n",
    "#     patience = 50, \n",
    "#     restore_best_weights = True)\n",
    "\n",
    "# oModel.fit(\n",
    "#     X_train, \n",
    "#     Y_train, \n",
    "#     epochs= 1000, \n",
    "#     batch_size=2**5, \n",
    "#     verbose=0, \n",
    "#     validation_data= (X_validation, Y_validation),\n",
    "#     # callbacks=[oEarlyStop]\n",
    "# )\n",
    "\n",
    "\n",
    "# # show epoch history\n",
    "# dfHistory = pd.DataFrame(oModel.history.history)\n",
    "\n",
    "# plt.figure(figsize = (20, 8))\n",
    "# sns.lineplot(data = dfHistory['loss'].iloc[1:], legend = True, label = 'Train')\n",
    "# sns.lineplot(data = dfHistory['val_loss'].iloc[1:], legend = True, label = 'Validation')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# # test model\n",
    "# a_Y_datasets = [Y_train, Y_validation]\n",
    "# a_X_datasets = [X_train, X_validation]\n",
    "# a_labels = ['train', 'validation'] \n",
    "\n",
    "# plt.figure(figsize = (20,8 ))\n",
    "# for X,Y, sLabel in list(zip(a_X_datasets, a_Y_datasets, a_labels)):\n",
    "#     aActual = oScalerOutput.inverse_transform(Y)\n",
    "#     aPred = oScalerOutput.inverse_transform(oModel.predict(X))\n",
    "\n",
    "#     df = pd.DataFrame(data = np.column_stack((aActual,aPred)),\n",
    "#                  columns = ['ACTUAL', 'PREDICTION']\n",
    "#                 )\n",
    "    \n",
    "#     fR2Score = round(r2_score(aActual, aPred), 1)\n",
    "#     sLabel = '{}   r2: {}'.format(sLabel,fR2Score)\n",
    "#     sns.scatterplot(data = df, x = 'ACTUAL', y = 'PREDICTION', label = sLabel)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d93e08-dd63-4a02-9461-bfc0cc522838",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# PIX-2-PIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4ee8bdb-7b92-4f0e-9862-d69e2f6e3972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define the discriminator model\n",
    "# def define_discriminator(image_shape = Y[0].shape):\n",
    "#     # weight initialization\n",
    "#     init = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
    "#     # source image input\n",
    "#     in_src_image = tf.keras.layers.Input(shape=image_shape)\n",
    "#     # target image input\n",
    "#     in_target_image = tf.keras.layers.Input(shape=image_shape)\n",
    "    \n",
    "    \n",
    "#     # concatenate images channel-wise\n",
    "#     merged = tf.keras.layers.Concatenate()([in_src_image, in_target_image])\n",
    "#     # C64\n",
    "#     d = tf.keras.layers.Conv1D(64, 1, strides=1, padding='same', kernel_initializer=init)(merged)\n",
    "#     d = tf.keras.layers.LeakyReLU(alpha=0.2)(d)\n",
    "#     # C128\n",
    "#     d = tf.keras.layers.Conv1D(128, 1, strides=1, padding='same', kernel_initializer=init)(d)\n",
    "#     d = tf.keras.layers.BatchNormalization()(d)\n",
    "#     d = tf.keras.layers.LeakyReLU(alpha=0.2)(d)\n",
    "#     # C256\n",
    "#     d = tf.keras.layers.Conv1D(256, 1, strides=1, padding='same', kernel_initializer=init)(d)\n",
    "#     d = tf.keras.layers.BatchNormalization()(d)\n",
    "#     d = tf.keras.layers.LeakyReLU(alpha=0.2)(d)\n",
    "#     # C512\n",
    "#     d = tf.keras.layers.Conv1D(512, 1, strides=1, padding='same', kernel_initializer=init)(d)\n",
    "#     d = tf.keras.layers.BatchNormalization()(d)\n",
    "#     d = tf.keras.layers.LeakyReLU(alpha=0.2)(d)\n",
    "#     # second last output layer\n",
    "#     d = tf.keras.layers.Conv1D(512, 1, padding='same', kernel_initializer=init)(d)\n",
    "#     d = tf.keras.layers.BatchNormalization()(d)\n",
    "#     d = tf.keras.layers.LeakyReLU(alpha=0.2)(d)\n",
    "#     # patch output\n",
    "#     d = tf.keras.layers.Conv1D(1, 1, padding='same', kernel_initializer=init)(d)\n",
    "#     patch_out = tf.keras.layers.Activation('sigmoid')(d)\n",
    "#     # define model\n",
    "#     model = tf.keras.Model([in_src_image, in_target_image], patch_out, name = 'DISCRIMINATOR_MODEL')\n",
    "\n",
    "#     return model\n",
    "\n",
    "\n",
    "# def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "#     loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    \n",
    "#     real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
    "\n",
    "#     generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "#     total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "#     return total_disc_loss\n",
    "\n",
    "\n",
    "# # define an encoder block\n",
    "# def define_encoder_block(layer_in, n_filters, batchnorm=True):\n",
    "#     # weight initialization\n",
    "#     init = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
    "#     # add downsampling layer\n",
    "#     g = tf.keras.layers.Conv1D(n_filters, 1, strides=1, padding='same', kernel_initializer=init)(layer_in)\n",
    "#     # conditionally add batch normalization\n",
    "#     if batchnorm:\n",
    "#         g = tf.keras.layers.BatchNormalization()(g, training=True)\n",
    "#     # leaky relu activation\n",
    "#     g = tf.keras.layers.LeakyReLU(alpha=0.2)(g)\n",
    "#     return g\n",
    "\n",
    "# # define a decoder block\n",
    "# def decoder_block(layer_in, skip_in, n_filters, dropout=True):\n",
    "#     # weight initialization\n",
    "#     init = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
    "#     # add upsampling layer\n",
    "#     g = tf.keras.layers.Convolution1DTranspose(n_filters, 1, strides=1, padding='same', kernel_initializer=init)(layer_in)\n",
    "#     # add batch normalization\n",
    "#     g = tf.keras.layers.BatchNormalization()(g, training=True)\n",
    "#     # conditionally add dropout\n",
    "#     if dropout:\n",
    "#         g = tf.keras.layers.Dropout(0.5)(g, training=True)\n",
    "#     # merge with skip connection\n",
    "#     g = tf.keras.layers.Concatenate()([g, skip_in])\n",
    "#     # relu activation\n",
    "#     g = tf.keras.layers.Activation('relu')(g)\n",
    "#     return g\n",
    "\n",
    "\n",
    "# # define the standalone generator model\n",
    "# def define_generator(image_shape=X[0].shape):\n",
    "#     # weight initialization\n",
    "#     init = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
    "#     # image input\n",
    "#     in_image = tf.keras.layers.Input(shape=image_shape)\n",
    "#     # encoder model\n",
    "    \n",
    "#     e1 = define_encoder_block(in_image, 64, batchnorm=False)\n",
    "#     e2 = define_encoder_block(e1, 128)\n",
    "#     e3 = define_encoder_block(e2, 256)\n",
    "#     e4 = define_encoder_block(e3, 512)\n",
    "#     e5 = define_encoder_block(e4, 512)\n",
    "#     e6 = define_encoder_block(e5, 512)\n",
    "#     e7 = define_encoder_block(e6, 512)\n",
    "\n",
    "    \n",
    "#     # bottleneck, no batch norm and relu\n",
    "#     b = tf.keras.layers.Conv1D(512, 1, strides=1, padding='same', kernel_initializer=init)(e7)\n",
    "#     b = tf.keras.layers.Activation('relu')(b)\n",
    "    \n",
    "#     # decoder model\n",
    "#     d1 = decoder_block(b, e7, 512)\n",
    "#     d2 = decoder_block(d1, e6, 512)\n",
    "#     d3 = decoder_block(d2, e5, 512)\n",
    "#     d4 = decoder_block(d3, e4, 512, dropout=False)\n",
    "#     d5 = decoder_block(d4, e3, 256, dropout=False)\n",
    "#     d6 = decoder_block(d5, e2, 128, dropout=False)\n",
    "#     d7 = decoder_block(d6, e1, 64, dropout=False)\n",
    "    \n",
    "    \n",
    "#     # output\n",
    "#     g = tf.keras.layers.Conv1DTranspose(FORWARD_WINDOW_LENGTH, 1, strides=1, padding='same', kernel_initializer=init)(d7)\n",
    "#     out_image = tf.keras.layers.Activation('tanh')(g)\n",
    "#     # define model\n",
    "#     model = tf.keras.Model(in_image, out_image, name = 'GENERATOR_MODEL')\n",
    "#     return model\n",
    "\n",
    "\n",
    "# def generator_loss(disc_generated_output, gen_output, target):\n",
    "#     loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    \n",
    "#     gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "#     # Mean absolute error\n",
    "#     l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "\n",
    "#     total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
    "\n",
    "#     return total_gen_loss, gan_loss, l1_loss\n",
    "\n",
    "\n",
    "# oDiscriminator = define_discriminator()\n",
    "\n",
    "# oGenerator = define_generator()\n",
    "\n",
    "# generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "# discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "    \n",
    "# @tf.function\n",
    "# def train_step(input_image, target, step):\n",
    "#     with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "#         gen_output = oGenerator(input_image, training=True)\n",
    "        \n",
    "#         # disc_real_output is not so much applicable for exchange rate prediction.\n",
    "#         # because, in image recognition, templates of both input and target should match.\n",
    "#         # in time series, template of backward and forward should be different.\n",
    "#         # due to this reason, we should either generate different type of loss function.\n",
    "#         # or we should use GAN for another purpose than pix2pix.\n",
    "#         disc_real_output = oDiscriminator([input_image, target], training=True)\n",
    "#         disc_generated_output = oDiscriminator([input_image, gen_output], training=True)\n",
    "\n",
    "#         gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n",
    "#         disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
    "\n",
    "#         generator_gradients = gen_tape.gradient(gen_total_loss, generator.trainable_variables)\n",
    "#         discriminator_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "#         generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
    "#         discriminator_optimizer.apply_gradients(zip(discriminator_gradients,discriminator.trainable_variables))\n",
    "\n",
    "#         with summary_writer.as_default():\n",
    "#             tf.summary.scalar('gen_total_loss', gen_total_loss, step=step//1000)\n",
    "#             tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=step//1000)\n",
    "#             tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=step//1000)\n",
    "#             tf.summary.scalar('disc_loss', disc_loss, step=step//1000)\n",
    "\n",
    "            \n",
    "# def generate_images(model, test_input, tar):\n",
    "#     prediction = model(test_input, training=True)\n",
    "    \n",
    "\n",
    "#     display_list = [test_input[0], tar[0], prediction[0]]\n",
    "#     title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
    "\n",
    "#     for i in range(3):\n",
    "#         plt.figure(figsize=(15, 15))\n",
    "#         plt.subplot(1, 3, i+1)\n",
    "#         plt.title(title[i])\n",
    "#         # Getting the pixel values in the [0, 1] range to plot.\n",
    "#         plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "#         plt.axis('off')\n",
    "#         plt.show()\n",
    "\n",
    "            \n",
    "            \n",
    "# X, Y = GET_DATASETS_2(dfPrep)\n",
    "# # split data to train-validation-test\n",
    "# c_fTrainingRatio = 0.70\n",
    "# c_fValidationRatio = 0.28\n",
    "# c_fTestRatio = 0.02\n",
    "\n",
    "# X_train,X_test, Y_train, Y_test = train_test_split(\n",
    "#     X,Y,\n",
    "#     test_size=1-c_fTrainingRatio,\n",
    "#     shuffle=False,\n",
    "#     random_state = 1\n",
    "# )\n",
    "\n",
    "# X_validation, X_test,Y_validation, Y_test= train_test_split(\n",
    "#     X_test,Y_test,\n",
    "#     test_size=c_fTestRatio/(c_fTestRatio + c_fValidationRatio),\n",
    "#     shuffle=False,\n",
    "#     random_state = 1\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# BATCH_SIZE = 50\n",
    "\n",
    "# train_ds = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "# validation_ds = tf.data.Dataset.from_tensor_slices((X_validation, Y_validation))\n",
    "\n",
    "# example_input, example_target = next(iter(validation_ds.take(1)))\n",
    "# example_input = tf.expand_dims(example_input, axis = 0)            \n",
    "# example_target = tf.expand_dims(example_target, axis = 0)   \n",
    "\n",
    "\n",
    "# start = time.time()\n",
    "\n",
    "# i = 0\n",
    "# for (input_image, target) in train_ds.batch(BATCH_SIZE):\n",
    "\n",
    "#     if (i) % 1000 == 0:\n",
    "#         display.clear_output(wait=True)\n",
    "\n",
    "#         if i != 0:\n",
    "#             print(f'Time taken for 1000 steps: {time.time()-start:.2f} sec\\n')\n",
    "\n",
    "#         start = time.time()\n",
    "        \n",
    "#         generate_images(oGenerator, example_input, example_target)\n",
    "#         print(f\"Step: {i//1000}k\")\n",
    "\n",
    "#     # input_image = tf.expand_dims(input_image, axis = 0)            \n",
    "#     # target = tf.expand_dims(target, axis = 0)           \n",
    "    \n",
    "#     train_step(input_image, target, i)\n",
    "\n",
    "#     # Training step\n",
    "#     if (i+1) % 10 == 0:\n",
    "#         print('.', end='', flush=True)\n",
    "    \n",
    "#     i = i +1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d45e7ab-d360-4866-8918-200424b15908",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# LUONG'S ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30b2365c-b7c8-4906-b6bd-ee56c2156d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = dfPrep.query('EXCHANGE_RATE == \"DBKGn.DE\"')\n",
    "# df.query('time >= \"2022-10-15\"', inplace = True)\n",
    "\n",
    "# X, Y, df = GET_DATASETS_2(df)\n",
    "\n",
    "# # split data to train-validation-test\n",
    "# c_fTrainingRatio = 0.70\n",
    "# c_fValidationRatio = 0.28\n",
    "# c_fTestRatio = 0.02\n",
    "\n",
    "# X_train,X_test, Y_train, Y_test = train_test_split(\n",
    "#     X,Y,\n",
    "#     test_size=1-c_fTrainingRatio,\n",
    "#     shuffle=False,\n",
    "#     random_state = 1\n",
    "# )\n",
    "\n",
    "# X_validation, X_test,Y_validation, Y_test= train_test_split(\n",
    "#     X_test,Y_test,\n",
    "#     test_size=c_fTestRatio/(c_fTestRatio + c_fValidationRatio),\n",
    "#     shuffle=False,\n",
    "#     random_state = 1\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# ModelInput = tf.keras.Input(\n",
    "#     shape=(NR_OF_PIXELS, BACKWARD_WINDOW_LENGTH))\n",
    "\n",
    "# aEncoderInputs = tf.keras.layers.Permute((2,1))(ModelInput)\n",
    "\n",
    "# aEncoderHiddens, aFinalH, aFinalC = tf.keras.layers.LSTM(300,\n",
    "#                                                          return_state = True, \n",
    "#                                                          return_sequences = True,\n",
    "#                                                          name = 'Lstm_Encoder'\n",
    "#                                                         )(aEncoderInputs)\n",
    "# aFinalH = tf.keras.layers.BatchNormalization(name = 'Encoder_Final_Hidden_State_Batch_Norm')(aFinalH)\n",
    "# aFinalC = tf.keras.layers.BatchNormalization(name = 'Encoder_Final_Carry_State_Batch_Norm')(aFinalC)\n",
    "\n",
    "# aDecoderInputs = tf.keras.layers.RepeatVector(FORWARD_WINDOW_LENGTH)(aFinalH)\n",
    "\n",
    "# aDecoderHiddens = tf.keras.layers.LSTM(300, \n",
    "#                        return_state = False, \n",
    "#                        return_sequences = True\n",
    "#                       )(aDecoderInputs, initial_state=[aFinalH, aFinalC])\n",
    "\n",
    "# aAttentions = tf.keras.layers.dot([aDecoderHiddens, aEncoderHiddens], axes=[2, 2])\n",
    "# aAttentions = tf.keras.layers.Softmax()(aAttentions)\n",
    "\n",
    "# aContextVector = tf.keras.layers.dot([aAttentions, aEncoderHiddens], axes=[2,1])\n",
    "# aContextVector = tf.keras.layers.BatchNormalization()(aContextVector)\n",
    "# aContextVector = tf.keras.layers.concatenate([aContextVector, aDecoderHiddens])\n",
    "\n",
    "# aDecoderOutputs = tf.keras.layers.TimeDistributed(\n",
    "#     tf.keras.layers.Dense(150)\n",
    "# )(aContextVector)\n",
    "\n",
    "# aDecoderOutputs = tf.keras.layers.TimeDistributed(\n",
    "#     tf.keras.layers.Dense(150)\n",
    "# )(aDecoderOutputs)\n",
    "\n",
    "# aDecoderOutputs = tf.keras.layers.TimeDistributed(\n",
    "#     tf.keras.layers.Dense(NR_OF_PIXELS)\n",
    "# )(aDecoderOutputs)\n",
    "\n",
    "# aModelOutput = tf.keras.layers.Permute((2,1))(aDecoderOutputs)\n",
    "\n",
    "# aModelOutput = tf.keras.layers.Softmax(axis = 1)(aModelOutput)\n",
    "\n",
    "# oModel = tf.keras.Model(\n",
    "#     inputs=ModelInput,\n",
    "#     outputs=aModelOutput,\n",
    "#     name = 'LUONG_ATTENTION_MODEL'\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# oOptimizer = tf.keras.optimizers.Adam(learning_rate=1e-04)\n",
    "# oModel.compile(\n",
    "#     loss = tf.keras.losses.CategoricalCrossentropy(), \n",
    "#     optimizer=oOptimizer\n",
    "# )\n",
    "\n",
    "\n",
    "# print(oModel.summary())\n",
    "\n",
    "\n",
    "# # fit model\n",
    "# oEarlyStop = tf.keras.callbacks.EarlyStopping(\n",
    "#     monitor = 'val_loss', \n",
    "#     mode = 'min', \n",
    "#     verbose = 0 , \n",
    "#     patience = 20, \n",
    "#     restore_best_weights = True)\n",
    "\n",
    "# oModel.fit(\n",
    "#     X_train, \n",
    "#     Y_train, \n",
    "#     epochs= 1000, \n",
    "#     batch_size=2**8, \n",
    "#     verbose=1, \n",
    "#     validation_data= (X_validation, Y_validation),\n",
    "#     callbacks=[oEarlyStop]\n",
    "# )\n",
    "\n",
    "# # show epoch history\n",
    "# dfHistory = pd.DataFrame(oModel.history.history)\n",
    "\n",
    "# plt.figure(figsize = (20, 8))\n",
    "# sns.lineplot(data = dfHistory['loss'].iloc[1:], legend = True, label = 'Train')\n",
    "# sns.lineplot(data = dfHistory['val_loss'].iloc[1:], legend = True, label = 'Validation')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# # test model\n",
    "# a_Y_datasets = [Y_train, Y_validation]\n",
    "# a_X_datasets = [X_train, X_validation]\n",
    "# a_labels = ['train', 'validation'] \n",
    "\n",
    "\n",
    "# for X_to_visualzie,Y_to_visualize, sLabel in list(zip(a_X_datasets, a_Y_datasets, a_labels)):\n",
    "#     pred = oModel.predict(X_to_visualzie)\n",
    "#     ground_true = Y_to_visualize \n",
    "#     for i in np.random.randint(low = 0,high = len(X_to_visualzie), size = 10):        \n",
    "#         fig, axs = plt.subplots(1,2)\n",
    "#         fig.suptitle('{}--{}'.format(sLabel, i))\n",
    "\n",
    "#         axs[0].imshow(ground_true[i], cmap='gray')\n",
    "#         axs[0].set_title( 'GROUND TRUE')\n",
    "\n",
    "#         axs[1].imshow(pred[i], cmap='gray')\n",
    "#         axs[1].set_title( 'PREDICTION')\n",
    "\n",
    "#         plt.show()\n",
    "\n",
    "# # this model should be enough capable to identify the times when the market is closed.\n",
    "# # maybe we can add seasonal features in additon.\n",
    "# # also, it doesn't predict well.\n",
    "# # maybe a learn-to-rank model could be helpful.\n",
    "# # also implement the R2s for each time_step-pixel_bin pairs.\n",
    "\n",
    "        \n",
    "# #     aActual = oScalerOutput.inverse_transform(Y)\n",
    "# #     aPred = oScalerOutput.inverse_transform(oModel.predict(X))\n",
    "\n",
    "# #     df = pd.DataFrame(data = np.column_stack((aActual,aPred)),\n",
    "# #                  columns = ['ACTUAL', 'PREDICTION']\n",
    "# #                 )\n",
    "    \n",
    "# #     fR2Score = round(r2_score(aActual, aPred), 1)\n",
    "# #     sLabel = '{}   r2: {}'.format(sLabel,fR2Score)\n",
    "# #     sns.scatterplot(data = df, x = 'ACTUAL', y = 'PREDICTION', label = sLabel)\n",
    "# # plt.show()\n",
    "\n",
    "\n",
    "# # tf.keras.utils.plot_model(oModel,  show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25572dea-2cfa-47f6-aa45-85561e3f1564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a02d5036-3525-4ed2-9ebd-d8b3ee05d8d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# LEARN-TO-RANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbd6fb26-e12a-49d9-8948-efc6e5ac9a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO BE DEVELOPED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5be55d0-9c57-45b4-816e-3e6b3afdd292",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TRANSFORMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60be1d81-fa3b-4748-bc52-d3f9801f9e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dfPrep.query('EXCHANGE_RATE == \"DBKGn.DE\"')\n",
    "df.query('time >= \"2022-10-15\"', inplace = True)\n",
    "\n",
    "X, Y, df = GET_DATASETS_2(df)\n",
    "\n",
    "# reshape the datasets\n",
    "X = np.transpose(X, (0,2,1))\n",
    "Y = np.transpose(Y, (0,2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aac3b301-6b63-4554-a293-bbcb302f9d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"TRANSFORMER_MODEL\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 19, 20)]     0           []                               \n",
      "                                                                                                  \n",
      " layer_normalization_16 (LayerN  (None, 19, 20)      40          ['input_3[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_8 (MultiH  (None, 19, 20)      85012       ['layer_normalization_16[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 19, 20)       0           ['multi_head_attention_8[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_16 (TFOpL  (None, 19, 20)      0           ['dropout_18[0][0]',             \n",
      " ambda)                                                           'input_3[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_17 (LayerN  (None, 19, 20)      40          ['tf.__operators__.add_16[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_16 (Conv1D)             (None, 19, 4)        84          ['layer_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 19, 4)        0           ['conv1d_16[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_17 (Conv1D)             (None, 19, 20)       100         ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_17 (TFOpL  (None, 19, 20)      0           ['conv1d_17[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_16[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_18 (LayerN  (None, 19, 20)      40          ['tf.__operators__.add_17[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_9 (MultiH  (None, 19, 20)      85012       ['layer_normalization_18[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 19, 20)       0           ['multi_head_attention_9[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_18 (TFOpL  (None, 19, 20)      0           ['dropout_20[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_17[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_19 (LayerN  (None, 19, 20)      40          ['tf.__operators__.add_18[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_18 (Conv1D)             (None, 19, 4)        84          ['layer_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 19, 4)        0           ['conv1d_18[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_19 (Conv1D)             (None, 19, 20)       100         ['dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_19 (TFOpL  (None, 19, 20)      0           ['conv1d_19[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_18[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_20 (LayerN  (None, 19, 20)      40          ['tf.__operators__.add_19[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_10 (Multi  (None, 19, 20)      85012       ['layer_normalization_20[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 19, 20)       0           ['multi_head_attention_10[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_20 (TFOpL  (None, 19, 20)      0           ['dropout_22[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_19[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_21 (LayerN  (None, 19, 20)      40          ['tf.__operators__.add_20[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_20 (Conv1D)             (None, 19, 4)        84          ['layer_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 19, 4)        0           ['conv1d_20[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_21 (Conv1D)             (None, 19, 20)       100         ['dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_21 (TFOpL  (None, 19, 20)      0           ['conv1d_21[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_20[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_22 (LayerN  (None, 19, 20)      40          ['tf.__operators__.add_21[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_11 (Multi  (None, 19, 20)      85012       ['layer_normalization_22[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 19, 20)       0           ['multi_head_attention_11[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_22 (TFOpL  (None, 19, 20)      0           ['dropout_24[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_21[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_23 (LayerN  (None, 19, 20)      40          ['tf.__operators__.add_22[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_22 (Conv1D)             (None, 19, 4)        84          ['layer_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_25 (Dropout)           (None, 19, 4)        0           ['conv1d_22[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_23 (Conv1D)             (None, 19, 20)       100         ['dropout_25[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_23 (TFOpL  (None, 19, 20)      0           ['conv1d_23[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_22[0][0]']\n",
      "                                                                                                  \n",
      " global_average_pooling1d_2 (Gl  (None, 19)          0           ['tf.__operators__.add_23[0][0]']\n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 128)          2560        ['global_average_pooling1d_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 128)          0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_26 (Dropout)           (None, 128)          0           ['re_lu_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 40)           5160        ['dropout_26[0][0]']             \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None, 2, 20)        0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 348,824\n",
      "Trainable params: 348,824\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/1000\n",
      "12/12 [==============================] - 58s 2s/step - loss: 0.3771 - val_loss: 0.1552\n",
      "Epoch 2/1000\n",
      "12/12 [==============================] - 23s 2s/step - loss: 0.1485 - val_loss: 0.0451\n",
      "Epoch 3/1000\n",
      "12/12 [==============================] - 22s 2s/step - loss: 0.0621 - val_loss: 0.0449\n",
      "Epoch 4/1000\n",
      "12/12 [==============================] - 23s 2s/step - loss: 0.0476 - val_loss: 0.0449\n",
      "Epoch 5/1000\n",
      "12/12 [==============================] - 23s 2s/step - loss: 0.0455 - val_loss: 0.0449\n",
      "Epoch 6/1000\n",
      "12/12 [==============================] - 23s 2s/step - loss: 0.0449 - val_loss: 0.0449\n",
      "Epoch 7/1000\n",
      "12/12 [==============================] - 23s 2s/step - loss: 0.0447 - val_loss: 0.0449\n",
      "Epoch 8/1000\n",
      "12/12 [==============================] - 22s 2s/step - loss: 0.0445 - val_loss: 0.0449\n",
      "Epoch 9/1000\n",
      "12/12 [==============================] - 23s 2s/step - loss: 0.0444 - val_loss: 0.0449\n",
      "Epoch 10/1000\n",
      "12/12 [==============================] - 22s 2s/step - loss: 0.0443 - val_loss: 0.0449\n",
      "Epoch 11/1000\n",
      "12/12 [==============================] - 23s 2s/step - loss: 0.0442 - val_loss: 0.0449\n",
      "Epoch 12/1000\n",
      "12/12 [==============================] - 23s 2s/step - loss: 0.0443 - val_loss: 0.0449\n",
      "Epoch 13/1000\n",
      "12/12 [==============================] - 22s 2s/step - loss: 0.0442 - val_loss: 0.0449\n",
      "Epoch 14/1000\n",
      "12/12 [==============================] - 22s 2s/step - loss: 0.0441 - val_loss: 0.0449\n",
      "Epoch 15/1000\n",
      "12/12 [==============================] - 22s 2s/step - loss: 0.0442 - val_loss: 0.0449\n",
      "Epoch 16/1000\n",
      "12/12 [==============================] - 22s 2s/step - loss: 0.0442 - val_loss: 0.0449\n",
      "Epoch 17/1000\n",
      "12/12 [==============================] - 23s 2s/step - loss: 0.0441 - val_loss: 0.0449\n",
      "Epoch 18/1000\n",
      "12/12 [==============================] - 20s 2s/step - loss: 0.0440 - val_loss: 0.0449\n",
      "Epoch 19/1000\n",
      "12/12 [==============================] - 31s 3s/step - loss: 0.0441 - val_loss: 0.0449\n",
      "Epoch 20/1000\n",
      "12/12 [==============================] - 16s 1s/step - loss: 0.0441 - val_loss: 0.0449\n",
      "Epoch 21/1000\n",
      "12/12 [==============================] - 20s 2s/step - loss: 0.0440 - val_loss: 0.0449\n",
      "Epoch 22/1000\n",
      "12/12 [==============================] - 18s 1s/step - loss: 0.0440 - val_loss: 0.0449\n",
      "Epoch 23/1000\n",
      "12/12 [==============================] - 18s 2s/step - loss: 0.0440 - val_loss: 0.0449\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f8f985a208>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data to train-validation-test\n",
    "c_fTrainingRatio = 0.70\n",
    "c_fValidationRatio = 0.28\n",
    "c_fTestRatio = 0.02\n",
    "\n",
    "X_train,X_test, Y_train, Y_test = train_test_split(\n",
    "    X,Y,\n",
    "    test_size=1-c_fTrainingRatio,\n",
    "    shuffle=False,\n",
    "    random_state = 1\n",
    ")\n",
    "\n",
    "X_validation, X_test,Y_validation, Y_test= train_test_split(\n",
    "    X_test,Y_test,\n",
    "    test_size=c_fTestRatio/(c_fTestRatio + c_fValidationRatio),\n",
    "    shuffle=False,\n",
    "    random_state = 1\n",
    ")\n",
    "\n",
    "\n",
    "# compile model\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = tf.keras.layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = tf.keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res\n",
    "\n",
    "\n",
    "ModelInput = tf.keras.Input(\n",
    "    shape=(BACKWARD_WINDOW_LENGTH, NR_OF_PIXELS))\n",
    "    \n",
    "W = ModelInput\n",
    "for _ in range(4):\n",
    "    W = transformer_encoder(W, 256, 4, 4, 0.5)\n",
    "\n",
    "W = tf.keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(W)\n",
    "for dim in [128]:\n",
    "    W = tf.keras.layers.Dense(dim)(W)\n",
    "    W = tf.keras.layers.ReLU()(W)\n",
    "    W = tf.keras.layers.Dropout(0.5)(W)\n",
    "\n",
    "W = tf.keras.layers.Dense(FORWARD_WINDOW_LENGTH*NR_OF_PIXELS, activation=\"sigmoid\")(W)\n",
    "W = tf.keras.layers.Reshape((FORWARD_WINDOW_LENGTH, NR_OF_PIXELS))(W)\n",
    "\n",
    "ModelOutput = W\n",
    "oModel = tf.keras.Model(ModelInput, ModelOutput, name = 'TRANSFORMER_MODEL')\n",
    "\n",
    "oOptimizer = tf.keras.optimizers.Adam(learning_rate=1e-03)\n",
    "oModel.compile(\n",
    "    loss = tf.keras.losses.MeanAbsoluteError(), \n",
    "    optimizer=oOptimizer\n",
    ")\n",
    "\n",
    "print(oModel.summary())\n",
    "\n",
    "tf.keras.utils.plot_model(oModel, show_shapes=True)\n",
    "\n",
    "\n",
    "# fit model\n",
    "oEarlyStop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss', \n",
    "    mode = 'min', \n",
    "    verbose = 0 , \n",
    "    patience = 20, \n",
    "    restore_best_weights = True)\n",
    "\n",
    "oModel.fit(\n",
    "    X_train, \n",
    "    Y_train, \n",
    "    epochs= 1000, \n",
    "    batch_size=2**8, \n",
    "    verbose=1, \n",
    "    validation_data= (X_validation, Y_validation),\n",
    "    callbacks=[oEarlyStop]\n",
    ")\n",
    "\n",
    "# show epoch history\n",
    "dfHistory = pd.DataFrame(oModel.history.history)\n",
    "\n",
    "plt.figure(figsize = (20, 8))\n",
    "sns.lineplot(data = dfHistory['loss'].iloc[1:], legend = True, label = 'Train')\n",
    "sns.lineplot(data = dfHistory['val_loss'].iloc[1:], legend = True, label = 'Validation')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# test model\n",
    "a_Y_datasets = [Y_train, Y_validation]\n",
    "a_X_datasets = [X_train, X_validation]\n",
    "a_labels = ['train', 'validation'] \n",
    "for X_to_visualzie,Y_to_visualize, sLabel in list(zip(a_X_datasets, a_Y_datasets, a_labels)):\n",
    "    pred = oModel.predict(X_to_visualzie)\n",
    "    ground_true = Y_to_visualize \n",
    "    for i in np.random.randint(low = 0,high = len(X_to_visualzie), size = 10):        \n",
    "        fig, axs = plt.subplots(1,2)\n",
    "        fig.suptitle('{}--{}'.format(sLabel, i))\n",
    "\n",
    "        axs[0].imshow(ground_true[i], cmap='gray')\n",
    "        axs[0].set_title( 'GROUND TRUE')\n",
    "\n",
    "        axs[1].imshow(pred[i], cmap='gray')\n",
    "        axs[1].set_title( 'PREDICTION')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "# this model should be enough capable to identify the times when the market is closed.\n",
    "# maybe we can add seasonal features in additon.\n",
    "# also, it doesn't predict well.\n",
    "# maybe a learn-to-rank model could be helpful.\n",
    "# also implement the R2s for each time_step-pixel_bin pairs.\n",
    "\n",
    "        \n",
    "#     aActual = oScalerOutput.inverse_transform(Y)\n",
    "#     aPred = oScalerOutput.inverse_transform(oModel.predict(X))\n",
    "\n",
    "#     df = pd.DataFrame(data = np.column_stack((aActual,aPred)),\n",
    "#                  columns = ['ACTUAL', 'PREDICTION']\n",
    "#                 )\n",
    "    \n",
    "#     fR2Score = round(r2_score(aActual, aPred), 1)\n",
    "#     sLabel = '{}   r2: {}'.format(sLabel,fR2Score)\n",
    "#     sns.scatterplot(data = df, x = 'ACTUAL', y = 'PREDICTION', label = sLabel)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # tf.keras.utils.plot_model(oModel,  show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7f75df-1ed7-4be5-96c4-cf4c8a5db758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
