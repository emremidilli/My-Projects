{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5ab04b1-7487-40a9-a791-e1459379234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow as tf\n",
    "\n",
    "import CONSTANTS as c\n",
    "FEATURE_ENGINEERED_DATA_PATH = c.FEATURE_ENGINEERED_DATA_PATH\n",
    "RANDOM_STATE = c.RANDOM_STATE\n",
    "\n",
    "\n",
    "i = 0\n",
    "for oFile in os.walk(FEATURE_ENGINEERED_DATA_PATH):\n",
    "    sFolderPath = oFile[0]\n",
    "    if sFolderPath != FEATURE_ENGINEERED_DATA_PATH:\n",
    "        aX = np.load(r'{}\\X.npy'.format(sFolderPath))\n",
    "        aX_time = np.load(r'{}\\X_TIME.npy'.format(sFolderPath))\n",
    "        aY = np.load(r'{}\\Y.npy'.format(sFolderPath))\n",
    "        if i == 0:\n",
    "            X = aX\n",
    "            Y= aY\n",
    "            X_TIME = aX_time\n",
    "        else:\n",
    "            X = np.concatenate([X, aX])\n",
    "            X_TIME = np.concatenate([X_TIME, aX_time])\n",
    "            Y = np.concatenate([Y, aY])\n",
    "            \n",
    "        i = i + 1\n",
    "        \n",
    "\n",
    "Y = Y[:,0,:,0]\n",
    "\n",
    "Y = Y[:, int(Y.shape[1]/2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "771b83dc-f64d-40ef-9f53-8cff48953667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "[ 91891 501592  74719  15870   4786   3430]\n",
      "Fold 1:\n",
      "[ 91891 501592  74719  15870   4786   3430]\n",
      "Fold 2:\n",
      "[ 91891 501592  74719  15870   4786   3430]\n",
      "Fold 3:\n",
      "[ 91891 501592  74719  15870   4786   3430]\n",
      "Fold 4:\n",
      "[ 91891 501592  74719  15870   4786   3430]\n",
      "Fold 5:\n",
      "[ 91891 501592  74719  15870   4786   3430]\n",
      "Fold 6:\n",
      "[ 91891 501592  74719  15870   4786   3430]\n",
      "Fold 7:\n",
      "[ 91891 501592  74719  15870   4786   3430]\n",
      "Fold 8:\n",
      "[ 91891 501592  74719  15870   4786   3430]\n",
      "Fold 9:\n",
      "[ 91891 501592  74719  15870   4786   3430]\n",
      "Fold 10:\n",
      "[ 91891 501592  74719  15870   4786   3430]\n",
      "Fold 11:\n",
      "[ 91891 501592  74719  15870   4786   3430]\n",
      "Fold 12:\n",
      "[ 91891 501592  74719  15870   4786   3430]\n",
      "Fold 13:\n",
      "[ 91891 501592  74719  15870   4786   3430]\n",
      "Fold 14:\n",
      "[ 91891 501592  74719  15870   4786   3430]\n",
      "Fold 15:\n",
      "[ 91891 501592  74719  15870   4786   3430]\n",
      "Fold 16:\n",
      "[ 91891 501592  74719  15870   4786   3430]\n",
      "Fold 17:\n",
      "[ 91891 501592  74719  15870   4787   3430]\n",
      "Fold 18:\n",
      "[ 91891 501592  74719  15870   4787   3430]\n",
      "Fold 19:\n",
      "[ 91891 501592  74719  15870   4787   3430]\n",
      "Fold 20:\n",
      "[ 91891 501592  74719  15870   4787   3430]\n",
      "Fold 21:\n",
      "[ 91891 501592  74719  15870   4787   3430]\n",
      "Fold 22:\n",
      "[ 91891 501592  74719  15870   4787   3430]\n",
      "Fold 23:\n",
      "[ 91890 501593  74719  15870   4787   3430]\n",
      "Fold 24:\n",
      "[ 91890 501593  74719  15870   4787   3430]\n",
      "Fold 25:\n",
      "[ 91890 501593  74719  15870   4787   3430]\n",
      "Fold 26:\n",
      "[ 91890 501593  74719  15870   4787   3430]\n",
      "Fold 27:\n",
      "[ 91890 501593  74719  15870   4787   3430]\n",
      "Fold 28:\n",
      "[ 91890 501593  74719  15870   4787   3430]\n",
      "Fold 29:\n",
      "[ 91890 501593  74719  15870   4787   3430]\n",
      "Fold 30:\n",
      "[ 91890 501593  74719  15870   4787   3430]\n",
      "Fold 31:\n",
      "[ 91890 501593  74719  15870   4787   3430]\n",
      "Fold 32:\n",
      "[ 91890 501593  74719  15870   4787   3430]\n",
      "Fold 33:\n",
      "[ 91890 501593  74719  15871   4786   3430]\n",
      "Fold 34:\n",
      "[ 91890 501593  74719  15871   4786   3430]\n",
      "Fold 35:\n",
      "[ 91890 501593  74719  15871   4786   3430]\n",
      "Fold 36:\n",
      "[ 91890 501593  74719  15871   4786   3430]\n",
      "Fold 37:\n",
      "[ 91890 501593  74719  15871   4786   3430]\n",
      "Fold 38:\n",
      "[ 91890 501593  74719  15871   4786   3430]\n",
      "Fold 39:\n",
      "[ 91891 501593  74719  15870   4786   3430]\n",
      "Fold 40:\n",
      "[ 91891 501593  74719  15870   4786   3430]\n",
      "Fold 41:\n",
      "[ 91891 501593  74719  15870   4786   3430]\n",
      "Fold 42:\n",
      "[ 91891 501593  74719  15870   4786   3430]\n",
      "Fold 43:\n",
      "[ 91891 501593  74719  15870   4786   3430]\n",
      "Fold 44:\n",
      "[ 91891 501592  74720  15870   4786   3430]\n",
      "Fold 45:\n",
      "[ 91891 501592  74720  15870   4786   3430]\n",
      "Fold 46:\n",
      "[ 91891 501592  74720  15870   4786   3430]\n",
      "Fold 47:\n",
      "[ 91891 501592  74720  15870   4786   3430]\n",
      "Fold 48:\n",
      "[ 91891 501592  74720  15870   4786   3430]\n",
      "Fold 49:\n",
      "[ 91891 501592  74720  15870   4786   3430]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=50, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, np.sum(Y,axis = 1))):\n",
    "    print(f\"Fold {i}:\")\n",
    "\n",
    "    print(np.unique(Y[train_index],axis = 0, return_counts=True)[1])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69671808-cdc3-4902-9760-196e8113a4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Time2Vec(Layer):\n",
    "    def __init__(self, kernel_size):\n",
    "        '''\n",
    "        :param kernel_size:         The length of time vector representation.\n",
    "        :param periodic_activation: The periodic activation, sine or cosine, or any future function.\n",
    "        '''\n",
    "        super(Time2Vec, self).__init__(\n",
    "            trainable=True,\n",
    "            name='Time2VecLayer_SIN'\n",
    "        )\n",
    "        \n",
    "        self.k = kernel_size\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # While i = 0\n",
    "        self.wb = self.add_weight(\n",
    "            shape=(1, 1),\n",
    "            initializer='uniform',\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "        self.bb = self.add_weight(\n",
    "            shape=(1, 1),\n",
    "            initializer='uniform',\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "        # Else needs to pass the periodic activation\n",
    "        self.wa = self.add_weight(\n",
    "            shape=(1, self.k),\n",
    "            initializer='uniform',\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "        self.ba = self.add_weight(\n",
    "            shape=(1, self.k),\n",
    "            initializer='uniform',\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "        super(Time2Vec, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs, **kwargs):\n",
    "        '''\n",
    "        :param inputs: A Tensor with shape (batch_size, feature_size, 1)\n",
    "        :param kwargs:\n",
    "        :return: A Tensor with shape (batch_size, feature_size, length of time vector representation + 1)\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        bias = self.wb * inputs + self.bb\n",
    "        wgts = K.sin(K.dot(inputs, self.wa) + self.ba)\n",
    "        \n",
    "        return K.concatenate([bias, wgts], -1)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1], self.k + 1)\n",
    "\n",
    "    \n",
    "\n",
    "def transformer_encoder(inputs):\n",
    "    # Normalization and Attention\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = tf.keras.layers.MultiHeadAttention(key_dim=128, num_heads=4, dropout=0.1)(x, x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = tf.keras.layers.Conv1D(filters=1, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7936d16-aa0f-4e24-a600-a33286c4566b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TermInput = tf.keras.Input(\n",
    "    shape=(X.shape[1], X.shape[2])\n",
    ")\n",
    "\n",
    "TimeInput = tf.keras.Input(\n",
    "    shape=(X_TIME.shape[1]))\n",
    "\n",
    "W = TermInput\n",
    "W = tf.keras.layers.Flatten()(W)\n",
    "\n",
    "\n",
    "W2= tf.expand_dims(TimeInput, -1)\n",
    "\n",
    "W2 = Time2Vec(2)(W2)\n",
    "W2 = tf.keras.layers.Flatten()(W2)\n",
    "\n",
    "\n",
    "W = tf.keras.layers.concatenate([W, W2], -1)\n",
    "\n",
    "# for _ in range(2):\n",
    "#     W = transformer_encoder(W)\n",
    "    \n",
    "# W = tf.keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(W)\n",
    "\n",
    "for _ in range(2):\n",
    "    W = tf.keras.layers.Dense(528)(W)\n",
    "    W = tf.keras.layers.ReLU()(W)\n",
    "    W = tf.keras.layers.Dropout(0.1)(W)\n",
    "\n",
    "\n",
    "W = tf.keras.layers.Dense(Y.shape[1], activation = 'sigmoid')(W)\n",
    "\n",
    "ModelOutput = W\n",
    "oModel = tf.keras.Model([TermInput, TimeInput], ModelOutput, name = 'TRANSFORMER_MODEL')\n",
    "\n",
    "\n",
    "oLrSchedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-2,\n",
    "    decay_steps=10**2,\n",
    "    decay_rate=0.9)\n",
    "\n",
    "\n",
    "oOptimizer = tf.keras.optimizers.Adam(learning_rate=oLrSchedule)\n",
    "oModel.compile(\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(), \n",
    "    metrics = tf.keras.metrics.AUC(),\n",
    "    optimizer=oOptimizer\n",
    ")\n",
    "\n",
    "tf.keras.utils.plot_model(oModel, show_shapes=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11feb458-3b2e-4fcd-87b3-7df3195252da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "oModel.fit(\n",
    "    (X, X_TIME), \n",
    "    Y, \n",
    "    epochs= 10**3, \n",
    "    batch_size=2**15, \n",
    "    verbose=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
