{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2af4929-b380-4dd2-a1ff-45ad80b97e59",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e75a4348-2eb5-4afb-8dfb-008f21dbe988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import CONSTANTS as c\n",
    "\n",
    "BACKWARD_WINDOW_LENGTH = c.BACKWARD_WINDOW_LENGTH\n",
    "FORWARD_WINDOW_LENGTH = c.FORWARD_WINDOW_LENGTH\n",
    "\n",
    "EXCHANGE_RATES = c.EXCHANGE_RATES\n",
    "\n",
    "FROM_TIMESTAMP = c.FROM_TIMESTAMP\n",
    "TO_TIMESTAMP = c.TO_TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252909ca-39f6-4c2f-84e6-b1f3cb9c694f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74ae4a93-469b-4fe1-a429-d32e521ef7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import MetaTrader5 as mt5\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0092d9-9dcc-404c-a8f1-47a65ff39f9b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# SOURCE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6cd4079-1e31-4db9-b780-4a62d3c638ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish connection to the MetaTrader 5 terminal\n",
    "if not mt5.initialize():\n",
    "    print(\"initialize() failed, error code =\",mt5.last_error())\n",
    "    quit()\n",
    "    \n",
    "tplSymbols = mt5.symbols_get()\n",
    "dfSymbols = pd.DataFrame(tplSymbols, columns = tplSymbols[0]._asdict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e71b9259-90cd-4c67-8c7b-6d3a137da97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get OHLC data\n",
    "dfOhlc = pd.DataFrame()\n",
    "for sExchangeRate in EXCHANGE_RATES:\n",
    "    \n",
    "    df = pd.read_csv(r'Data\\{}_M1_202010010001_202210312359.csv'.format(sExchangeRate), delimiter = '\\t')\n",
    "    df.loc[:, 'PRICE_TIME_STAMP'] = pd.to_datetime(df['<DATE>'] + df['<TIME>'], format='%Y.%m.%d%H:%M:%S')\n",
    "    df.drop(['<DATE>', '<TIME>'], axis = 1, inplace = True)\n",
    "    df.loc[:, 'EXCHANGE_RATE'] = sExchangeRate\n",
    "    df.query('@FROM_TIMESTAMP<= PRICE_TIME_STAMP <= @TO_TIMESTAMP ', inplace = True)\n",
    "    dfOhlc = pd.concat([dfOhlc, df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ab87e9-9e75-4670-b360-511f5e8a7b13",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ANALYZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd63ba72-1b6e-42cd-9b3e-5497cf7c53f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GET_DATASETS_1(dfPreprocessed, dfTimes):\n",
    "    #Create PySpark SparkSession\n",
    "    oSparkSess = SparkSession.builder \\\n",
    "        .master(\"local[1]\") \\\n",
    "        .appName(\"SparkByExamples.com\") \\\n",
    "        .config(\"spark.executor.memory\", \"70g\") \\\n",
    "        .config(\"spark.driver.memory\", \"50g\") \\\n",
    "        .config(\"spark.memory.offHeap.enabled\",True) \\\n",
    "        .config(\"spark.memory.offHeap.size\",\"16g\")   \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    oSparkSess.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "    def dfGetPriceAnalysis(df, iFrom, iTo, dfTimes):\n",
    "        \n",
    "        df = df[['PRICE_TIME_STAMP','<CLOSE>', '<HIGH>']]\n",
    "\n",
    "        sdf= oSparkSess.createDataFrame(df)\n",
    "        sdf.createOrReplaceTempView(\"sdf\")\n",
    "\n",
    "        dfTimes['FROM_TIME_STAMP'] = dfTimes['TIME_STAMP'] + pd.DateOffset(hours=iFrom)\n",
    "        dfTimes['TO_TIME_STAMP'] = dfTimes['TIME_STAMP'] + pd.DateOffset(hours=iTo)\n",
    "\n",
    "        sdfTimes= oSparkSess.createDataFrame(dfTimes)\n",
    "        sdfTimes.createOrReplaceTempView(\"sdfTimes\")\n",
    "\n",
    "        dfToReturn =  oSparkSess.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM\n",
    "            (\n",
    "                SELECT \n",
    "                t.*,\n",
    "                df.`<CLOSE>` AS CURRENT_CLOSE,\n",
    "                df2.`<HIGH>` AS HISTORY_HIGH,\n",
    "                (df2.`<HIGH>`-df.`<CLOSE>`)/(df.`<CLOSE>`) AS DIFF\n",
    "                FROM\n",
    "                (\n",
    "                    SELECT df1.*, df2.TIME_STAMP AS HISTORY_TIME_STAMP FROM sdfTimes df1\n",
    "                    INNER JOIN sdfTimes df2\n",
    "                    ON df2.TIME_STAMP >= df1.FROM_TIME_STAMP and df2.TIME_STAMP < df1.TO_TIME_STAMP\n",
    "                ) t\n",
    "                LEFT JOIN sdf df\n",
    "                ON t.TIME_STAMP = df.PRICE_TIME_STAMP\n",
    "                LEFT JOIN sdf df2\n",
    "                ON t.HISTORY_TIME_STAMP = df2.PRICE_TIME_STAMP\n",
    "            ) df\n",
    "            ORDER BY df.TIME_STAMP, df.HISTORY_TIME_STAMP\n",
    "        \"\"\").toPandas()\n",
    "        # find the unique timestamps where there is no historical or current data. and drop them from the dataset.\n",
    "        aTimeStampsToDrop = dfToReturn.query('CURRENT_CLOSE.isna() == True or HISTORY_HIGH.isna() == True')['TIME_STAMP'].unique()\n",
    "        dfToReturn.query('TIME_STAMP not in @aTimeStampsToDrop', inplace = True)\n",
    "        dfToReturn.reset_index(drop = True, inplace = True)\n",
    "\n",
    "        # drop the time stamps where there is no 60-mins data avaiable\n",
    "        aTimeStampsToDrop = dfToReturn.groupby(['TIME_STAMP']).count().reset_index().query('HISTORY_TIME_STAMP < 60')['TIME_STAMP'].unique()\n",
    "        dfToReturn.query('TIME_STAMP not in @aTimeStampsToDrop', inplace = True)\n",
    "        dfToReturn.reset_index(drop = True, inplace = True)    \n",
    "\n",
    "        dfToReturn.loc[:, 'MINUTE_DIFF'] = ((dfToReturn.loc[:, 'HISTORY_TIME_STAMP']-dfToReturn.loc[:, 'FROM_TIME_STAMP']).dt.seconds/60).astype(int)\n",
    "        return dfToReturn\n",
    "\n",
    "\n",
    "\n",
    "    dfPrep = dfPreprocessed.copy()\n",
    "\n",
    "    aExchangeRates = list(dfPrep['EXCHANGE_RATE'].unique())\n",
    "\n",
    "    dicDatasets = {\n",
    "        'INPUT':\n",
    "        {\n",
    "        },\n",
    "        'OUTPUT':\n",
    "        {   \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    \n",
    "    for sExchangeRate in aExchangeRates:\n",
    "        df_single_exc = dfPrep.query('EXCHANGE_RATE == @sExchangeRate')\n",
    "        df_single_exc = df_single_exc[['PRICE_TIME_STAMP','<CLOSE>', '<HIGH>']].fillna(0)\n",
    "\n",
    "        dic_input_single_exc = {}\n",
    "        for i in range(-BACKWARD_WINDOW_LENGTH, 0):\n",
    "            iFrom = i\n",
    "            iTo = i +1\n",
    "\n",
    "            print(iFrom)\n",
    "            dfPriceAnalysis = dfGetPriceAnalysis(df_single_exc, iFrom, iTo, dfTimes)\n",
    "\n",
    "            dic_input_single_exc[iFrom] = dfPriceAnalysis\n",
    "\n",
    "\n",
    "\n",
    "        dic_output_single_exc = {}\n",
    "        for i in range(0, FORWARD_WINDOW_LENGTH):\n",
    "            iFrom = i\n",
    "            iTo = i +1\n",
    "\n",
    "            print(iFrom)\n",
    "            dfPriceAnalysis = dfGetPriceAnalysis(df_single_exc, iFrom, iTo, dfTimes)\n",
    "\n",
    "            dic_output_single_exc[iFrom] = dfPriceAnalysis\n",
    "\n",
    "\n",
    "        dicDatasets['INPUT'][sExchangeRate] = dic_input_single_exc\n",
    "        dicDatasets['OUTPUT'][sExchangeRate] = dic_output_single_exc\n",
    "\n",
    "    \n",
    "    \n",
    "    # identify common time stamps\n",
    "    aCommonTimeStamps = list()\n",
    "    for i in dicDatasets:\n",
    "        for j in dicDatasets[i]:\n",
    "            for k in dicDatasets[i][j]:\n",
    "                df = dicDatasets[i][j][k]\n",
    "\n",
    "                if len(aCommonTimeStamps) == 0:\n",
    "                    aCommonTimeStamps = df['TIME_STAMP'].unique()\n",
    "                else:\n",
    "                    aCommonTimeStamps = np.intersect1d(aCommonTimeStamps, df['TIME_STAMP'].unique())\n",
    "\n",
    "\n",
    "    # drop the datafrom datasets that don't have common time stamps\n",
    "    for i in dicDatasets:\n",
    "        for j in dicDatasets[i]:\n",
    "            for k in dicDatasets[i][j]:\n",
    "                df = dicDatasets[i][j][k]\n",
    "                dicDatasets[i][j][k] = df.query('TIME_STAMP in @aCommonTimeStamps').reset_index(drop = True)\n",
    "\n",
    "\n",
    "\n",
    "    def dfCompileDic(p_dic, tplFormat,sKey):\n",
    "        aToReturn =  np.zeros(tplFormat)\n",
    "        dic = p_dic[sKey]\n",
    "        ixExcRate = 0\n",
    "        for i in dic:\n",
    "            ixTimeStep = 0\n",
    "            for j in dic[i]:\n",
    "                df = dic[i][j]\n",
    "                df = pd.pivot_table(\n",
    "                    data = df[['DIFF','MINUTE_DIFF', 'TIME_STAMP']], \n",
    "                    values='DIFF', index='TIME_STAMP',\n",
    "                    columns='MINUTE_DIFF', \n",
    "                    aggfunc=np.sum\n",
    "                )\n",
    "                df.sort_index(ascending=True, inplace=True)\n",
    "\n",
    "                aToReturn[:, ixTimeStep,:,ixExcRate] = df.values\n",
    "\n",
    "                ixTimeStep = ixTimeStep + 1\n",
    "\n",
    "            ixExcRate = ixExcRate  + 1\n",
    "\n",
    "\n",
    "        return aToReturn\n",
    "\n",
    "    def aGetTimeFeatures(aTimeStamps):\n",
    "        df =pd.DataFrame(data = aTimeStamps, columns = ['TIME_STAMP'])\n",
    "        df.sort_values(by = 'TIME_STAMP' , ascending=True, inplace=True)\n",
    "        df.loc[:, 'MINUTE'] = pd.to_datetime(df.loc[:, 'TIME_STAMP'],unit='s').dt.minute\n",
    "        df.loc[:, 'HOUR'] = pd.to_datetime(df.loc[:, 'TIME_STAMP'],unit='s').dt.hour\n",
    "        df.loc[:, 'DAY_OF_WEEK'] = pd.to_datetime(df.loc[:, 'TIME_STAMP'],unit='s').dt.day_of_week\n",
    "        df.loc[:, 'DAY_OF_MONTH'] = pd.to_datetime(df.loc[:, 'TIME_STAMP'],unit='s').dt.day\n",
    "        df.loc[:, 'MONTH'] = pd.to_datetime(df.loc[:, 'TIME_STAMP'],unit='s').dt.month\n",
    "        df.loc[:, 'YEAR'] = pd.to_datetime(df.loc[:, 'TIME_STAMP'],unit='s').dt.year\n",
    "        df.drop('TIME_STAMP', axis = 1, inplace = True)\n",
    "        aToReturn = df.values\n",
    "        return aToReturn\n",
    "    \n",
    "    \n",
    "\n",
    "    X = dfCompileDic(dicDatasets, (len(aCommonTimeStamps), BACKWARD_WINDOW_LENGTH , 60 , len(aExchangeRates)), 'INPUT') # sample_size, time_steps, features, channel_size\n",
    "    Y = dfCompileDic(dicDatasets, (len(aCommonTimeStamps), FORWARD_WINDOW_LENGTH , 60 , len(aExchangeRates)), 'OUTPUT') # sample_size, time_steps, features, channel_size\n",
    "    X_TIME= aGetTimeFeatures(aCommonTimeStamps) #sample size, features\n",
    "    \n",
    "    return X,Y,X_TIME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff7ea24-3084-477e-a0cc-474945a46cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4\n"
     ]
    }
   ],
   "source": [
    "dfPrep = dfOhlc.copy()\n",
    "dfTimeStamps = pd.DataFrame(\n",
    "    data = pd.date_range(\n",
    "        start=FROM_TIMESTAMP, \n",
    "        end=TO_TIMESTAMP, freq = 'min'\n",
    "    ),\n",
    "    columns  = ['TIME_STAMP']\n",
    ")\n",
    "dfTimeStamps.query('TIME_STAMP.dt.day_of_week not in (6,7)', inplace = True)\n",
    "dfTimeStamps.reset_index(drop = True, inplace = True)\n",
    "\n",
    "X_ORIGINAL, Y_ORIGINAL, X_TIME_ORIGINAL = GET_DATASETS_1(dfPrep, dfTimeStamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969c83dc-fb19-40f7-a587-926c38b11504",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe73cab-c483-4450-8dcb-7c9ce3e3b703",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(r'Temp\\X_ORIGINAL.npy', X_ORIGINAL)\n",
    "np.save(r'Temp\\Y_ORIGINAL.npy', Y_ORIGINAL)\n",
    "np.save(r'Temp\\X_TIME_ORIGINAL.npy', X_TIME_ORIGINAL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
