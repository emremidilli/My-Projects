\documentclass{article}
\usepackage[linesnumbered, ruled, vlined]{algorithm2e}
\usepackage{algpseudocode}
\usepackage[a4paper, total={6in, 8in}]{geometry}


\begin{document}

    \pagenumbering{gobble}
    \setlength{\interspacetitleruled}{-.4pt}
    \begin{algorithm}

    
    \textbf{Input:}
    $\epsilon_{0}$, initial learning rate
    
    \textbf{Input:}
    $\alpha$, decay rate of learning rate
    
    \textbf{Input:}
    $\beta$, momentum rate
    
    \textbf{Input:}
    $\rho$, discount factor for historical gradient

    \textbf{Input:}
    $\zeta$, small constant to avoid zero division
    
    \textbf{Input:}
    $m$, minibatch size
    
	\textbf{Input:}
    $k$, epoch size
    
    \textbf{Input:}
    $\theta$, initial weights
    
    \textbf{Input:}
    $v$, initial velocity
     
    \textbf{Input:}
    $\mathbf{X}$, training dataset inputs
    
    \textbf{Input:}
    $\mathbf{y}$, training dataset targets
    
    \textbf{Initialize:}
    $r \gets 0$, accumulation of historical gradient
    
    \textbf{Initialize:}
	$j \gets 1$, current epoch
	
	\While{$j \leq k$ }{
			
		update learning rate  $\epsilon_{j} \gets \epsilon_{0} + \alpha(\epsilon_{j-1}- \epsilon_{0})$		
		
    		\SetKwRepeat{Do}{do}{while}
		\While{stopping criteria is not satisfied}{

			$\{\mathbf{x}^{1}...\mathbf{x}^{m}\}$, $\{\mathbf{y}^{1}...\mathbf{y}^{m}\} \gets $  get a sample from $\mathbf{X}$ and $\mathbf{y}$ randomly

			calculate estimation of gradient $\hat{g} \gets \frac{1}{m}\sum_{i = 1}^{m}{L(f(\mathbf{x}^{i}; \theta ), \mathbf{y}^{i})}$		
			
			accumulate historical graidents $r \gets \rho r + (1- \rho) \hat{g} \odot \hat{g} $
			
			calculate step size $v \gets \beta v - \frac{\epsilon_{j}}{\sqrt{\zeta+r}} \odot \hat{g}$

			update weights $\theta \gets \theta + v $

		}
				
		$j \gets j + 1$ go to next epoch 
	
	}        


    \end{algorithm}    
    
\end{document}



