{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5f1beb-a52d-4fca-af8c-9ce292df0dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BERT import BERT\n",
    "from BERT import mlm_custom_loss, nsp_custom_loss\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "DISERT_DATA_PATH = r'C:\\Users\\yunus\\Desktop\\DisERT Data'\n",
    "CHECKPOINT_PATH = r'C:\\Users\\yunus\\Desktop\\Checkpoints\\DisERT'\n",
    "\n",
    "\n",
    "X_MLM = np.load(f'{DISERT_DATA_PATH}\\X_MLM.npy')\n",
    "Y_MLM = np.load(f'{DISERT_DATA_PATH}\\Y_MLM.npy')\n",
    "X_NSP = np.load(f'{DISERT_DATA_PATH}\\X_NSP.npy')\n",
    "Y_NSP = np.load(f'{DISERT_DATA_PATH}\\Y_NSP.npy')\n",
    "\n",
    "X_MLM = X_MLM[:4000]\n",
    "X_NSP = X_NSP[:4000]\n",
    "Y_MLM = Y_MLM[:4000]\n",
    "Y_NSP = Y_NSP[:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db806e8f-e3d5-4283-9de3-57d1ccab129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sCheckPointFilePath = f'{CHECKPOINT_PATH}\\model'\n",
    "sCsvLogFilePath = f'{CHECKPOINT_PATH}\\log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3763c9e7-8672-402d-960e-e75f478f7d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "class CsvLogger(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, sFilePath, sDelimeter = ';'):\n",
    "        sFilePath = f'{sFilePath}.txt'\n",
    "        self.sFilePath = sFilePath\n",
    "        self.sDelimeter = sDelimeter\n",
    "\n",
    "\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "        \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        endTime = time.time()\n",
    "        \n",
    "        logs['start'] = self.epoch_time_start\n",
    "        logs['end'] = endTime\n",
    "        logs['duration'] = endTime - self.epoch_time_start\n",
    "        \n",
    "        logs['epoch'] = epoch\n",
    "        logs['learning_rate'] = self.model.optimizer.lr.numpy()\n",
    "        \n",
    "        if os.path.exists(self.sFilePath) == False:\n",
    "            with open(self.sFilePath, 'a') as f: \n",
    "                f.write(self.sDelimeter.join([str(i) for i in logs.keys()]))\n",
    "                f.write('\\n') \n",
    "                \n",
    "        with open(self.sFilePath, 'a') as f: \n",
    "            f.write(self.sDelimeter.join([str(i) for i in logs.values()]))\n",
    "            f.write('\\n')\n",
    "            \n",
    "    \n",
    "class ThresholdStopper(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, threshold):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        if logs.get('loss') <= self.threshold:\n",
    "             self.model.stop_training = True\n",
    "                \n",
    "                \n",
    "                \n",
    "oCsvLogger=  CsvLogger(sCsvLogFilePath)\n",
    "\n",
    "oDisERTCheckPoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=sCheckPointFilePath,\n",
    "    save_weights_only=False,\n",
    "    monitor='loss',\n",
    "    mode='min',\n",
    "    save_best_only=True,\n",
    "    save_traces = True\n",
    ")\n",
    "\n",
    "oLearningRateReducer = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='loss', \n",
    "    factor=0.80,\n",
    "    patience=3, \n",
    "    min_lr=1e-4\n",
    ")\n",
    "\n",
    "oEarlyStopper = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=20)\n",
    "\n",
    "oThresholdStopper = ThresholdStopper(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2718691-0b12-41ca-b1a3-882041a38687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oDisERT = BERT(\n",
    "#     mlm_input_shape = (10, 129), \n",
    "#     nsp_input_shape = (1, 129),\n",
    "#     nr_of_encoder_blocks = 4,\n",
    "#     attention_key_dims = 32,\n",
    "#     attention_nr_of_heads = 2,\n",
    "#     attention_dense_dims = 128,\n",
    "#     dropout_rate = 0.0\n",
    "# )\n",
    "\n",
    "# oDisERT.compile(\n",
    "#     optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "#     loss = [mlm_custom_loss, nsp_custom_loss]\n",
    "# )\n",
    "\n",
    "# oDisERT.fit(\n",
    "#     x = [X_MLM, X_NSP], \n",
    "#     y = [Y_MLM, Y_NSP], \n",
    "#     batch_size= 512,\n",
    "#     epochs=2,\n",
    "#     verbose=1,\n",
    "#     # callbacks = [oDisERTCheckPoint, oCsvLogger, oLearningRateReducer, oEarlyStopper, oThresholdStopper]\n",
    "# )\n",
    "\n",
    "# oDisERT.save(sCheckPointFilePath)\n",
    "\n",
    "# # oDisERT.summary()\n",
    "\n",
    "# # tf.keras.utils.plot_model(oDisERT, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70b5862-a089-432e-87a5-e94ae8467b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "oDisERT = tf.keras.models.load_model(\n",
    "    sCheckPointFilePath, \n",
    "    custom_objects={\n",
    "        'mlm_custom_loss':mlm_custom_loss,\n",
    "        'nsp_custom_loss':nsp_custom_loss\n",
    "    }\n",
    ")\n",
    "\n",
    "oDisERT.fit(\n",
    "    x = [X_MLM, X_NSP], \n",
    "    y = [Y_MLM, Y_NSP], \n",
    "    batch_size= 512,\n",
    "    epochs=2,\n",
    "    verbose=1,\n",
    "    # callbacks = [oDisERTCheckPoint, oCsvLogger, oLearningRateReducer, oEarlyStopper, oThresholdStopper]\n",
    ")\n",
    "\n",
    "oDisERT.save(sCheckPointFilePath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
