{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4500d9c9",
   "metadata": {},
   "source": [
    "# IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1466,
   "id": "d0c93419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531cfad8",
   "metadata": {},
   "source": [
    "# CONFIGURATION OF LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1500,
   "id": "ae7a207a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1008x576 with 0 Axes>"
      ]
     },
     "execution_count": 1500,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1008x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "plt.figure(figsize=(14,8))\n",
    "\n",
    "pd.options.display.max_rows = 100\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be01d6d",
   "metadata": {},
   "source": [
    "# DATA READING DATA FROM SOURCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1109af6",
   "metadata": {},
   "source": [
    "dfPolicyData = pd.read_csv(\"PolicyData.csv\", delimiter = \";\", encoding='latin-1')\n",
    "dfPolicyData.set_index(\"policy_guid\", inplace = True)\n",
    "\n",
    "dfInvoiceData = pd.read_csv(\"InvoiceData.csv\", delimiter = \";\")\n",
    "dfInvoiceData.set_index(\"invoice_guid\", inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1469,
   "id": "bb47c651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# They are converted from object to float\n",
    "dfInvoiceData[\"amount_premium\"] = dfInvoiceData[\"amount_premium\"].apply(lambda x: x.replace(',','.'), ).astype(float, errors = 'raise')\n",
    "dfPolicyData[\"Premium\"] = dfPolicyData[\"Premium\"].apply(lambda x: x.replace(',','.'), ).astype(float, errors = 'raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1470,
   "id": "2b8568fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Invoice is paid late\n",
    "# 0: Invoice is not paid late\n",
    "aLabels = [1, 0]\n",
    "aConditions = [\n",
    "    (dfInvoiceData[\"due_date\"] < dfInvoiceData[\"paid_date\"]),\n",
    "    (dfInvoiceData[\"due_date\"] >= dfInvoiceData[\"paid_date\"])\n",
    "]\n",
    "\n",
    "dfInvoiceData[\"is_it_paid_late\"] = np.select(aConditions, aLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1471,
   "id": "7d5c8be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the dataset not all policies are ended. It s a snapshot probably around June 2018.\n",
    "# There could be 3 status of policies: \"Ended on time\", \"Terminated\" and \"Ongoing\"\n",
    "# We can build model not based on policy status, but based on number of issued invoices.\n",
    "\n",
    "oInvocieGroupByPolicy = dfInvoiceData[[\"policy_guid\", \"is_it_paid_late\"]].groupby([\"policy_guid\"])\n",
    "\n",
    "dfInvoiceIssueStatistics = oInvocieGroupByPolicy.agg([\"count\", \"sum\"])\n",
    "\n",
    "dfInvoiceIssueStatistics = dfInvoiceIssueStatistics[\"is_it_paid_late\"] \n",
    "\n",
    "dfInvoiceIssueStatistics.columns = [\"number_of_invoices\", \"number_of_late_payments\"]\n",
    "\n",
    "dfInvoiceIssueStatistics[\"late_payment_ratio\"] = dfInvoiceIssueStatistics[\"number_of_late_payments\"]/dfInvoiceIssueStatistics[\"number_of_invoices\"]\n",
    "\n",
    "dfModelData = dfPolicyData.join(dfInvoiceIssueStatistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1472,
   "id": "19ece68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Policy is paid late at least once\n",
    "# 0: Policy is never paid late\n",
    "aLabels = [1, 0]\n",
    "aConditions = [\n",
    "    (dfModelData[\"number_of_late_payments\"] >= 1),\n",
    "    (dfModelData[\"number_of_late_payments\"] == 0)\n",
    "]\n",
    "\n",
    "dfModelData[\"is_it_paid_late\"] = np.select(aConditions, aLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798f64c8",
   "metadata": {},
   "source": [
    "# MISSING DATA INPUTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1474,
   "id": "7bbfda1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Deductible_general', 'ClientBirthday', 'BMClassMOD', 'avgFuelConsumption']"
      ]
     },
     "execution_count": 1474,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Region', 'FuelType', 'DriveTrain']"
      ]
     },
     "execution_count": 1474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns that are NaN or 'Missing' in policy dataset\n",
    "\n",
    "dfPolicyData.columns[dfPolicyData.isna().any()].tolist()\n",
    "\n",
    "dfMissing = dfPolicyData.astype(str) == \"Missing\"\n",
    "dfPolicyData.columns[dfMissing.any()].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1475,
   "id": "b262593d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1475,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns that are NaN or 'Missing' in invoice dataset.\n",
    "# There is no missing data invoice dataset. \n",
    "\n",
    "dfInvoiceData.columns[dfInvoiceData.isna().any()].tolist()\n",
    "\n",
    "dfMissing = dfInvoiceData.astype(str) == \"Missing\"\n",
    "dfInvoiceData.columns[dfMissing.any()].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1476,
   "id": "3b38bbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are policies where number of invoices are greater than number of issued invoices.\n",
    "dfToQuestion = dfModelData[dfModelData[\"number_of_invoices\"] > dfModelData[\"Nb_of_payments\"]].loc[:, [\"number_of_invoices\", \"Nb_of_payments\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca084c0",
   "metadata": {},
   "source": [
    "## Deductible_general"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcd28ad",
   "metadata": {},
   "source": [
    "There are only 2 rows where Deductible_general is missing. \n",
    "\n",
    "Since they are relatively small amount of rows for this dataset, these rows are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1477,
   "id": "428abfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMissingDecutibleGeneral = dfModelData[dfModelData[\"Deductible_general\"].isna()]\n",
    "dfModelData.drop(dfMissingDecutibleGeneral.index, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9238a45b",
   "metadata": {},
   "source": [
    "## ClientBirthday"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2154799",
   "metadata": {},
   "source": [
    "There are only 43 rows where ClientBirthday is missing.\n",
    "\n",
    "Since they are relatively small amount of rows for this dataset, these rows are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1478,
   "id": "1f817651",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMissingClientBirthday = dfModelData[dfModelData[\"ClientBirthday\"].isna()]\n",
    "dfModelData.drop(dfMissingClientBirthday.index, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b85eb7a",
   "metadata": {},
   "source": [
    "BMClassMOD:\n",
    "\n",
    "There are only 14 rows where BMClassMOD is missing.\n",
    "\n",
    "Since they are relatively small amount of rows for this dataset, these rows are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1479,
   "id": "1ed45792",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMissingBmClassMod = dfModelData[dfModelData[\"BMClassMOD\"].isna()]\n",
    "dfModelData.drop(dfMissingBmClassMod.index, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f654393",
   "metadata": {},
   "source": [
    "avgFuelConsumption:\n",
    "\n",
    "There are 27927 rows where avgFuelConsumption is missing.\n",
    "\n",
    "Since it s a big amount of rows, correlation between avgFuelConsumption and other fields are calculated for non-missing data.\n",
    "\n",
    "It is found out that avgFuelConsumption has fair linear correlation with the attributes of Power, Weight,  VehicleFirstRegistrationYear and Premium.\n",
    "\n",
    "Missing data is filled based on random forest classifier model since avgFuelConsumption field contains cardinal-categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1480,
   "id": "b062c91c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dfMissingAvgFuelConsumption = dfModelData[dfModelData[\"avgFuelConsumption\"].isna()]\n",
    "# dfNonMissingAvgFuelConsumption = dfModelData.drop(dfMissingAvgFuelConsumption.index, inplace = False)\n",
    "\n",
    "# aUniqueAvgFuelConsumption = dfNonMissingAvgFuelConsumption[\"avgFuelConsumption\"].unique()\n",
    "\n",
    "\n",
    "# dfAvgFuelConsumptionClassified = pd.get_dummies(dfNonMissingAvgFuelConsumption[\"avgFuelConsumption\"])\n",
    "# dfNonMissingAvgFuelConsumption[aUniqueAvgFuelConsumption] = dfAvgFuelConsumptionClassified\n",
    "\n",
    "# dfCorr = dfNonMissingAvgFuelConsumption[[\"avgFuelConsumption\", \"Power\", \"Weight\", \"VehicleFirstRegistrationYear\", \"Premium\"]].corr()\n",
    "\n",
    "# sns.heatmap(dfCorr.abs(), vmin=0, vmax=1, annot = True, cmap=\"Greens\")\n",
    "\n",
    "# dfX = dfNonMissingAvgFuelConsumption[[\"Power\", \"Weight\", \"VehicleFirstRegistrationYear\", \"Premium\"]]\n",
    "# dfY = dfNonMissingAvgFuelConsumption[aUniqueAvgFuelConsumption]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(dfX, dfY, test_size=0.3, random_state=1)\n",
    "\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# oRandForModel = RandomForestClassifier()\n",
    "# oRandForModel.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = oRandForModel.predict(X_test)\n",
    "\n",
    "# print('RMSE: ', metrics.mean_squared_error(y_test, y_pred, squared = False))\n",
    "\n",
    "# print('R2: ', metrics.r2_score(y_test, y_pred))\n",
    "\n",
    "# print('Accuracy: ', metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# print('Recall: ', metrics.recall_score(y_test, y_pred,zero_division=0,  average = 'micro'))\n",
    "\n",
    "# print('Precision: ', metrics.precision_score(y_test, y_pred,zero_division=0,  average = 'micro'))\n",
    "\n",
    "# print('F1-Score: ', metrics.f1_score(y_test, y_pred,zero_division=0,  average = 'micro'))\n",
    "\n",
    "# aPredictedClassesForMissing = oRandForModel.predict(dfMissingAvgFuelConsumption[[\"Power\", \"Weight\", \"VehicleFirstRegistrationYear\", \"Premium\"]])\n",
    "\n",
    "# dfPredictedClassesForMissing = pd.DataFrame(data = aPredictedClassesForMissing ,  columns = aUniqueAvgFuelConsumption, index = dfMissingAvgFuelConsumption.index )\n",
    "\n",
    "# sPredictedLabels = dfPredictedClassesForMissing.idxmax(axis=1)\n",
    "\n",
    "# dfModelData[\"avgFuelConsumption\"].fillna(sPredictedLabels, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1481,
   "id": "c6ca4ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfModelData[\"FuelType\"] = dfModelData[\"FuelType\"].astype(str).apply(\n",
    "#     lambda x: x.replace('Missing','Missing_FuelType'), )\n",
    "\n",
    "# aUniqueFuelTypes = dfModelData[\"FuelType\"].unique()\n",
    "\n",
    "# dfFuelTypesClassified = pd.get_dummies(dfModelData[\"FuelType\"])\n",
    "# dfModelData = dfModelData.join(dfFuelTypesClassified, on= \"policy_guid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1482,
   "id": "94fa6a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfModelData[\"DriveTrain\"] = dfModelData[\"DriveTrain\"].astype(str).apply(\n",
    "#     lambda x: x.replace('Missing','Missing_DriveTrain'), )\n",
    "\n",
    "# aUniqueDriveTrains = dfModelData[\"DriveTrain\"].unique()\n",
    "\n",
    "# dfDriveTrainsClassified = pd.get_dummies(dfModelData[\"DriveTrain\"])\n",
    "# dfModelData = dfModelData.join(dfDriveTrainsClassified, on= \"policy_guid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16ad3c0",
   "metadata": {},
   "source": [
    "Region: \n",
    "\n",
    "This field is converted to latitude and longitude form.\n",
    "\n",
    "Exploratory analysis is performed to identify relationships.\n",
    "\n",
    "It is observed that there is no strong linear relationship between coordinates and other attributes of policy dataset.\n",
    "\n",
    "Since there is no relationship identified for non-missing data. And since there are 6727 rows (aprx. 9% of whole dataset), rows that are missing have been removed from dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1483,
   "id": "199cb3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from geopy.geocoders import Nominatim\n",
    "# import time\n",
    "# from pprint import pprint\n",
    "\n",
    "# # instantiate a new Nominatim client\n",
    "# oGeolocator = Nominatim(user_agent=\"tutorial\")\n",
    "\n",
    "# dfRegionsWithCoordinates = pd.DataFrame(index = aUniqueRegions, columns = [\"latitude\", \"longitude\"])\n",
    "\n",
    "# for i in range(len(aUniqueRegions)):\n",
    "#     sRegion = aUniqueRegions[i]\n",
    "    \n",
    "#     if sRegion != \"Missing_Region\":\n",
    "#         oLocation = oGeolocator.geocode(sRegion)\n",
    "#         fLatitue = oLocation.latitude\n",
    "#         fLongitude = oLocation.longitude\n",
    "#     else:\n",
    "#         fLatitue = 0\n",
    "#         fLongitude = 0\n",
    "\n",
    "#     dfRegionsWithCoordinates.loc[sRegion, \"latitude\"] = fLatitue\n",
    "#     dfRegionsWithCoordinates.loc[sRegion, \"longitude\"] = fLongitude\n",
    "\n",
    "# dfRegionsWithCoordinates.index.name = \"Region\"\n",
    "# dfRegionsWithCoordinates.reset_index(level=0, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1484,
   "id": "d728ee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfModelData = pd.merge(dfModelData,dfRegionsWithCoordinates,on='Region')\n",
    "\n",
    "# dfModelData[[\"latitude\", \"longitude\"]] = dfModelData[[\"latitude\", \"longitude\"]].astype(float, errors = 'raise')\n",
    "\n",
    "# dfModelNonMissingRegions = dfModelData[dfModelData[\"Region\"] != \"Missing_Region\"]\n",
    "\n",
    "# aColumnsToAnalyze = np.concatenate((dfPolicyData.columns, [\"latitude\", \"longitude\"]))\n",
    "\n",
    "# dfCorr = dfModelNonMissingRegions[aColumnsToAnalyze].corr()\n",
    "\n",
    "# sns.heatmap(dfCorr.abs(), vmin=0, vmax=1, cmap=\"Greens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1485,
   "id": "7eff0f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfModelNonMissingRegions.shape\n",
    "# dfModelData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1486,
   "id": "9bc54a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPolicyDataWithMissingRegion = dfModelData[dfModelData[\"Region\"] == \"Missing\"]\n",
    "dfModelData.drop(dfPolicyDataWithMissingRegion.index, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfe6eab",
   "metadata": {},
   "source": [
    "Fuel Type:\n",
    "\n",
    "For missing fuel types, we can use policy dataset as a \"vehicle\" dataset where we can build a classification model to identify fuel type. Logically, fuel type is related with vehicle attributes related features such as 'VehicleType', 'VehicleUsage', 'Power', 'Weight','VehicleFirstRegistrationYear', 'Mark', 'Model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1522,
   "id": "4444e717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReplaceMissingDataWithClassifier(dfModelData, aCategoricalFeatures, aContinuousFeatures, sTargetFeature):\n",
    "\n",
    "    dfModelDataCopy = dfModelData.copy()\n",
    "    \n",
    "    aFeaturesX = []\n",
    "    \n",
    "    for i in range(len(aCategoricalFeatures)):\n",
    "        sCategoricalFeature = aCategoricalFeatures[i]\n",
    "\n",
    "        # to avoid \"other\" value for multiple attributes\n",
    "        dfTemp = dfModelDataCopy[dfModelDataCopy[sCategoricalFeature] == \"OTHER\"]\n",
    "        dfTemp[sCategoricalFeature] = \"Other_\" + str(sCategoricalFeature)\n",
    "        \n",
    "        dfModelDataCopy[sCategoricalFeature] = dfTemp[sCategoricalFeature]\n",
    "        \n",
    "        dfCategoricalFeatureClassified = pd.get_dummies(dfModelDataCopy[sCategoricalFeature])\n",
    "        \n",
    "        dfModelDataCopy = dfModelDataCopy.join(dfCategoricalFeatureClassified, on= \"policy_guid\")\n",
    "        \n",
    "        aFeaturesX = np.concatenate([aFeaturesX, dfCategoricalFeatureClassified.columns])\n",
    " \n",
    "        \n",
    "    aFeaturesX = np.concatenate([aFeaturesX, aContinuousFeatures])\n",
    "\n",
    "    dfTargetFeatureClassified= pd.get_dummies(dfModelDataCopy[sTargetFeature])\n",
    "    \n",
    "    dfModelDataCopy = dfModelDataCopy.join(dfTargetFeatureClassified, on= \"policy_guid\")\n",
    "\n",
    "    aFeaturesY = np.delete(dfTargetFeatureClassified.columns, np.where(dfTargetFeatureClassified.columns == \"Missing\") )\n",
    "\n",
    "    dfMissingData = dfModelDataCopy[dfModelData[sTargetFeature] == \"Missing\"]\n",
    "    dfNonMissingData = dfModelDataCopy.drop(dfMissingData.index, inplace = False)\n",
    "    \n",
    "    dfX = dfNonMissingData[aFeaturesX]\n",
    "    dfY = dfNonMissingData[aFeaturesY]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dfX, dfY, test_size=0.3, random_state=1)\n",
    "\n",
    "    oDecTreeModel =  DecisionTreeClassifier()\n",
    "    oDecTreeModel.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = oDecTreeModel.predict(X_test)\n",
    "\n",
    "    print('Accuracy: ', metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    print('Recall: ', metrics.recall_score(y_test, y_pred,zero_division=0,  average = 'micro'))\n",
    "\n",
    "    print('Precision: ', metrics.precision_score(y_test, y_pred,zero_division=0,  average = 'micro'))\n",
    "\n",
    "    print('F1-Score: ', metrics.f1_score(y_test, y_pred,zero_division=0,  average = 'micro'))\n",
    "    \n",
    "    dfX_Missing = dfMissingData[aFeaturesX]\n",
    "    aPredictionsForMissing = oDecTreeModel.predict(dfX_Missing)\n",
    "    dfPredictionsForMissing = pd.DataFrame(data = aPredictionsForMissing, columns = aFeaturesY, index = dfMissingData.index)\n",
    "    \n",
    "    aPredictedLabels = dfPredictionsForMissing.idxmax(axis=1)\n",
    "\n",
    "    dfModelData.loc[dfMissingData.index,sTargetFeature] = aPredictedLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1514,
   "id": "01b46f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9784807281717047\n",
      "Recall:  0.9784807281717047\n",
      "Precision:  0.9804639117216529\n",
      "F1-Score:  0.9794713160854893\n"
     ]
    }
   ],
   "source": [
    "ReplaceMissingDataWithClassifier(dfModelData, \n",
    "                                ['VehicleType', 'VehicleUsage', 'Mark', 'Model'], \n",
    "                                ['Power', 'Weight','VehicleFirstRegistrationYear'], \n",
    "                                'FuelType')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b573c719",
   "metadata": {},
   "source": [
    "DriveTrain:\n",
    "\n",
    "There are 22558 rows that have missing information for this field. Since it s a around 36% of whole data and 'DriveTrain' depends on vehicle attributes, we can replace missing data with a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1523,
   "id": "c924f7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9669183136822461\n",
      "Recall:  0.9669183136822461\n",
      "Precision:  0.9727769243898277\n",
      "F1-Score:  0.9698387714297869\n"
     ]
    }
   ],
   "source": [
    "ReplaceMissingDataWithClassifier(dfModelData, \n",
    "                                ['VehicleType', 'VehicleUsage', 'Mark', 'Model'], \n",
    "                                ['Power', 'Weight','VehicleFirstRegistrationYear'], \n",
    "                                'DriveTrain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190020f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
