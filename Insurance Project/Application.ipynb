{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ba4e342",
   "metadata": {},
   "source": [
    "# IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c93419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from geopy.geocoders import Nominatim\n",
    "import time\n",
    "from pprint import pprint\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b2cd46",
   "metadata": {},
   "source": [
    "# CONFIGURATION OF LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7a207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba77547f",
   "metadata": {},
   "source": [
    "# READING DATA FROM SOURCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d856c7",
   "metadata": {},
   "source": [
    "Both datasets are stored in the same direcotry with script as csv files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8e903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPolicyData = pd.read_csv(\"PolicyData.csv\", delimiter = \";\", encoding='latin-1')\n",
    "dfPolicyData.set_index(\"policy_guid\", inplace = True)\n",
    "\n",
    "dfInvoiceData = pd.read_csv(\"InvoiceData.csv\", delimiter = \";\")\n",
    "dfInvoiceData.set_index(\"invoice_guid\", inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5053aefd",
   "metadata": {},
   "source": [
    "Premium and amount premium fields are not numberic. They are converted from object to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb93b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInvoiceData[\"amount_premium\"] = dfInvoiceData[\"amount_premium\"].apply(lambda x: x.replace(',','.'), ).astype(float, errors = 'raise')\n",
    "dfPolicyData[\"Premium\"] = dfPolicyData[\"Premium\"].apply(lambda x: x.replace(',','.'), ).astype(float, errors = 'raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c72824",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPolicyData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0128a044",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInvoiceData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8f7a6e",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4f55fe",
   "metadata": {},
   "source": [
    "dfModelData that represents all the possible attributes for input and target features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab8f402",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfModelData = dfPolicyData.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d9ef36",
   "metadata": {},
   "source": [
    "# MISSING DATA INPUTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f8f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReplaceMissingDataWithClassifier(dfModelData, aCategoricalFeatures, aContinuousFeatures, sTargetFeature):\n",
    "\n",
    "    dfModelDataCopy = dfModelData.copy()\n",
    "    \n",
    "    aFeaturesX = []\n",
    "    \n",
    "    for i in range(len(aCategoricalFeatures)):\n",
    "        sCategoricalFeature = aCategoricalFeatures[i]\n",
    "\n",
    "        # to avoid \"other\" value for multiple attributes\n",
    "        dfMaskOther = dfModelDataCopy[sCategoricalFeature] == \"OTHER\"\n",
    "        \n",
    "        dfModelDataCopy.loc[dfMaskOther, sCategoricalFeature] = \"OTHER_\" + str(sCategoricalFeature)\n",
    "        \n",
    "        dfCategoricalFeatureClassified = pd.get_dummies(dfModelDataCopy[sCategoricalFeature])\n",
    "        \n",
    "        dfModelDataCopy = dfModelDataCopy.join(dfCategoricalFeatureClassified, on= \"policy_guid\")\n",
    "        \n",
    "        aFeaturesX = np.concatenate([aFeaturesX, dfCategoricalFeatureClassified.columns])\n",
    " \n",
    "        \n",
    "    aFeaturesX = np.concatenate([aFeaturesX, aContinuousFeatures])\n",
    "\n",
    "    dfTargetFeatureClassified= pd.get_dummies(dfModelDataCopy[sTargetFeature])\n",
    "    \n",
    "    dfModelDataCopy = dfModelDataCopy.join(dfTargetFeatureClassified, on= \"policy_guid\")\n",
    "\n",
    "    aFeaturesY = np.delete(dfTargetFeatureClassified.columns, np.where(dfTargetFeatureClassified.columns == \"Missing\") )\n",
    "\n",
    "    dfMissingData = dfModelDataCopy[dfModelData[sTargetFeature] == \"Missing\"]\n",
    "    dfNonMissingData = dfModelDataCopy.drop(dfMissingData.index, inplace = False)\n",
    "    \n",
    "    dfX = dfNonMissingData[aFeaturesX]\n",
    "    dfY = dfNonMissingData[aFeaturesY]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dfX, dfY, test_size=0.3, random_state=1)\n",
    "\n",
    "    oDecTreeModel =  DecisionTreeClassifier()\n",
    "    oDecTreeModel.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = oDecTreeModel.predict(X_test)\n",
    "\n",
    "    print('Accuracy: ', metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    print('Recall: ', metrics.recall_score(y_test, y_pred,zero_division=0,  average = 'micro'))\n",
    "\n",
    "    print('Precision: ', metrics.precision_score(y_test, y_pred,zero_division=0,  average = 'micro'))\n",
    "\n",
    "    print('F1-Score: ', metrics.f1_score(y_test, y_pred,zero_division=0,  average = 'micro'))\n",
    "    \n",
    "    dfX_Missing = dfMissingData[aFeaturesX]\n",
    "    aPredictionsForMissing = oDecTreeModel.predict(dfX_Missing)\n",
    "    dfPredictionsForMissing = pd.DataFrame(data = aPredictionsForMissing, columns = aFeaturesY, index = dfMissingData.index)\n",
    "    \n",
    "    aPredictedLabels = dfPredictionsForMissing.idxmax(axis=1)\n",
    "    \n",
    "    dfModelData.loc[dfMissingData.index,sTargetFeature] = aPredictedLabels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66ded35",
   "metadata": {},
   "source": [
    "Columns that are NaN in policy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31bb1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPolicyData.columns[dfPolicyData.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b9dd12",
   "metadata": {},
   "source": [
    "Columns that are 'Missing' in policy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e18837",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMissing = dfPolicyData.astype(str) == \"Missing\"\n",
    "dfPolicyData.columns[dfMissing.any()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7721dcaa",
   "metadata": {},
   "source": [
    "Columns that are NaN in invoice dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490129b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInvoiceData.columns[dfInvoiceData.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30ef632",
   "metadata": {},
   "source": [
    "Columns that are 'Missing' in invoice dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e3bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMissing = dfInvoiceData.astype(str) == \"Missing\"\n",
    "dfInvoiceData.columns[dfMissing.any()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d9ea6f",
   "metadata": {},
   "source": [
    "## Deductible_general"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c494bac",
   "metadata": {},
   "source": [
    "There are only 2 rows where Deductible_general is missing. \n",
    "\n",
    "Since they are relatively small amount of rows for this dataset, these rows are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0d33b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMissingDecutibleGeneral = dfModelData[dfModelData[\"Deductible_general\"].isna()]\n",
    "dfModelData.drop(dfMissingDecutibleGeneral.index, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8aad5f",
   "metadata": {},
   "source": [
    "## ClientBirthday"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2707f1",
   "metadata": {},
   "source": [
    "There are only 43 rows where ClientBirthday is missing.\n",
    "\n",
    "Since they are relatively small amount of rows for this dataset, these rows are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e189795",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMissingClientBirthday = dfModelData[dfModelData[\"ClientBirthday\"].isna()]\n",
    "dfModelData.drop(dfMissingClientBirthday.index, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec433376",
   "metadata": {},
   "source": [
    "## BMClassMOD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00583ff",
   "metadata": {},
   "source": [
    "There are only 14 rows where BMClassMOD is missing.\n",
    "\n",
    "Since they are relatively small amount of rows for this dataset, these rows are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb660704",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMissingBmClassMod = dfModelData[dfModelData[\"BMClassMOD\"].isna()]\n",
    "dfModelData.drop(dfMissingBmClassMod.index, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7a8926",
   "metadata": {},
   "source": [
    "## avgFuelConsumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf687c1",
   "metadata": {},
   "source": [
    "There are 27927 rows where avgFuelConsumption is missing.\n",
    "\n",
    "Since it s a big amount of rows, correlation between avgFuelConsumption and other fields are calculated for non-missing data.\n",
    "\n",
    "It is found out that avgFuelConsumption has fair linear correlation with the attributes of Power, Weight,  VehicleFirstRegistrationYear and Premium.\n",
    "\n",
    "Missing data is filled based on random forest classifier model since avgFuelConsumption field contains cardinal-categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e073420c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMissingAvgFuelConsumption = dfModelData[dfModelData[\"avgFuelConsumption\"].isna()]\n",
    "dfNonMissingAvgFuelConsumption = dfModelData.drop(dfMissingAvgFuelConsumption.index, inplace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abdfa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aUniqueAvgFuelConsumption = dfNonMissingAvgFuelConsumption[\"avgFuelConsumption\"].unique()\n",
    "\n",
    "dfAvgFuelConsumptionClassified = pd.get_dummies(dfNonMissingAvgFuelConsumption[\"avgFuelConsumption\"])\n",
    "dfNonMissingAvgFuelConsumption[aUniqueAvgFuelConsumption] = dfAvgFuelConsumptionClassified\n",
    "\n",
    "dfCorr = dfNonMissingAvgFuelConsumption[[\"avgFuelConsumption\", \"Power\", \"Weight\", \"VehicleFirstRegistrationYear\", \"Premium\"]].corr()\n",
    "\n",
    "sns.heatmap(dfCorr.abs(), vmin=0, vmax=1, annot = True, cmap=\"Greens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07812b20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfX = dfNonMissingAvgFuelConsumption[[\"Power\", \"Weight\", \"VehicleFirstRegistrationYear\", \"Premium\"]]\n",
    "dfY = dfNonMissingAvgFuelConsumption[aUniqueAvgFuelConsumption]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dfX, dfY, test_size=0.3, random_state=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "oRandForModel = RandomForestClassifier()\n",
    "oRandForModel.fit(X_train, y_train)\n",
    "\n",
    "y_pred = oRandForModel.predict(X_test)\n",
    "\n",
    "print('Accuracy: ', metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "print('Recall: ', metrics.recall_score(y_test, y_pred,zero_division=0,  average = 'micro'))\n",
    "\n",
    "print('Precision: ', metrics.precision_score(y_test, y_pred,zero_division=0,  average = 'micro'))\n",
    "\n",
    "print('F1-Score: ', metrics.f1_score(y_test, y_pred,zero_division=0,  average = 'micro'))\n",
    "\n",
    "aPredictedClassesForMissing = oRandForModel.predict(dfMissingAvgFuelConsumption[[\"Power\", \"Weight\", \"VehicleFirstRegistrationYear\", \"Premium\"]])\n",
    "\n",
    "dfPredictedClassesForMissing = pd.DataFrame(data = aPredictedClassesForMissing ,  columns = aUniqueAvgFuelConsumption, index = dfMissingAvgFuelConsumption.index )\n",
    "\n",
    "sPredictedLabels = dfPredictedClassesForMissing.idxmax(axis=1)\n",
    "\n",
    "dfModelData[\"avgFuelConsumption\"].fillna(sPredictedLabels, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb4f108",
   "metadata": {},
   "source": [
    "## Region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba16bbb",
   "metadata": {},
   "source": [
    "There are 6727 rows (aprx. 9% of whole dataset).\n",
    "\n",
    "According to given attributes, it is difficult to build a pattern that can help to predict region.\n",
    "\n",
    "Regional data may give information about person's financial information. That's why, this attribue is kept in model data but missing rows have been removed from dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132baf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPolicyDataWithMissingRegion = dfModelData[dfModelData[\"Region\"] == \"Missing\"]\n",
    "dfModelData.drop(dfPolicyDataWithMissingRegion.index, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1afd3d6",
   "metadata": {},
   "source": [
    "## FuelType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198d7fd6",
   "metadata": {},
   "source": [
    "For missing fuel types, we can use policy dataset as a \"vehicle\" dataset where we can build a classification model to identify fuel type. Logically, fuel type is related with vehicle attributes related features such as 'VehicleType', 'VehicleUsage', 'Power', 'Weight','VehicleFirstRegistrationYear', 'Mark', 'Model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa0eba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ReplaceMissingDataWithClassifier(dfModelData, \n",
    "                                ['VehicleType', 'VehicleUsage', 'Mark', 'Model'], \n",
    "                                ['Power', 'Weight','VehicleFirstRegistrationYear'], \n",
    "                                'FuelType')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74346e40",
   "metadata": {},
   "source": [
    "## DriveTrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b6a52d",
   "metadata": {},
   "source": [
    "There are 22558 rows that have missing information for this field. Since it s a around 36% of whole data and 'DriveTrain' depends on vehicle attributes, we can replace missing data with a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d575a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "ReplaceMissingDataWithClassifier(dfModelData, \n",
    "                                ['VehicleType', 'VehicleUsage', 'Mark', 'Model'], \n",
    "                                ['Power', 'Weight','VehicleFirstRegistrationYear'], \n",
    "                                'DriveTrain')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c73588f",
   "metadata": {},
   "source": [
    "# FEATURE ENCODING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594598e6",
   "metadata": {},
   "source": [
    "\"One-Hot Encoding\" is applied when:\n",
    "\n",
    "The categorical feature is not ordinal.\n",
    "The number of categorical features is less so one-hot encoding can be effectively applied\n",
    "\n",
    "\"Label Encoding\" is applied when:\n",
    "\n",
    "The categorical feature is ordinal.\n",
    "The number of categories is quite large as one-hot encoding can lead to high memory consumption\n",
    "\n",
    "source: \n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2020/03/one-hot-encoding-vs-label-encoding-using-scikit-learn/\n",
    "\n",
    "https://datascience.stackexchange.com/questions/9443/when-to-use-one-hot-encoding-vs-labelencoder-vs-dictvectorizor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b3ad55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aCreateOneHotEncoding(dfModelData, sCategoricalFeature):\n",
    "    \n",
    "    dfTemp = dfModelData[dfModelData[sCategoricalFeature] == \"OTHER\"]\n",
    "    dfTemp[sCategoricalFeature] = \"Other_\" + str(sCategoricalFeature)\n",
    "    \n",
    "    dfModelData[dfModelData[sCategoricalFeature] == \"OTHER\"] = dfTemp\n",
    "    \n",
    "    aUniqueValues = dfModelData[sCategoricalFeature].unique()\n",
    "    \n",
    "    dfFeatureClassified = pd.get_dummies(dfModelData[sCategoricalFeature])\n",
    "\n",
    "    dfModelData = dfModelData.join(dfFeatureClassified, on= \"policy_guid\")\n",
    "\n",
    "    dfModelData.drop([sCategoricalFeature], axis = 1 , inplace = True)\n",
    "\n",
    "    return aUniqueValues, dfModelData\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1586bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aCreateLabelEncoding(dfModelData, sCategoricalFeature):\n",
    "\n",
    "    oLabelEncoder = LabelEncoder()\n",
    "\n",
    "    oLabelEncoder.fit(dfModelData[sCategoricalFeature])\n",
    "    \n",
    "    dfModelData[sCategoricalFeature] =oLabelEncoder.transform(dfModelData[sCategoricalFeature])\n",
    "    \n",
    "    return oLabelEncoder.classes_, dfModelData\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ca61f5",
   "metadata": {},
   "source": [
    "## TARGET_LABEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b658bd9c",
   "metadata": {},
   "source": [
    "Target labels that represent late payment of invoices of corresponding policy is generated based on following criteria:\n",
    "\n",
    "1: Paid late at least once\n",
    "\n",
    "0: Paid always on time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5c8be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "aConditions = [\n",
    "    (dfInvoiceData[\"due_date\"] < dfInvoiceData[\"paid_date\"]),\n",
    "    (dfInvoiceData[\"due_date\"] >= dfInvoiceData[\"paid_date\"])\n",
    "]\n",
    "\n",
    "dfInvoiceData[\"isInvoiceLatePaid\"] = np.select(aConditions, [1, 0])\n",
    "\n",
    "oInvocieGroupByPolicy = dfInvoiceData[[\"policy_guid\", \"isInvoiceLatePaid\"]].groupby([\"policy_guid\"])\n",
    "\n",
    "dfInvoiceIssueStatistics = oInvocieGroupByPolicy.agg([\"sum\"])\n",
    "\n",
    "dfInvoiceIssueStatistics = dfInvoiceIssueStatistics[\"isInvoiceLatePaid\"] \n",
    "\n",
    "dfInvoiceIssueStatistics.columns = [\"number_of_late_payments\"]\n",
    "\n",
    "aTargetLabels = [1, 0]\n",
    "aConditions = [\n",
    "    (dfInvoiceIssueStatistics[\"number_of_late_payments\"] >= 1),\n",
    "    (dfInvoiceIssueStatistics[\"number_of_late_payments\"] == 0)\n",
    "]\n",
    "\n",
    "dfTargetLabels = pd.DataFrame(data = np.select(aConditions, aTargetLabels), index = dfInvoiceIssueStatistics.index, columns = [\"TARGET_LABEL\"])\n",
    "\n",
    "dfModelData = dfTargetLabels.join(dfModelData,  on= \"policy_guid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8585fdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfModelData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8875e0ae",
   "metadata": {},
   "source": [
    "## Country"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97085f71",
   "metadata": {},
   "source": [
    "\"Country\" field is same in whole dataset. It is not considered in model dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5a30b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfModelData[\"Country\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d88a855",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfModelData.drop([\"Country\"], axis = 1 , inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259822fe",
   "metadata": {},
   "source": [
    "## VehicleType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2a2e90",
   "metadata": {},
   "source": [
    "By logical judgement, \"VehicleType\" can be explained by \"Weight\" and maybe \"Mark\" and \"Model\" of vehicle. That's why, this variable is excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5223e687",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfModelData.drop(\"VehicleType\", axis = 1 , inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eecfadb",
   "metadata": {},
   "source": [
    "## VehicleUsage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bbc94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aVehicleUsages, dfModelData = aCreateOneHotEncoding(dfModelData, \"VehicleUsage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea10484",
   "metadata": {},
   "source": [
    "## Mark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a71621",
   "metadata": {},
   "source": [
    "Since there are big amount of marks, label encoding is used to not increase the size of dataset massively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf391aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aMarks, dfModelData = aCreateLabelEncoding(dfModelData, \"Mark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4123317",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cbb166",
   "metadata": {},
   "source": [
    "Since there are big amount of models, label encoding is used to not increase the size of dataset massively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b383e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "aModels, dfModelData = aCreateLabelEncoding(dfModelData, \"Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b6f69a",
   "metadata": {},
   "source": [
    "## Region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac5eb7f",
   "metadata": {},
   "source": [
    "Region information is converted into geographical coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe99914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a new Nominatim client\n",
    "aUniqueRegions = dfModelData[\"Region\"].unique()\n",
    "dfRegionsWithCoordinates = pd.DataFrame(index = aUniqueRegions, columns = [\"latitude\", \"longitude\"])\n",
    "\n",
    "oGeolocator = Nominatim(user_agent=\"tutorial\")\n",
    "\n",
    "for i in range(len(aUniqueRegions)):\n",
    "    sRegion = aUniqueRegions[i]\n",
    "    \n",
    "    oLocation = oGeolocator.geocode(sRegion)\n",
    "    fLatitue = oLocation.latitude\n",
    "    fLongitude = oLocation.longitude\n",
    "\n",
    "\n",
    "    dfRegionsWithCoordinates.loc[sRegion, \"latitude\"] = fLatitue\n",
    "    dfRegionsWithCoordinates.loc[sRegion, \"longitude\"] = fLongitude\n",
    "\n",
    "dfRegionsWithCoordinates.index.name = \"Region\"\n",
    "dfRegionsWithCoordinates.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ae625b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfRegionsWithCoordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd533a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfModelData = pd.DataFrame(\n",
    "    data = pd.merge(dfModelData,dfRegionsWithCoordinates, on='Region').values,\n",
    "    index = dfModelData.index,\n",
    "    columns = np.concatenate([dfModelData.columns, ['latitude', 'longitude']]))\n",
    "\n",
    "dfModelData[['latitude', 'longitude']] = dfModelData[['latitude', 'longitude']].astype(float)\n",
    "\n",
    "dfModelData.drop(\"Region\", axis = 1 , inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835f95bf",
   "metadata": {},
   "source": [
    "## Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee6dda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "aGenders, dfModelData = aCreateOneHotEncoding(dfModelData, \"Gender\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e777a86d",
   "metadata": {},
   "source": [
    "## ClientBirthDay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3505ec",
   "metadata": {},
   "source": [
    "Client age on policy start date is more relevant than client birthday since we are handling historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9e0985",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfModelData[\"ClientAgeOnPolicyStart\"] = (pd.to_datetime(dfModelData[\"PolicyStartDate\"])-pd.to_datetime(dfModelData[\"ClientBirthday\"])).astype('<m8[Y]')\n",
    "dfModelData.drop(\"ClientBirthday\", axis = 1 , inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82647c7",
   "metadata": {},
   "source": [
    "## BMClassMOD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c2ff9c",
   "metadata": {},
   "source": [
    "BM Class Mod looks like ordinal categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09da450c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aBMClassMODs, dfModelData =  aCreateLabelEncoding(dfModelData, \"BMClassMOD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2968902d",
   "metadata": {},
   "source": [
    "## PolicyIssueDate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f80849",
   "metadata": {},
   "source": [
    "By logical judgement, \"PolicyIssueDate\" is not related if a person pays late or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0e781a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfModelData.drop(\"PolicyIssueDate\", axis = 1 , inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cb7f81",
   "metadata": {},
   "source": [
    "## PolicyStartDate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99493c6",
   "metadata": {},
   "source": [
    "\"PolicyStartDate\" could be a reason of payment date. Especially day part of the date could be a reason. Maybe persons prefer to pay after their salary. \n",
    "\n",
    "This field is converted to following fields: \n",
    "\n",
    "1. year, month and day.\n",
    "\n",
    "2. In addition, expected duration of policies are calculated based on difference between start and end date.\n",
    "\n",
    "3. Vehicle age on PolicyStartDate\n",
    "\n",
    "Since this fied is date time field it is removed from model dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67156988",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfModelData[\"PolicyStartYear\"] = pd.DatetimeIndex(dfModelData[\"PolicyStartDate\"]).year\n",
    "\n",
    "dfModelData[\"PolicyStartMonth\"] = pd.DatetimeIndex(dfModelData[\"PolicyStartDate\"]).month\n",
    "\n",
    "dfModelData[\"PolicyStartDay\"] = pd.DatetimeIndex(dfModelData[\"PolicyStartDate\"]).day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda33ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfModelData[\"PolicyDurationInMonths\"] = (pd.to_datetime(dfModelData[\"PolicyEndDate\"])-pd.to_datetime(dfModelData[\"PolicyStartDate\"])).astype('<m8[M]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcb5902",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfModelData[\"VehicleAgeOnPolicyStart\"] = dfModelData[\"PolicyStartYear\"]-dfModelData[\"VehicleFirstRegistrationYear\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7d6582",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfModelData.drop(\"PolicyStartDate\", axis = 1 , inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920c97b3",
   "metadata": {},
   "source": [
    "## PolicyEndDate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec4ee0e",
   "metadata": {},
   "source": [
    "Newly created \"PolicyDurationInMonths\" attribute would cover \"PolicyEndDate\" attribute. That's why, it is removed from model dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d5062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfModelData.drop(\"PolicyEndDate\", axis = 1 , inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f9d1b0",
   "metadata": {},
   "source": [
    "## PolicyActualEndDate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8882d617",
   "metadata": {},
   "source": [
    "\"PolicyActualEndDate\" attribute doesn't have a meanining since the prediction algorithm will be running on current time when policy is still active. That's why \"PolicyActualEndDate\" is removed from model dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c99a1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfModelData.drop(\"PolicyActualEndDate\", axis = 1 , inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16bcd5b",
   "metadata": {},
   "source": [
    "## Channel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da1f2c5",
   "metadata": {},
   "source": [
    "Channel may be related with the payment operation as well. It could be so that electronic channels may have more stable payment routine. It is observed that direct and unknown payments have highest late payment rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bd4ea1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfChannelStats = dfModelData[[\"Channel\", \"TARGET_LABEL\"]].groupby([\"Channel\"]).agg([\"sum\", \"count\"])\n",
    "\n",
    "dfChannelStats = dfChannelStats[\"TARGET_LABEL\"]\n",
    "\n",
    "dfChannelStats = pd.DataFrame(data = dfChannelStats.values , index  = dfChannelStats.index, columns=[\"policies paid late\", \"number of policies\"])\n",
    "\n",
    "dfChannelStats.reset_index(inplace = True)\n",
    "\n",
    "dfChannelStats[\"policies paid on time\"] = dfChannelStats[\"number of policies\"]-dfChannelStats[\"policies paid late\"]\n",
    "\n",
    "dfChannelStats[\"policies paid on time (%)\"] = (dfChannelStats[\"policies paid on time\"]/dfChannelStats[\"number of policies\"])*100\n",
    "\n",
    "dfChannelStats[\"policies paid late (%)\"] = (dfChannelStats[\"policies paid late\"]/dfChannelStats[\"number of policies\"])*100\n",
    "\n",
    "dfChannelStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aa7fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "aChannels, dfModelData = aCreateOneHotEncoding(dfModelData, \"Channel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a74bb7d",
   "metadata": {},
   "source": [
    "## FuelType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d8e427",
   "metadata": {},
   "outputs": [],
   "source": [
    "aFuelTypes, dfModelData = aCreateOneHotEncoding(dfModelData, \"FuelType\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b9f38a",
   "metadata": {},
   "source": [
    "## DriveTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0e4dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "aDriveTrains, dfModelData = aCreateOneHotEncoding(dfModelData, \"DriveTrain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b63232",
   "metadata": {},
   "source": [
    "## sales_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8858a788",
   "metadata": {},
   "outputs": [],
   "source": [
    "aSalesTypes, dfModelData = aCreateOneHotEncoding(dfModelData, \"sales_type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b852e41",
   "metadata": {},
   "source": [
    "Finally all data is in numerical format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eac1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfModelData = dfModelData.astype(float)\n",
    "\n",
    "dfModelData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35e19ac",
   "metadata": {},
   "source": [
    "## Variance Inflation Factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1605c11",
   "metadata": {},
   "source": [
    "A rule of thumb for interpreting the variance inflation factor:\n",
    "\n",
    "1 = not correlated.\n",
    "Between 1 and 5 = moderately correlated.\n",
    "Greater than 5 = highly correlated.\n",
    "\n",
    "Source:\n",
    "\n",
    "https://www.investopedia.com/terms/v/variance-inflation-factor.asp\n",
    "\n",
    "https://www.statisticshowto.com/variance-inflation-factor/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80252030",
   "metadata": {},
   "source": [
    "## Principle Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2ea92d",
   "metadata": {},
   "source": [
    "# PREDICTIVE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c64268",
   "metadata": {},
   "outputs": [],
   "source": [
    "aFeaturesY = [\"TARGET_LABEL\"]\n",
    "aFeaturesX = dfModelData.drop(aFeaturesY, axis = 1, inplace = False).columns\n",
    "\n",
    "\n",
    "dfX = dfModelData[aFeaturesX]\n",
    "\n",
    "dfY = dfModelData[aFeaturesY]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dfX, dfY, test_size=0.3, random_state=1)\n",
    "\n",
    "oScaler = StandardScaler()\n",
    "X_train = oScaler.fit_transform(X_train)\n",
    "X_test = oScaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5753ca0e",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ee6101",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "oDecTreeModel =  DecisionTreeClassifier()\n",
    "oDecTreeModel.fit(X_train, y_train)\n",
    "\n",
    "y_pred = oDecTreeModel.predict(X_test)\n",
    "\n",
    "print('Accuracy: ', metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "print('Recall: ', metrics.recall_score(y_test, y_pred,zero_division=0,  average = 'micro'))\n",
    "\n",
    "print('Precision: ', metrics.precision_score(y_test, y_pred,zero_division=0,  average = 'micro'))\n",
    "\n",
    "print('F1-Score: ', metrics.f1_score(y_test, y_pred,zero_division=0,  average = 'micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e21a7ea",
   "metadata": {},
   "source": [
    "## Multi Layer Perception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59863d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "oMlpModel = tf.keras.Sequential()\n",
    "\n",
    "oMlpModel.add(tf.keras.layers.Dense(50, activation='relu', kernel_initializer='he_normal', input_shape=aFeaturesX.shape))\n",
    "oMlpModel.add(tf.keras.layers.Dense(20, activation='relu', kernel_initializer='he_normal'))\n",
    "oMlpModel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "oMlpModel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "oMlpModel.fit(X_train, y_train, epochs=50, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5563452",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = oMlpModel.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=-1)\n",
    "\n",
    "print('Accuracy: ', metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "print('Recall: ', metrics.recall_score(y_test, y_pred,zero_division=0,  average = 'micro'))\n",
    "\n",
    "print('Precision: ', metrics.precision_score(y_test, y_pred,zero_division=0,  average = 'micro'))\n",
    "\n",
    "print('F1-Score: ', metrics.f1_score(y_test, y_pred,zero_division=0,  average = 'micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3d85d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
